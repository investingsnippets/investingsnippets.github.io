{"pageProps":{"posts":[{"slug":"pension-planning-lite","frontmatter":{"title":"Pension Planning - Lite","description":"Understand how to account for your pension.","date":"January 8, 2023","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/pension.jpeg"},"excerpt":"","content":"\nIt has been some time now that I have been looking around the internet for a pension calculator that is intuitive and will help me understand\nhow things work and how I can plan based on my current situation.\n\nWhile there is a plethora of tools, videos and blog posts, I was not able to find something that would guide me step by step\nin an easy way.\n\nSo, I decided to create something from scratch and make it available :satisfied:!\n\n\nYou can find a simple, lightweight, and intuitive pension planner in this [link](https://pension-calculator-lite.streamlit.app/).\n\nI hope you enjoy it!\n"},{"slug":"rule-of-72","frontmatter":{"title":"The rule of 72","description":"A reverse engineered proof.","date":"September 24, 2022","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/inflation.jpeg"},"excerpt":"","content":"\n## Definition\n\n> a quick, useful formula that is popularly used to estimate the number of years required to double the invested money at a given annual rate of return. Alternatively, it can compute the annual rate of compounded return from an investment given how many years it will take to double the investment. (Investopedia)\n\n$$\nYears-To-Double = \\frac{72}{Expected-Rate-of-Return}\n$$\n\nAnd\n\n$$\nExpected-Rate-of-Return = \\frac{72}{Years-To-Double}\n$$\n\n## The proof\n\nI will be cheating here :) But, how does the years to double/half correlate with the rate of return?\n\nThis is nothing more than:\n\n$$\nP_1 = \\frac{1}{(1+r)^{year}} P_0 => \\frac{P1}{P0} = \\frac{1}{(1+r)^{year}}\n$$\n\ngiven that we target at $\\frac{P1}{P0} = 0.5$ then\n\n$$\n\\frac{1}{(1+r)^{year}} = \\frac{1}{2} => (1+r)^{year} = 2\n$$\n\n```python\nimport pandas as pd\nimport plotly.express as px\n\nx = [i for i in range(1, 5000)]\ndf = pd.DataFrame(data={\"r10\": [1/(1.1**r) for r in x], \"r5\": [1/(1.05**r) for r in x], \"r1\": [1/(1.01**r) for r in x], \"r05\": [1/(1.005**r) for r in x]})\nfig = px.line(df)\nfig.add_hline(y=0.5)\nfig.show()\n```\n\nAdd algo to find for several rates , like from 1% to 20% the years to halve"},{"slug":"arithmetic-vs-log-returns","frontmatter":{"title":"Arithmetic vs. Log returns","description":"Why and when.","date":"June 29, 2022","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/inflation.jpeg"},"excerpt":"","content":"\n\n## Arithmetic vs. Log returns\n\n\nThe main advantage of log returns is that we can easily aggregate them across time unlike simple returns. For instance, the log return for a year is the sum of the log returns of the days within the year. Additionally, log returns are symmetric around 0 and log return values can range from minus infinity to plus infinity. Whereas, simple returns’ downside is limited at -100% and a negative movement of -25% (movement from 100$ to 75$) does not reverse the losses by going +25% (75$ to 93.75$).\n\n\nhttps://dspyt.com/simple-returns-log-return-and-volatility-simple-introduction/\n"},{"slug":"perpetuities-annuities","frontmatter":{"title":"Annuities and Perpetuities","description":"How a credit installment is repaid? How to plan for pension. A cheat sheet for annuities and perpetuities.","date":"May 29, 2022","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/inflation.jpeg"},"excerpt":"","content":"\n## Annuities\n\n**A stream of cash flows**.\n\n## Perpetuities\n\n**Investments that offer a constant stream of cash flows in perpetuity (eternity)**. Imagine something like a bond without maturity (pays coupons but not the principal).\n\nThe Present Value of a perpetuity is:\n\n$$\nPV-of-perpetuity = \\frac{C}{r} \\qquad (1)\n$$\n\nwhere $C$ the cash payment and $r$ the discount rate.\n\n> We discussed about the `Present Value` of an investment in [Present Value & the Funding Ratio](/post/pv-and-funding-ratio) post.\n\nSo, to receive 1\\$ every year with a discount rate of 10% in perpetuity you just have to invest 10\\$ today.\n\n\nAnother scenario of a perpetuity is the one that \"delays\" the first payment for a number years. That means that the present value of the 10\\$ in some years will need to be discounted.\n\n$$\nPV-of-delayed-perpetuity = \\frac{C}{r(1 + r)^{t}} \\qquad (2)\n$$\n\n"},{"slug":"google-colab-and-beancount","frontmatter":{"title":"Hosting Beancount on Google Colab","description":"Access your Accounting books for free, wherever you are ;)","date":"March 26, 2022","topic":{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/idea-lamp.png"},"excerpt":"","content":"\nMaybe you are in a similar situation like me, where you have deposit and savings accounts on several banks and investment accounts on several services! Keeping track of all that is painful and, unfortunately, there is no service yet that can connect everything together and properly present an aggregate view as well as a historical return of all the accounts together :(\n\nTo overcome this issue, I use [Beancount](https://github.com/beancount/beancount), an opensource `Double-Entry Accounting from Text Files` tool (python) that comes with several extensions and a nice web interface.\n\nAs always, I like to automate and keep my work in one place (on the cloud). I'm extensively using Google Colab and as a consequence, I had to bring all my accounting there too. :P\n\nBeancount comes with a web server (called [fava](https://github.com/beancount/fava)) and Google Colab gives you a virtual machine running your python code. That means that we can use colab to start the web server, BUT we are not allowed to access it through our public browser, UNLESS we create a tunnel! That is possible with [ngrok](https://ngrok.com/) which `exposes local servers behind NATs and firewalls to the public internet over secure tunnels` :)\n\nHere is the code I'm using...\n\n```python\n%%capture\n!pip install beancount\n!pip install fava\n!pip install pyngrok\n!pip install Jinja2 --upgrade  # Needed by fava. Restart runtime \n\nimport os\nimport logging\nfrom fava.cli import app\nfrom pyngrok import ngrok, conf\nfrom google.colab import drive\n\nNGROK_TOKEN = \"your token here\"\nLOCATION_OF_MAIN_BEANCOUNT_FILE = '/content/drive/My Drive/Colab Notebooks/beancount/base.beancount'\n\n# Mounts the google drive to the virtual machine and gives access to storage\ndrive.mount('/content/drive', force_remount=True)\n\n# disables logs for flask\nlog = logging.getLogger('werkzeug')\nlog.setLevel(logging.ERROR)\n\napp.config.from_mapping(\n    BASE_URL=\"http://localhost:5000\",\n    INCOGNITO=False,  # Set to True if you want to show your balance to someone but not reveal the actual numbers\n    USE_NGROK=os.environ.get(\"USE_NGROK\", \"True\") == \"True\" and os.environ.get(\"WERKZEUG_RUN_MAIN\") != \"true\",\n    BEANCOUNT_FILES=[\n        LOCATION_OF_MAIN_BEANCOUNT_FILE\n    ]\n)\n\nngrok.set_auth_token(NGROK_TOKEN)\nconf.get_default().monitor_thread = False\npublic_url = ngrok.connect(5000, bind_tls=True).public_url\nprint(\" * ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:{}\\\"\".format(public_url, 5000))\n\napp.config[\"BASE_URL\"] = public_url\napp.run()\n```\n\nSimple, but comes with some more steps:\n\n1. Colab has an older version of Jinja2 preinstalled and when you update to latest one, you have to restart the runtime!\n2. You need an NGROK authentication token (but that is free). Register at [ngrok](https://ngrok.com/). Set the `NGROK_TOKEN` variable in the script when you are done with registering.\n3. You have to have your beancount files uploaded to google drive under `Colab Notebooks`, so they get mounted. Then you set the `LOCATION_OF_MAIN_BEANCOUNT_FILE` variable in the script.\n\nThat's it for now :) Enjoy!\n"},{"slug":"reports-with-pyinvestingsnippets-and-plotly","frontmatter":{"title":"pyinvestingsnippets + plotly = AWESOME","description":"An amazing Dash + Plotly + Pyinvestingsnippets stock/portfolio benchmarking tool.","date":"February 12, 2022","topic":{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/magic-lamp.png"},"excerpt":"","content":"\nThe last several months I have been extending [pyinvestingsnippets](https://pyinvestingsnippets.readthedocs.io/en/latest/). Some of the changes include:\n\n* Renamed extensions to fit common financial jargon\n* Added support for dataframes\n* Added support for plotly\n* Included fantastic matplotlib and plotly [examples](https://github.com/investingsnippets/pyinvestingsnippets/tree/master/examples)\n\n<!-- [![pyinvestingsnippets + plotly = AWESOME](https://img.youtube.com/vi/IDzR-N9nfDg/0.jpg)](https://youtu.be/IDzR-N9nfDg \"pyinvestingsnippets + plotly = AWESOME\") -->\n\n<p align=\"center\"><iframe width=\"420\" height=\"315\" \nsrc=\"https://www.youtube.com/embed/IDzR-N9nfDg\">\n</iframe></p>\n\n## Run the examples\n\nTo run the examples, clone [`v3.0.0`](https://github.com/investingsnippets/pyinvestingsnippets/tree/v3.0.0) and follow the `README.md` guidelines.\n\nCurrently the examples generate asset prices based on the random walk hypothesis (Brownian Motion)! This is great for a \"look and feel\" of the tool. If you want to try the examples with your own symbols then change the [ASSET_TICKERS & BENCHMARK_TICKERS](https://github.com/investingsnippets/pyinvestingsnippets/blob/v3.0.0/examples/plotly_report_dataframe.py#L25) and replace the [random price generator](https://github.com/investingsnippets/pyinvestingsnippets/blob/v3.0.0/examples/plotly_report_dataframe.py#L87) with the `yahoo` finance price [DataReader](https://github.com/investingsnippets/pyinvestingsnippets/blob/v3.0.0/examples/plotly_report_dataframe.py#L86).  \n\n## Future Plans\n\nThe journey doesn't stop here! I'm planning to extend [pyinvestingsnippets](https://pyinvestingsnippets.readthedocs.io/en/latest/) even more by introducing:\n\n* more portfolio benchmarking metrics\n* support of inflation & risk free rates\n* etc. \n\nWhat would you like so see more? Send me your thoughts...\n"},{"slug":"the-richest-man-in-babylon","frontmatter":{"title":"The Richest Man in Babylon","description":"Takes just 7 steps to become wealthy!","date":"November 29, 2021","topic":{"id":"booknotes","name":"Book Notes","image":"booknotes.jpg","description":"Reading books, and keeping notes! I seek to what the books have to tell me.","color":"bg-green-300"},"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/richest-man-book.jpeg","tags":[]},"excerpt":"","content":"\n## The 7 steps to build wealth\n\n**<u>1. Start saving.</u>**\n   \n  > Save 10% out of the earnings from your labor. To accomplish such a stream of savings, start disfavoring the gratifications of the daily life and appreciate the accomplishment of owing substantial belongings.\n\n**<u>2. Control your expenses.</u>**\n\n  > All people are burdened with more desires than they gratify. To limit that, study thoughtfully your accustomed habits of living and prioritize your expenses while at the same time respecting each coin you earn.\n\n**<u>3. Make your savings multiply.</u>**\n\n  > While accumulating savings is gratifying, locking them down brings only loss (Inflation!). Instead, find investment opportunities! This of course increases the risk, but may also increase the reward.  \n\n**<u>4. Protect your investment from loss.</u>**\n\n  > The first principle of investment is security for your principal. The penalty of risk is probable loss. Study carefully before investing. The romantic desire to make wealth rapidly is misleading.\n\n  > Each investment is considered a type of loan where you expect to get back the principal plus a profit/loss. To ensure at least the return of the principal, make sure the \"borrower\" is able to repay and has a good reputation on that (otherwise you just making him a present).  \n\n**<u>5. Own your dwelling.</u>**\n\n  > It brings gladness to the heart when one eats from their own trees and leave in their own home. It builds confidence and will to care about the land. It also boosts the efforts behind any of the endeavors. \n\n**<u>6. Ensure a future income.</u>**\n\n  > Prepare for a suitable income in the days to come, for when you grow old. Make sure you can comfort and support your family even when you are no longer with them.   \n\n**<u>7. Increase your ability to earn.</u>**\n\n  > Cultivate your own powers, study and become wiser, become more skillful. Act as to respect yourself! Your desires must be simple and definite. They defeat their own purpose should they be too many.\n\n## The Goddess of Good Luck\n\n  > To attract good luck to yourself, it is necessary to take advantage of opportunities. Those of action on these opportunities are favored by the Goddess of Good Luck.\n\n## Lending money advices\n\n  * To borrowing and lending there is more than the passing of money from the hands of one to the hands of another.\n  * If you desire to help your friend, do so in a way that will not bring your friend's burdens to yourself.\n  * Types of loan securities/pledges:\n      * based on property. The pawn can be land, jewels etc.\n      * based on human effort. Has a steady income from labor and is trustworthy.\n      * if none of the above, can friends of the borrower guarantee that is honorable?\n  * A borrower in the throes of great emotions is not a safe risk.\n  * Ask for what purpose the borrower will use the money.\n  * Ask what knowledge the borrower has for the business they are getting into.\n  * Ask for a security in case the borrower cannot repay.\n  * Always think that your well deserved savings are yours due to the sacrifices you have made. \n\n  <center><h2><u>Better a little caution than a great regret!</u><h2></center>\n"},{"slug":"generating-stock-reports-with-pyinvestingsnippets","frontmatter":{"title":"Pyinvestingsnippets","description":"A python library to aid stock and portfolio analysis and reporting.","date":"October 3, 2021","topic":{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"colab":"https://colab.research.google.com/drive/1ffDTOO0xjlHWJR-XbyoRcoGiZvgN5L9-?usp=sharing","img":"/static/python-module.jpeg"},"excerpt":"","content":"\nIt's been a while since the last time! All this period, I was building a new python library :) This library extends pandas and implements most of the concepts we have discussed in earlier articles.\n\nIt is called `pyinvestingsnippets` and can be found [here](https://pyinvestingsnippets.readthedocs.io/en/latest/).\n\nIn practice, the library extends pandas series with attributes that perform stock analysis operations like:\n\n* [Returns](https://www.investingsnippets.com/post/geometric-progression-and-compounding-of-returns)\n* [Volatility](https://www.investingsnippets.com/post/measures-of-variability)\n* [WealthIndex](https://www.investingsnippets.com/post/from-portfolio-wealth-index-to-index-fund)\n* [Drawdown](https://www.investingsnippets.com/post/drawdown)\n* [SRRI](https://www.esma.europa.eu/sites/default/files/library/2015/11/10_673.pdf)\n* beta (I will write an article about that later on)\n\nBellow you can see how this library can be used to generate simple reports.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%%capture\n%pip install yahoofinancials\n%pip install pyinvestingsnippets==1.0.0\n\nimport sys\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nimport datetime\n\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport dateutil.parser\n\nimport pyinvestingsnippets\n```\n\n</p>\n</details>\n\n<details><summary>Helper Functions</summary>\n<p>\n\n```python\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    df.columns = [ticker]\n    return df\n```\n\n</p>\n</details>\n\n```python\n# Let's first pick two assets to compare\nASSET_1 = 'AAPL'\nASSET_2 = 'SPY'\n\n# We will work with the last 5 years of historical data\nend_date = datetime.datetime.now()\nstart_date = end_date - datetime.timedelta(days=5 * 365)\nend_date_frmt = end_date.strftime(\"%Y-%m-%d\")\nstart_date_frmt = start_date.strftime(\"%Y-%m-%d\")\n\n# We fetch the historical prices from yahoofinance\nasset1_prices = retrieve_stock_data(ASSET_1, start_date_frmt, end_date_frmt)\nasset2_prices = retrieve_stock_data(ASSET_2, start_date_frmt, end_date_frmt)\n\n# From the prices series we can now get the returns by using an extension\nasset1_rets = asset1_prices[ASSET_1].prices.returns\nasset2_rets = asset2_prices[ASSET_2].prices.returns\n\n# We can get the wealth index from the returns\nasset1_wi = asset1_rets.wealth_index\nasset2_wi = asset2_rets.wealth_index\n\n# and the drawdown from the wealth index\nasset1_dd = asset1_wi.drawdown\nasset1_dd_dur = asset1_dd.durations\nasset2_dd = asset2_wi.drawdown\nasset2_dd_dur = asset2_dd.durations\n\nrolling_rets = pyinvestingsnippets.RollingReturns(asset1_rets.data, rolling_window=252)\n\n# 3 month rolling annualized vol\nrolling_vol = pyinvestingsnippets.RollingVolatility(asset1_rets.data, rolling_window=90, window=252)\n\n# 30 days rolling beta\nrolling_beta = pyinvestingsnippets.RollingBetaCovariance(asset2_rets.data, asset1_rets.data, 30)\n\n# total beta\nbeta = pyinvestingsnippets.BetaCovariance(asset2_wi.monthly_returns.data, asset1_wi.monthly_returns.data)\n\n# the downside risk as an ExponantiallyWeightedDownsideRisk\ndownside_risk = pyinvestingsnippets.ExponantiallyWeightedDownsideRisk(asset1_rets.data)\n\n# the risk classes and the risk values for SRRI\nasset1_srri_rc = asset1_prices[ASSET_1].prices.monthly_returns.srri.risk_class\nasset1_srri_rv = asset1_prices[ASSET_1].prices.monthly_returns.srri.value\nasset2_srri_rc = asset2_prices[ASSET_2].prices.monthly_returns.srri.risk_class\nasset2_srri_rv = asset2_prices[ASSET_2].prices.monthly_returns.srri.value\n```\n\nThe outcome as a plot looks like:\n\n<details><summary>Generate a report with matplotlib</summary>\n<p>\n\n```python\nfig = plt.figure(figsize=(14, 16), constrained_layout=True)\nfig.suptitle(\"Report\", weight='bold')\ngs = gridspec.GridSpec(5, 6, figure=fig)\n\nax_equity = plt.subplot(gs[0, :])\nax_drawdown = plt.subplot(gs[1, :])\nax_monthly_returns = plt.subplot(gs[2, :3])\nax_yearly_returns = plt.subplot(gs[2, 3:])\nax_rolling_returns = plt.subplot(gs[3, :2])\nax_rolling_vol = plt.subplot(gs[3, 2:4])\nax_downside_risk = plt.subplot(gs[3, 4:])\nax_stats = plt.subplot(gs[4, :3])\nax_beta = plt.subplot(gs[4, 3:])\n\nasset1_wi.plot(ax=ax_equity, color='blue', label=ASSET_1)\nasset2_wi.plot(ax=ax_equity, color='grey', label=ASSET_2)\n\nasset1_dd.plot(ax=ax_drawdown, color='blue')\nasset2_dd.plot(ax=ax_drawdown, color='grey')\n\nasset1_wi.monthly_returns.plot(ax=ax_monthly_returns, color='blue')\nasset1_wi.annual_returns.plot(ax=ax_yearly_returns, color='blue')\nrolling_beta.plot(ax=ax_beta, color='blue', label='cov')\n\nrolling_rets.plot(ax=ax_rolling_returns, color='blue')\nrolling_vol.plot(ax=ax_rolling_vol, color='blue')\n\ndownside_risk.plot(ax=ax_downside_risk, color='blue')\n\ndef _plot_stats(ax=None, **kwargs):\n    if ax is None:\n        ax = plt.gca()\n\n    data = [\n        ['Total Return', '{:.0%}'.format(asset1_wi.total_return), '{:.0%}'.format(asset2_wi.total_return)],\n        ['CAGR', '{:.2%}'.format(asset1_wi.cagr), '{:.2%}'.format(asset2_wi.cagr)],\n        ['Max Drawdown', '{:.2%}'.format(asset1_dd.max_drawdown), '{:.2%}'.format(asset2_dd.max_drawdown)],\n        ['Avg Drawdown Duration', asset1_dd_dur.mean(), asset2_dd_dur.mean()],\n        ['Max Drawdown Duration', asset1_dd_dur.max(), asset2_dd_dur.max()],\n        ['SRRI', '{}/7 ({:.2%})'.format(asset1_srri_rc, asset1_srri_rv),\n                 '{}/7 ({:.2%})'.format(asset2_srri_rc, asset2_srri_rv)],\n        ['Beta', '{:.2}'.format(beta.beta), '1']\n    ]\n    column_labels=[\"Metric\", f\"{ASSET_1}\", f\"{ASSET_2}\"]\n    ax.axis('tight')\n    ax.axis('off')\n    table = ax.table(cellText=data, colLabels=column_labels, loc=\"center\", edges='open')\n    table.set_fontsize(10)\n    table.scale(1.5, 1.5)\n\n    ax.grid(False)\n    ax.get_yaxis().set_visible(False)\n    ax.get_xaxis().set_visible(False)\n    ax.set_ylabel(\"Stats\")\n    ax.set_xlabel('')\n\n    ax.axis([0, 10, 0, 10])\n\n    return ax\n\n_plot_stats(ax=ax_stats)\n\nplt.show()\n```\n\n</p>\n</details>\n    \n![png](generating-stock-reports-with-pyinvestingsnippets/generating-stock-reports-with-pyinvestingsnippets_5_0.png)\n\nAs a next step, I will extend this module to support even more metrics and portfolio functions.\n\nAs always, stay tuned!\n"},{"slug":"modern-portfolio-theory-gmv-msr-mixed","frontmatter":{"title":"Modern Portfolio Theory - Part 2","description":"Global Minimum Variance (GMV), Maximum Sharpe Ratio (MSR), Capital Allocation Line (Cal) and the mixed portfolio.","date":"June 21, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/nobel-price.jpg","colab":"https://colab.research.google.com/drive/1UhsvOExkQsHE_wIBzQR9rqwcKUSI_FZr?usp=sharing"},"excerpt":"","content":"\nPreviously in [Modern Portfolio Theory - Part 1](/post/modern-portfolio-theory) we located the global minimum variance portfolio (GMV) on the Efficient Frontier. In [Present Value & the Funding Ratio](/post/pv-and-funding-ratio) we showed how inflation drags down the real return of an investment. \n\nWhile GMV would pose the minimum (estimated) risk for an investor, it might not be the one that outperforms inflation. In addition to that, government bonds are considered very low risk and yield an annual return which is the profit for the investor. When constructing a portfolio of assets that are riskier, the expectation on return should be higher. A return that surpasses inflation and bond returns should be the goal.\n\nQuoting from [Investopedia - Risk-Free Rate Of Return](https://www.investopedia.com/terms/r/risk-freerate.asp)\n\n> The risk-free rate of return is the theoretical rate of return of an investment with zero risk. The real risk-free rate can be calculated by subtracting the current inflation rate from the yield of the Treasury bond matching your investment duration.\n\nBack to the [wikipedia](https://en.wikipedia.org/wiki/Modern_portfolio_theory) article on Modern Portfolio Theory we notice some pretty interesting aspects. \n\n![jpg](https://upload.wikimedia.org/wikipedia/commons/e/e1/Markowitz_frontier.jpg)\n\n1. There can be a portfolio which yields the risk-free rate and has ~0 standard deviation. It is with minimal risk. That can be a government bond/bill/note for example.\n2. When we start mixing low risk assets (like bonds) with risky assets (like stocks) the return increases but it cannot cross above a line. That, is presented in the graph above with the `Best possible CAL` line which is the Efficient Frontier of the portfolios that hold both risky and non-risky assets. It is called Capital Allocation Line (CAL) or Capital Market Line (CML).\n3. There is a point where the line touches the risky asset portfolios Efficient Frontier (`Tangency Portfolio` or `Maximum Sharpe Ratio Portfolio`).\n\n> $Sharpe Ratio = \\frac{R_p - R_f}{\\sigma_p}$ where \n> * $R_p$ = Portfolio return\n> * $R_f$ = Risk free rate\n> * $\\sigma_p$ = Portfolio standard deviation\n\nThe point in the graph that gives us the best possible return for every unit of risk we take is called `Maximum Sharpe Ratio Portfolio`.\n\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%%capture\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.lines as mlines\nimport matplotlib.pyplot as plt\nimport dateutil.parser\nimport numpy as np\nfrom scipy.optimize import minimize\n```\n\n</p>\n</details>\n\n<details><summary>Helper Functions</summary>\n<p>\n\n```python\ndef retrieve_stock_data(symbol, start, end):\n    \"\"\"Fetches daily stock prices from Yahoo Finance\"\"\"\n    json = YahooFinancials(symbol).get_historical_price_data(start, end, \"daily\")\n    df = pd.DataFrame(columns=[\"adjclose\"])\n    for row in json[symbol][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef arithmetic_returns(S):\n    \"\"\"Returns the arithmetic returns. (price_today - price_yesterday)/price_yesterday\"\"\"\n    return S.pct_change().dropna()\n\ndef annualize_rets(r, periods_per_year):\n    compounded_growth = (1+r).prod()\n    n_periods = r.shape[0]\n    return compounded_growth**(periods_per_year/n_periods)-1\n\ndef portfolio_return(weights, returns):\n    return weights.T @ returns\n\ndef portfolio_volatility(weights, covariance_matrix):\n    return (weights.T @ covariance_matrix @ weights)**0.5\n\ndef generate_returns_dataframe(symbols_list, d_from=\"2020-01-01\", d_to=\"2021-01-01\"):\n    \"\"\"Generates a DataFrame with daily returns for a list of Symbols.\"\"\"\n    returns = pd.DataFrame()\n    for symbol in symbols_list:\n        stock_prices = retrieve_stock_data(symbol, d_from, d_to)\n        rets = arithmetic_returns(stock_prices).dropna()\n        rets.columns = [symbol]\n\n        if returns.empty:\n            returns = rets\n        else:\n            returns = returns.merge(rets, left_index=True, right_index=True)\n    return returns\n\ndef global_minimum_variance(returns, covariance_matrix):\n  \"\"\"Returns the weights of the GMV portfolio\"\"\"\n  n = returns.shape[0]\n  weights_sum_to_1 = {\n      'type': 'eq',\n      'fun': lambda weights: np.sum(weights) - 1\n  }\n  result = minimize(portfolio_volatility, np.repeat(1/n, n), args=(covariance_matrix, ),\n                    method=\"SLSQP\", options={'disp': False},\n                    constraints=(weights_sum_to_1), bounds=((0.0, 1.0),)*n)\n  return result.x\n\ndef efficient_frontier_dataframe(annualized_returns, covariance_matrix): \n  # first we construct 10 pairs of weights like [(0.1,0.9), (0.2,0.8) ...]\n  weights = [np.array([w, 1-w]) for w in np.linspace(0, 1, 10)]\n\n  # then we calculate the return of the portfolio for each pair of weights\n  portfolio_returns = [portfolio_return(w, annualized_returns) for w in weights]\n\n  # and the volatility of the portfolio for each pair of weights\n  vols = [portfolio_volatility(w, covariance_matrix) for w in weights]\n\n  return pd.DataFrame({\n      \"Return\": portfolio_returns, \n      \"Volatility\": vols,\n      \"weights\": weights\n  })\n\ndef print_weights_on_graph(ax, x, y, val):\n  \"\"\"Adds the weight on the efficient frontier\"\"\"\n  a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n  for i, point in a.iterrows():\n    prettified_p = f\"({round(point['val'][0], 2)},{round(point['val'][1], 2)})\"\n    ax.text(point['x'], point['y'], prettified_p)\n\ndef plot_custom_legend():\n  \"\"\"Adds a custom legend for the different points on the graph\"\"\"\n  blue_star = mlines.Line2D([], [], color='blue', marker='*', linestyle='None',\n                            markersize=10, label=f\"MSR\")\n  red_square = mlines.Line2D([], [], color='red', marker='s', linestyle='None',\n                            markersize=10, label=f\"GMV\")\n  purple_triangle = mlines.Line2D([], [], color='green', marker='^', linestyle='None',\n                            markersize=10, label=f\"Mix\")\n\n  plt.legend(handles=[blue_star, red_square, purple_triangle])\n```\n\n</p>\n</details>\n\n```python\nSTOCK_SYMBOLS = ['MSFT', 'APT']\nreturns = generate_returns_dataframe(STOCK_SYMBOLS)\ncovariance_matrix = returns.cov()\nannualized_returns = annualize_rets(returns, 252)\ngmv = global_minimum_variance(annualized_returns, covariance_matrix)\n```\n\n## Capital Allocation Line & Maximum Sharpe Ratio Portfolio (MSR)\n\nSimilar to what we did with the GMV portfolio (in [Modern Portfolio Theory - Part 1](/post/modern-portfolio-theory)), we will do with finding the MSR Portfolio. Instead of trying to minimize the volatility, we will maximize the sharpe ratio. To achieve that, we will use the exact same quadratic programming approach and the `minimize` function from `scipy`.\n\nHowever, to turn maximization into minimization, we will do a simple trick! we will minimize the negative value of the sharpe ratio :) \n\n\n```python\ndef maximum_sharpe_ratio(returns, covariance_matrix, risk_free_rate):\n  \"\"\"Returns the weights of the MSR portfolio\"\"\"\n  n = returns.shape[0]\n  weights_sum_to_1 = {\n      'type': 'eq',\n      'fun': lambda weights: np.sum(weights) - 1\n  }\n\n  def negative_sharpe_ratio(weights, returns, covariance_matrix, risk_free_rate):\n    p_rets = portfolio_return(weights, returns)\n    p_vol = portfolio_volatility(weights, covariance_matrix)\n    return -(p_rets - risk_free_rate)/p_vol\n\n  result = minimize(negative_sharpe_ratio, np.repeat(1/n, n),\n                    args=(returns, covariance_matrix, risk_free_rate),\n                    method=\"SLSQP\", options={'disp': False},\n                    constraints=(weights_sum_to_1), bounds=((0.0, 1.0),)*n)\n  return result.x\n```\n\n\n```python\nrisk_free_rate = 0.03\nmsr = maximum_sharpe_ratio(annualized_returns, covariance_matrix, risk_free_rate)\nmsr\n```\n  array([0.75444897, 0.24555103])\n\n## Plotting GMV, MSR, CAL & Mixed portfolio\n\n```python\n# get the efficient frontier as dataframe and plot it\nef = efficient_frontier_dataframe(annualized_returns, covariance_matrix)\nax = ef.plot(x=\"Volatility\", y=\"Return\", style=\".-\", figsize=(14,7),\n             title=\"2 Asset Portfolio Risk/Return\", legend=False)\nax.set_xlim(left=0)\nplt.ylabel(\"Return\")\n\n# show the different weights on the efficient frontier\nprint_weights_on_graph(ax, ef.Volatility, ef.Return, ef.weights)\n\n# find the MSR point and add a marker\nrets_msr = portfolio_return(msr, annualized_returns)\nvol_msr = portfolio_volatility(msr, covariance_matrix)\nax.plot(vol_msr, rets_msr, 'b*', markersize=10)\n\n# construct and plot the Capital Allocation Line\ncal_x = np.arange(0, vol_msr, (vol_msr-0)/100)\ncal_y = np.arange(risk_free_rate, rets_msr, (rets_msr-risk_free_rate)/100)\nax.plot(cal_x, cal_y, color=\"green\", linestyle=\"dashed\")\n\n# find the GMV point and add a marker\nrets_gmv = portfolio_return(gmv, annualized_returns)\nvol_gmv = portfolio_volatility(gmv, covariance_matrix)\nax.plot(vol_gmv, rets_gmv, 'rs', markersize=10)\n\n# locate the Mixed portfolio on CAL that has less risk than the GMV\nidx = np.argwhere(np.diff(np.sign(rets_gmv - cal_y))).flatten()\nax.plot(cal_x[idx], cal_y[idx], 'g^', markersize=10)\n\n# Add a custom legend for the markers\nplot_custom_legend()\nplt.show()\n```\n\n![png](modern-portfolio-theory-gmv-msr-mixed/modern-portfolio-theory-gmv-msr-mixed-8-0.png)\n    \n\n## MSR vs. GMV vs. Mixed Portfolio\n\nFrom the graph above we can pinpoint 3 interesting portfolios:\n\n* The GMV portfolio which depends solely on the covariance matrix.\n* The MSR portfolio which depends both on the covariance matrix and the expected returns.\n* The Mixed Portfolio which can yield same return as the GMV but with less risk.\n\nSo far, we took advantage of the historical data and constructed some portfolios. However, **historical returns are no guarantee of future returns**. Estimating returns is very difficult and requires experience, deep knowledge of the respective assets, continuous monitoring of the market and much more.\n\nSo, which portfolio is best to hold? The answer to that cannot be given with certainty!\n\nGMV does not take into account returns (or estimated expected returns) and thus makes it easier to calculate, but at the same time, constructing the covariance matrix for a long list of assets falls under the well known [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) problem.\n\nMSR poses the difficulty of estimating expected returns and thus includes increased risk.\n\nThe mixed portfolio combines non risky assets with risky assets while lowering the risk. However, the covariance matrix is yet there to calculate.\n\nAll in all, each portfolio includes risk, which we call **model risk**. There are several ways to tackle this model risk and I will try to address them in future articles.\n\nUntil then, enjoy!\n"},{"slug":"inflation-looking-back","frontmatter":{"title":"Inflation - Looking Back!","description":"See how the piggy bank :P can digest part of your money.","date":"June 8, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/no-piggy-bank.png","colab":"https://drive.google.com/file/d/1DUyPzZAVkdg1QMiPNnkbBMB1cu2AT33o/view?usp=sharing"},"excerpt":"","content":"\nIn [Present Value & the Funding Ratio](/post/pv-and-funding-ratio) we picked a static annual inflation rate to plan the financing of future expenses. In this article we will use the inflation historical data to determine how inflation had affected our savings.\n\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%%capture\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nimport matplotlib.pyplot as plt\n!pip install -q requests\n!pip install -q pyscbwrapper\n```\n\n</p>\n</details>\n\nLet us assume that 10 years ago a person started depositing 500\\$ of hers/his salary to the bank once a month.\n\n\n```python\nSAVINGS_PER_MONTH = 500\ndef generate_montly_savings_account_balance(monthly_savings):\n  today = datetime.today()\n  beginning_of_month = datetime(today.year, today.month, 1)\n  ten_years_ago = beginning_of_month - relativedelta(years=10)\n\n  savings_df = pd.DataFrame({\n      'date': pd.date_range(start=ten_years_ago, end=beginning_of_month, closed=None, freq='MS'),\n      'savings': monthly_savings\n      })\n  savings_df = savings_df.set_index(['date'])\n  return savings_df\n\nsavings_df = generate_montly_savings_account_balance(SAVINGS_PER_MONTH)\nsavings_accumulated = savings_df.savings.cumsum()\ncurrent_total = savings_accumulated[savings_df.index[-1]]\nsavings_plot = savings_accumulated.plot(figsize=(14,7))\nsavings_plot.set_title('Deposit account growth')\nsavings_plot.set_xlabel('Date')\nsavings_plot.set_ylabel('Amount')\nsavings_plot.axhline(current_total, color='g', linestyle='--', label='{:0.2f}'.format(current_total))\nsavings_plot.legend()\nplt.show()\n```\n\n![png](inflation-looking-back/inflation-looking-back_3_0.png)\n\nNext, let's fetch the US inflation rates from the [U.S. BUREAU OF LABOR STATISTICS](https://www.bls.gov/) and see how it advanced over the last decade.\n\n```python\ndef get_us_cpi(start_year='2012', end_year = '2021'):\n  '''\n    from https://www.bls.gov/developers/api_python.htm#python1\n  '''\n  import requests\n  import json\n  headers = {'Content-type': 'application/json'}\n  data = json.dumps({\"seriesid\": ['SUUR0000SA0'],\"startyear\":start_year, \"endyear\":end_year})\n  p = requests.post('https://api.bls.gov/publicAPI/v1/timeseries/data/', data=data, headers=headers)\n  json_data = json.loads(p.text)\n  cpi_list = []\n  for series in json_data['Results']['series']:\n    seriesId = series['seriesID']\n    for item in series['data']:\n      year = item['year']\n      period = item['period']\n      value = item['value']\n  \n      if 'M01' <= period <= 'M12':\n        datetime_object = datetime.strptime('{}-{}-01'.format(year, period[1:]), '%Y-%m-%d')\n        value = float(value)\n        cpi_list.append({\n                    'date': datetime_object,\n                    'cpi':  0.001 if value == 0.0 else value\n                  })\n  cpi= pd.DataFrame(cpi_list)\n  cpi['date'] = cpi['date'].astype('datetime64[ns]')\n  cpi = cpi.set_index('date')\n  cpi = cpi.sort_values(by='date')\n  return cpi\n\nus_cpi = get_us_cpi()\ninflation_rate_change_plot = us_cpi['cpi'].plot(figsize=(14,7))\ninflation_rate_change_plot.set_title('Inflation Monthly')\ninflation_rate_change_plot.set_xlabel('Date')\ninflation_rate_change_plot.set_ylabel('Infaltion Value')\nplt.show()\n```\n\n![png](inflation-looking-back/inflation-looking-back_5_0.png)\n    \nWe will now see how inflation would affect the purchase value of 1\\$. To do that we will calculate the monthly percentage change and then create what we previously referred to as [wealth index](/post/from-portfolio-wealth-index-to-index-fund) of 1\\$.\n\n\n```python\ndef plot_cpi_on_unit(cpi, unit='$'):\n  if 'pct_change' not in cpi.columns:\n    cpi['pct_change'] = cpi['cpi'].pct_change().dropna() \n  change_on_unit = 1 * (1+cpi['pct_change'][savings_df.index[0]:]).cumprod()\n  change_on_unit_plot = change_on_unit.plot(figsize=(14,7))\n  change_on_unit_plot.set_title(f\"How Inflation affects 1 {unit}\")\n  change_on_unit_plot.set_xlabel('Date')\n  change_on_unit_plot.set_ylabel(f\"Worth of 1 {unit}\")\n  change_on_unit_plot.axhline(change_on_unit[-1], color='g', linestyle='--', label='{:0.2f}'.format(change_on_unit[-1]))\n  change_on_unit_plot.legend()\n  plt.show()\n\nplot_cpi_on_unit(us_cpi, unit='$')\n```\n\n![png](inflation-looking-back/inflation-looking-back_8_0.png)\n\nWhat the graph shows above is that buying a product at 1\\$ in the beginning of 2012, this exact same product would today cost 1.15\\$. If you had saved the dollar in an account you would today miss 0.15\\$ to be able to purchase the product. In practice, you have lost 0.15\\$!\n\nNow that we have the percentage changes of the inflation, lets calculate the total loss of our savings all these years. For each month we add money to the cash account we have to start measuring the impact of inflation from that point on.\n\n```python\ndef plot_loss_on_savings(savings, cpi):\n  last_valid_index = cpi.last_valid_index()\n  s = pd.DataFrame(columns=savings.index.array, index=savings.index)\n  for col in s.columns:\n    wealth_index = savings['savings'][col] * (1+cpi['pct_change'][col:last_valid_index]).cumprod()\n    s[col] = wealth_index - savings['savings'][col]\n  inf_losses = s.sum(axis=1)\n  ax = inf_losses.plot(figsize=(14,7))\n  ax.set_title('Total loss due to inflation')\n  ax.set_xlabel('Date')\n  ax.set_ylabel('Loss in $')\n  ax.axhline(inf_losses[last_valid_index], color='r', linestyle='--', label='{:0.2f}'.format(inf_losses[last_valid_index]))\n  ax.legend()\n  ax.plot()\n\nplot_loss_on_savings(savings_df, us_cpi)\n```\n\n![png](inflation-looking-back/inflation-looking-back_11_0.png)\n    \n\n5349\\$ is the loss of saving in the bank without investing. That is almost 10% of the total account value!\n\n## Sweden\n\nHaving a special interest in the Swedish market, I will include the code needed to fetch inflation statistics from [SCB](https://www.scb.se/en/finding-statistics/statistics-by-subject-area/prices-and-consumption/consumer-price-index/consumer-price-index-cpi/pong/tables-and-graphs/cpi-with-fixed-interest-rate-cpif-cpif-ct-and-cpif-xe/cpif-annual-changes/).\n\n\n```python\ndef get_swedish_cpi():\n  \"\"\" Returns the CPI as a monthly percentage change.\n  For more information: https://github.com/kirajcg/pyscbwrapper/blob/master/pyscbwrapper.ipynb\n  \"\"\"\n  import re\n  from pyscbwrapper import SCB\n  scb = SCB('en', 'PR', 'PR0101', 'PR0101A', 'KPIFFMP')\n  scb.set_query(month=scb.get_variables()['month'])\n  data = scb.get_data()['data']\n  my_list = []\n  for obj in data:\n    date_scb_formatted = obj['key'][0].strip()  # this looks like '2020M06'\n    year_month_search = re.search('^(\\d{4})M(\\d{2})$', date_scb_formatted)\n    datetime_object = datetime.strptime('{}-{}-01'.format(year_month_search.group(1), year_month_search.group(2)), '%Y-%m-%d')\n    my_list.append({'date': datetime_object,\n                    'pct_change': float(obj['values'][0])/100\n                    })\n  cpi= pd.DataFrame(my_list)\n  cpi = cpi.set_index('date')\n  return cpi\n\nsw_cpi = get_swedish_cpi()\n\nplot_cpi_on_unit(sw_cpi, unit='SEK')\n```\n   \n![png](inflation-looking-back/inflation-looking-back_16_0.png)\n"},{"slug":"pv-and-funding-ratio","frontmatter":{"title":"Present Value & the Funding Ratio - Planning Forward!","description":"How to plan for future cash flows taking into account inflation, taxes and fees.","date":"May 23, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/inflation.jpeg","colab":"https://drive.google.com/file/d/13aiLUQUIXhfjfnO_20U4Dap0tsfRiF-B/view?usp=sharing"},"excerpt":"","content":"\nInflation is the reason behind the erosion of the value of money over time. 1\\$ today is more valuable than 1\\$ after 5 years (given a positive Inflation rate). I will not get into the details of what inflation is, since there is lots of literature around that.\n\nIn this article I will showcase how to plan the financing of future expenses by including inflation, taxes on earnings and annual fees.\n\nLet's see that with an example. Let's assume that a person decides a job break for 3 years. Estimates his/her monthly expenses to be 300\\$. What is the initial amount of money the person needs to have to be able to fulfill their goal?\n\nThe quick response to that is $3 * 12 * 300 = 10800$\\$! but that is not actually true due to the inflation. Let's (for the sake of the example) say that the inflation is 2% per year. That means that 1\\$ today will be valued at $1*\\frac{1}{1 + .02}=0.9804$\\$ in one year from now.\n\n> Note: the notation $(1+i)^n$ where i is the annual inflation rate and n is the periods in years, is something we discussed in the [Geometric Progression and the Compounding of the Returns](/post/geometric-progression-and-compounding-of-returns) article. Think of inflation as compounding with negative rate.\n\nIn other words we are asking for the `Present Value` of a future ($t=1$) 1\\$ ($C_1$) with a discount rate ($r$) of 2%. The formula is:\n\n$$\nPresent-Value = \\frac{C_t}{(1 + r)^{t}} \\qquad (1)\n$$\n\nWe call the $1*\\frac{1}{(1 + r)^{t}}$ `discount factor`.\n\nSo, for the first year, the person will pay 3600\\$ but the actual cost of 3600\\$ today is $3600*(1+.02)^1 =3672$\\$. That means that there are $3672-3600=72$\\$ that are missing and should be added to the initial amount. The second year the person will need to pay 3600 again, but the value today is $3600*(1+.02)^2=3745.44$\\$. The third year, the situation is the same and the calculated value for the 3600\\$ would today be $3600*(1+.02)^3=3820.348$\\$.\n\nSo far, the deficit for the first year is 72, for the second year is 145 and for the third year is 220. A total of 437\\$ needs to be added to our initial thought of 10800!\n\nThe example above can easily be extended to loan payments, house rent or even pension forecasting (the financial term for all these is [`annuities`](/post/perpetuities-annuities)).\n\nLet's get into code and replicate the scenario above.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%%capture\nimport pandas as pd\nimport numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display\n%matplotlib inline\n```\n\n</p>\n</details>\n\n```python\nmonthly_liabilities = 300\nyears = 3\nannual_liabilities = 12 * monthly_liabilities\ninflation_rate = 0.02\nliabilities = pd.Series(data=[annual_liabilities for i in range(years)], index=[i+1 for i in range(years)])\n\ndef liabilities_inflated(liabilities, inflation_rate):\n  years = liabilities.index\n  inflation_over_years = pd.Series(data=(1+inflation_rate)**years, index=years)\n  return liabilities * inflation_over_years\n\ninflated_liabilities = liabilities_inflated(liabilities, inflation_rate)\nprint(f\"Liabilities per year: {list(liabilities.values)}, \\\n        \\nInflated liabilities: {list(inflated_liabilities.values)}, \\\n        \\nTotal amount needed today: {inflated_liabilities.sum():.2f}, \\\n        \\nNo inflation case: {liabilities.sum():.2f}\")\n```\n    Liabilities per year: [3600, 3600, 3600],         \n    Inflated liabilities: [3672.0, 3745.44, 3820.3488000000007],         \n    Total amount needed today: 11237.79,         \n    No inflation case: 10800.00\n\n\n## Present Value - Balancing inflation through investing\n\nInflation is eroding the value of money when they stand still! However, a balanced situation can be achieved if the money is invested to something that yields a return equal (or even better, more) than the inflation rate. There are several options to that, from 0 risk saving accounts that pay an interest, to a type of bond that increases the risk a bit and gives a better return (or even inflation-adjusted bonds), to more risky assets like equity funds that might yield an even better return.\n\nIn any case, we should take into account the taxes on profits and the administration fees each investment scenario requires.\n\nLet's go back to our example and say that we found a savings account that pays 2% annually in interest without fees and taxes. That would mean that we could invest 10800 to this account today and each year pay out the bill.\n\nBut what if there was a 10% tax on the profits from this interest? How much money would we need today to start with? Or else, what is the present value of the investment?\n\n\n\n```python\ninvestment_annual_return = 0.02\ninvestment_profit_tax = 0.1\n```\n\n```python\ndef discount(t, r, earnings_tax):\n  \"\"\"\n    Computes the amount needed today which when invested for t periods with\n    an r return per period and earning_tax on the profits, will equal to 1$  \n  \"\"\"\n  return (1 + r - r*earnings_tax)**-t \n\ndef present_value(liabilities, return_annual, earnings_tax):\n  \"\"\"\n    Given a list of liabilities, an annual return and tax rate on earnings,\n    returns the amount of money needed today.\n  \"\"\"\n  dates = liabilities.index\n  discounts = discount(dates, return_annual, earnings_tax)\n  return (discounts * liabilities).sum()\n```\n\n```python\npresent_value(inflated_liabilities, investment_annual_return, investment_profit_tax)\n```\n    10842.491757684631\n\n\nMoving forward, lets say that instead of a savings account we invest in funds that have an annual return of 2%, a profit tax of 10% and an annual administration fee of 0.5% on the total invested amount.\n\nWhat should the initial investment be?\n\n```python\nannual_fee = .005 # percentage of total amount per year\n```\n\n```python\ndef present_value_with_fees(liabilities, return_annual, earnings_tax, annual_fee):\n  dates = liabilities.index\n  discounts = discount(dates, return_annual, earnings_tax)\n  liabilities_with_fees = [liabilities[len(dates) +1 - i] * (1-annual_fee)**-i for i in list(dates)[::-1]]\n  return ((discounts * liabilities_with_fees).values).sum()\n```\n\n```python\npresent_value_with_fees(inflated_liabilities, investment_annual_return, investment_profit_tax, annual_fee)\n```\n    10951.755210394398\n\nIt becomes apparent that taxes and fees play an important role in calculating future cash flows.\n\n## The Funding Ratio\n\nThe funding ratio is a percentage that shows how much our current assets can contribute to future cash flows. A 100% funding ratio means that the initial amount of assets will be just enough to fulfill our goals. A ratio below 100% means that we will need more assets to cover for the future flows and a ratio more than 100% means that we are overfunded and as such we have a surplus of money we can use to fund other goals.\n\n\n```python\ndef funding_ratio(current_assets_value, liabilities, return_annual, earnings_tax, annual_fee):\n  return 100*current_assets_value/present_value_with_fees(liabilities, return_annual, earnings_tax, annual_fee)\n\ndef show_funding_ratio(monthly_liabilities, years, inflation_rate, current_assets_value, return_annual, earnings_tax, annual_fee):\n  annual_liabilities = 12 * monthly_liabilities\n  liabilities = pd.Series(data=[annual_liabilities for i in range(years)], index=[i+1 for i in range(years)])\n  inflated_liabilities = liabilities_inflated(liabilities, inflation_rate)\n  fr = funding_ratio(current_assets_value, inflated_liabilities, return_annual, earnings_tax, annual_fee)\n  print(f'{fr:.2f}')\n```\n\n```python\nfunding_ratio(10951.76, inflated_liabilities, investment_annual_return, investment_profit_tax, annual_fee)\n```\n    100.00004373368022\n\n\nBelow I have included a widget which helps me plan for my future goals. I will try to have it as an HTML widget at some point but for the ones who are interested try the [colab](https://drive.google.com/file/d/13aiLUQUIXhfjfnO_20U4Dap0tsfRiF-B/view?usp=sharing) representation of this article. \n\n```python\ncontrols = widgets.interactive(show_funding_ratio,\n                               monthly_liabilities = widgets.IntSlider(min=100, max=100000, step=100, value=300),\n                               years = widgets.IntSlider(min=3, max=100, step=1, value=3),\n                               inflation_rate = widgets.FloatSlider(min=0.01, max=0.3, step=.01, value=.02),\n                               current_assets_value = widgets.IntSlider(min=10000, max=30000000, step=1000, value=10800),\n                               return_annual = widgets.FloatSlider(min=0.01, max=0.2, step=.01, value=.08),\n                               earnings_tax = widgets.FloatSlider(min=0.1, max=.5, step=.02, value=.3),\n                               annual_fee = widgets.FloatSlider(min=0.01, max=.05, step=.01, value=.03))\ndisplay(controls)\n```\n\nEnjoy!"},{"slug":"modern-portfolio-theory","frontmatter":{"title":"Modern Portfolio Theory - Part 1","description":"A Nobel Price winning theory that has shaped Investing.","date":"April 2, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/nobel-price.jpg","colab":"https://colab.research.google.com/drive/1U6dR3xx-g4NWXl-5NKtGlMZVIkSJcJEz?usp=sharing"},"excerpt":"","content":"\nIn a previous post about [Return & Volatility of a Multi-Asset Portfolio](/post/portfolio-expected-return-and-risk) we saw how the correlation of the prices of two assets was a key part to achieving lower volatility than the volatility of the assets' individually. We built a visual proof with just two random assets.\n\nThis \"visual proof\" is called Efficient Frontier\n\n![jpg](https://upload.wikimedia.org/wikipedia/commons/e/e1/Markowitz_frontier.jpg)\n\nwhich is part of Modern portfolio theory (MPT) and according to [wikipedia](https://en.wikipedia.org/wiki/Modern_portfolio_theory)\n\n> Modern portfolio theory (MPT), or mean-variance analysis, is a mathematical framework for assembling a portfolio of assets such that the expected return is maximized for a given level of risk. It is a formalization and extension of diversification in investing, the idea that owning different kinds of financial assets is less risky than owning only one type. Its key insight is that an asset's risk and return should not be assessed by itself, but by how it contributes to a portfolio's overall risk and return. It uses the variance of asset prices as a proxy for risk.\n\n> Economist Harry Markowitz introduced MPT in a 1952 essay, for which he was later awarded a Nobel Memorial Prize in Economic Sciences; see [Markowitz model](https://en.wikipedia.org/wiki/Markowitz_model).\n\nThe \"two asset\" Efficient Frontier we built in the previous post was done through carefully picking the asset weights and printing the mean-variance graph. Then, we were able to find on the graph which pair of weights was responsible for the minimum volatility portfolio.\n\nWe will do the same today but instead of using the Efficient Frontier for that, we will use some optimizers provided by `numpy`. \n\n## Data Collection\n\nAs always, we set the ground work needed to fetch some stock data.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%%capture\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport dateutil.parser\nimport numpy as np\n```\n\n</p>\n</details>\n\n```python\nSTOCK_SYMBOLS = ['MSFT', 'APT']\n\ndef retrieve_stock_data(symbol, start, end):\n    '''\n    Fetches daily stock prices from Yahoo Finance\n    '''\n    json = YahooFinancials(symbol).get_historical_price_data(start, end, \"daily\")\n    df = pd.DataFrame(columns=[\"adjclose\"])\n    for row in json[symbol][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef arithmetic_rets(S):\n    '''Returns the arithmetic returns. (price_today - price_yesterday)/price_yesterday'''\n    return S.pct_change().dropna()\n\ndef annualize_rets(r, periods_per_year):\n    compounded_growth = (1+r).prod()\n    n_periods = r.shape[0]\n    return compounded_growth**(periods_per_year/n_periods)-1\n\ndef portfolio_return(weights, returns):\n    return weights.T @ returns\n\ndef portfolio_volatility(weights, covariance_matrix):\n    return (weights.T @ covariance_matrix @ weights)**0.5\n\ndef generate_returns_dataframe(symbols_list, d_from=\"2020-01-01\", d_to=\"2021-01-01\"):\n    '''\n    Generates a DataFrame with daily returns for a list of Symbols.\n    '''\n    returns = pd.DataFrame()\n    for symbol in symbols_list:\n        stock_prices = retrieve_stock_data(symbol, d_from, d_to)\n        rets = arithmetic_rets(stock_prices).dropna()\n        rets.columns = [symbol]\n\n        if returns.empty:\n            returns = rets\n        else:\n            returns = returns.merge(rets, left_index=True, right_index=True)\n    return returns\n```\n\n\n```python\nreturns = generate_returns_dataframe(STOCK_SYMBOLS)\nreturns.head(5)\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>APT</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-01-03</th>\n      <td>-0.012452</td>\n      <td>0.005780</td>\n    </tr>\n    <tr>\n      <th>2020-01-06</th>\n      <td>0.002585</td>\n      <td>0.008621</td>\n    </tr>\n    <tr>\n      <th>2020-01-07</th>\n      <td>-0.009118</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2020-01-08</th>\n      <td>0.015928</td>\n      <td>-0.002849</td>\n    </tr>\n    <tr>\n      <th>2020-01-09</th>\n      <td>0.012493</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n## Volatility Minimization\n\nThe data we collected above is the same as [Return & Volatility of a Multi-Asset Portfolio](/post/portfolio-expected-return-and-risk), and the purpose is to find the weights that minimize the volatility of the portfolio. As we saw from the graph in the previous article, the weights should be somewhere close to [0.89, 0.11].\n\nFor example, given weights [0.5, 0.5], the portfolio return is:\n\n\n```python\nportfolio_return(np.array([0.5, 0.5]), annualize_rets(returns, 252)) - 1\n```\n    0.3109859252509841\n\n\nTo avoid a brute-force approach with trying different weights and getting the ones that give the minimum volatility, we can use algorithms that do this for us. Algorithms like [Sequential quadratic programming](https://en.wikipedia.org/wiki/Sequential_quadratic_programming) which based on the amount of constraints performs different levels of differentiations with purpose to minimize a cost function. In general, the objective function which in our case is the Efficient Frontier, has a point where the tangent either gets 0 or 1. In our case the tangent should be 1. The algorithm iteratively walks through the objective function, finds the tangent, and applies the constraints. Of course, like any cost function optimization in Machine Learning the accuracy is not always 100%! The algorithm may get stuck in local minima and not be able to find the best result. However, in the case of Efficient Frontier, the objective function has only one minimum and thus makes it easier to find.\n\nLet's apply the algorithm on our data. The only constraints we provide for now is that the sum of the weights must be equal to 1.\n\n\n```python\nfrom scipy.optimize import minimize\nn = len(STOCK_SYMBOLS)\ncov = returns.cov()\nweights_sum_to_1 = {\n    'type': 'eq',\n    'fun': lambda weights: np.sum(weights) - 1\n}\nresult = minimize(portfolio_volatility, np.repeat(0, n), args=(cov, ),\n                  method=\"SLSQP\", options={'disp': False},\n                  constraints=(weights_sum_to_1), bounds=((0.0, 1.0),)*n)\nresult.x\n```\n    array([0.89906844, 0.10093156])\n\n\nThe portfolio that produces the minimum risk for an investor is called `Global Minimum Variance (GMV)` portfolio and lies at the far left of the Efficient Frontier.\n\nIn a next article we will go one step further in the Efficient Frontier analysis and try to find other points in the graph that are of interest.\n\nStay tuned!"},{"slug":"fit-distributions-to-asset-returns","frontmatter":{"title":"Fit Multiple Distributions to Asset Returns!","description":"Why normal distribution is not preferred for stock returns analysis.","date":"March 28, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/normal-dist-with-hist.png","colab":"https://colab.research.google.com/drive/1p3KbU09vOrplisEzMFEQqGV5Jtuxr3FC?usp=sharing"},"excerpt":"","content":"\nEarlier in [Are Stock Returns Normally Distributed?](post/are-stock-returns-normally-distributed) we went through different ways to validate when asset returns are normally distributed. While we used only one stock to prove that stock returns are not normally distributed, the phenomenon applies to any volatile asset, in general.\n\nThe question though is: Really, which distribution do returns follow?\n\nBelow we load the MSFT stock returns as we did before! \n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%%capture\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\n```\n\n</p>\n</details>\n\n```python\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n\nstock_prices = retrieve_stock_data(\"MSFT\", \"2019-01-01\", \"2020-01-01\")\n\nrets = normal_rets(stock_prices).dropna()\nrets.columns = ['returns']\n```\n\n## Fit all known distributions\n\nInstead of trying to fit the Gaussian Distribution to our data, we will try to fit all the known (scipy.stats implementations) distributions and see which one (or ones) fits the data best.\n\n\n```python\nimport numpy as np\nfrom scipy.stats._continuous_distns import _distn_names\nimport scipy.stats as st\nimport warnings\n\ndef fit_all_distributions(data):\n    \"\"\"\n    Returns: Dict of the density function and the Sum of Squared Errors (SSE)\n    \"\"\"\n    # Get histogram of original data. First, get the x for the pdf\n    # second, get y to calculate the distance between the \n    # distribution and the real data\n    y, x = np.histogram(data, bins='auto', density=True)\n    x = (x + np.roll(x, -1))[:-1] / 2.0\n\n    dist_fit = {}\n\n    # Estimate distribution parameters from data\n    for distname in _distn_names:\n        distribution = getattr(st, distname)\n\n        # Try to fit the distribution\n        try:\n            # Ignore warnings for data that can't be fit\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore')\n\n                # fit dist to data\n                params = distribution.fit(data)\n\n                # Calculate fitted PDF and error\n                pdf = distribution.pdf(x, *params)\n                sse = np.sum(np.power(y - pdf, 2.0))\n                dist_fit[distname] = {}\n                dist_fit[distname]['pdf'] = pdf\n                dist_fit[distname]['sse'] = sse\n        except Exception:\n            pass\n\n    return dist_fit\n```\n\n\n```python\ndist_fit = fit_all_distributions(rets.returns)\nlen(dist_fit)\n```\n    99\n\n\n\nWhat? 99? :) Well, It should not be of a surprise that several distributions would fit the data. But the \"fit\" could be really off :) For example an exponential or even a uniform distribution could fit the data, but the squared error (the distance between the actual data points and the respective distribution points) would be really large compared to distributions that approximate the histogram of our data.\n\nLet's get the 15 distributions with the lowest SSE (the ones that best fit the data).\n\n\n```python\ndname_list = []\nsse_list = []\npdf = []\nfor dname in dist_fit:\n  dname_list.append(dname)\n  sse_list.append(dist_fit[dname]['sse'])\n  pdf.append(dist_fit[dname]['pdf'])\n\ndf = pd.DataFrame({'dist': dname_list, 'sse': sse_list, 'pdf':pdf})\ndf = df.sort_values('sse')\ndf_15 = df[:15]\ndf_15\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dist</th>\n      <th>sse</th>\n      <th>pdf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>52</th>\n      <td>laplace</td>\n      <td>148.651407</td>\n      <td>[1.1152568034938246, 1.787521047421414, 2.8650...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>gennorm</td>\n      <td>164.078771</td>\n      <td>[0.8367820735065007, 1.5154080142470994, 2.686...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>dweibull</td>\n      <td>176.635717</td>\n      <td>[0.8793256263714793, 1.5296378660593237, 2.635...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>dgamma</td>\n      <td>191.050465</td>\n      <td>[0.8552917947109029, 1.4658525793451274, 2.502...</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>hypsecant</td>\n      <td>213.097016</td>\n      <td>[0.7998518705010982, 1.383459240576739, 2.3919...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>norminvgauss</td>\n      <td>230.918089</td>\n      <td>[0.9381562968673568, 1.5495654573259756, 2.580...</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>tukeylambda</td>\n      <td>232.611350</td>\n      <td>[0.7436837695141802, 1.2913003803519416, 2.270...</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>johnsonsu</td>\n      <td>241.389639</td>\n      <td>[0.9016851129004099, 1.4956683047905153, 2.519...</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>t</td>\n      <td>243.179748</td>\n      <td>[0.715121368715189, 1.2350959953532723, 2.1881...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>nct</td>\n      <td>250.858249</td>\n      <td>[0.8688042602613769, 1.4480199008827952, 2.464...</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>genlogistic</td>\n      <td>260.651315</td>\n      <td>[0.8748265286114669, 1.5253897598139983, 2.650...</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>logistic</td>\n      <td>263.548520</td>\n      <td>[0.6451586874653275, 1.2223472162162576, 2.299...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>burr12</td>\n      <td>271.252587</td>\n      <td>[0.7636589016658705, 1.3949317532979122, 2.529...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>cauchy</td>\n      <td>272.502209</td>\n      <td>[1.4774507773037833, 1.8876747700442502, 2.491...</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>levy_stable</td>\n      <td>285.357222</td>\n      <td>[0.7622030792115081, 1.252074636445709, 2.2016...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nInterestingly `laplace` was the distribution which approximates best the initial data. However, several other distributions are not that far from `laplace` with regards to SSE distances. To showcase that, let's see how the distributions visually correlate with the actual data.\n\n```python\ny, x = np.histogram(rets.returns, bins=19, density=True)\nx = (x + np.roll(x, -1))[:-1] / 2.0\nax = rets.returns.plot(kind='hist', bins=19, figsize=(14,7), density=True, alpha=0.5, \n                       color=list(matplotlib.rcParams['axes.prop_cycle'])[1]['color'])\n\nfor index, row in df_15.iterrows():\n  pd.Series(row.pdf, x).plot(ax=ax, label=row.dist)\nax.set_title(u'All Fitted Distributions')\nax.set_xlabel(u'Returns')\nax.set_ylabel('Frequency')\nplt.legend(loc=\"upper left\")\nax.plot()\n```\n \n![png](fit-distributions-to-asset-returns/fit_distributions_to_asset_returns_8_1.png)\n    \n\nAn interesting observation from the graph above is that all there is almost a concurrence of the distribution legs and tails. On the contrary they do not really agree on the height of the mean bar, but all of them approximate it pretty well.\n\nSo, are we done yet? Shall we, from now on, use `laplace` as the distribution to represent asset returns? \n\nUnfortunately no :(! And the reason why, is that there are several other parameters that can easily interfere with what we expect:\n\n* The number of data points (daily vs. minute vs. monthly)\n* The selected period. (last 3 months will give different result compared to last 6 months or 1 year, or more)\n* The reason we use a distribution (to measure risk? to predict returns? etc.)\n\n## Why T-Student is often used?\n\nYou might have wondered, why so many people use the T-Student distribution when analyzing stock return data!\n\nThe answer to that has to do with the risk that an investor can accept when placing money to a risky asset. There are different ways to measure risk and one of them is the risk estimation based on distributions (model risk). \n\nSay for example, in the MSFT scenario above, that we would like to avoid any daily price drop of more than 2%. And we ask, how confident are we that there will be no drop of more than 2% tomorrow (since we have daily returns)? \n\nThe answer to that question can be derived from the CDF (Cumulative Distribution Function) of a distribution (which is the area under the PDF (Point Distribution Function))\n\nHere is how the normal and student-t PDF looks like for MSFT returns in the period we test. \n\n```python\nax = rets.returns.plot(kind='hist', bins=19, figsize=(14,7), density=True, alpha=0.5, \n                       color=list(matplotlib.rcParams['axes.prop_cycle'])[1]['color'])\n\nstudent_t_pdf = df.loc[df['dist'] == 't'].pdf.values[0]\npd.Series(student_t_pdf, x).plot(ax=ax, label='t')\nnormal_pdf = df.loc[df['dist'] == 'norm'].pdf.values[0]\npd.Series(normal_pdf, x).plot(ax=ax, label='normal')\n\nax.set_title(u'Student-T and Normal Distributions')\nax.set_xlabel(u'Returns')\nax.set_ylabel('Frequency')\nplt.legend(loc=\"upper left\")\nax.plot()\n```\n \n![png](fit-distributions-to-asset-returns/fit_distributions_to_asset_returns_10_1.png)\n    \n\nBack to our question now. How confident are we that tomorrow the price will not drop below 2% (-0.02)? \n\n```python\nnorm_cdf = np.cumsum(normal_pdf)\nt_cdf = np.cumsum(student_t_pdf)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,7))\nfig.suptitle('CDF for Normal and T dists')\nax1.plot(x, norm_cdf)\nax2.plot(x, t_cdf)\nplt.show()\n```\n\n![png](fit-distributions-to-asset-returns/fit_distributions_to_asset_returns_12_0.png)\n    \n```python\nnorm_cdf[np.where(x <= -0.02)][-1]\n```\n    9.876651890962592\n\n```python\nt_cdf[np.where(x <= -0.02)][-1]\n```\n    8.086428763077489\n\nAccording to the normal distribution, there is 9.9% probability that the price will drop below 2% while only 8.1% student-t probability. Someone, based on their risk tolerance, would be more confident to place some more money knowing that the estimate is much closer to the reality (well, historical reality)!  \n\nBut, again, it is really up to us to use any distribution we feel comfortable with, based on the data we have, our risk level, and of course (when it comes to statistics) the tools to do our analysis easier."},{"slug":"a-tradeable-investment-portfolio-as-erc20-token","frontmatter":{"title":"A Tradeable Investment Portfolio as ERC20 Token","description":"Building a simple portfolio on Ethereum blockchain as an ERC20 token.","date":"February 21, 2021","topic":{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},"tags":[{"id":"cryptos","name":"cryptos","image":"bitcoin.png","description":"The amazing world of Blockchain opens one more chapter in the Investing.","color":"bg-green-300","icon":"bitcoin.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/erc20-exchange.png"},"excerpt":"","content":"\nIn a previous post we discussed how it is possible to create a [simple index fund by combining several tradeable assets](/post/from-portfolio-wealth-index-to-index-fund). We also mentioned that such a portfolio can be traded on the secondary market as a basic ETF. \n\nSuch a scenario is pretty difficult to achieve since special licenses and time consuming actions are needed. However, it is fairly easy to do on the Ethereum blockchain!\n\nThis is what we will try to showcase in this article. We will build a very simple contract that keeps information about a fictional portfolio of other tokens.\n\nIn its extended form, every time someone purchases this token, the amount invested, will be used to purchase the tokens listed in the portfolio based on their weights, and as such, it will follow the development of these tokens (like the index fund we described in the other article).\n\nBelow is a screenshot of the end result. The code is published on [github](https://github.com/investingsnippets/tradeable-investment-portfolio-as-erc20-token).\n\n![png](a-tradeable-investment-portfolio-as-erc20-token/portfolio-token.png)\n\n## Introduction to Ethereum Tokens\n\nI will not go deep into how Ethereum works, since it is a huge topic. We can describe it as a decentralized computer which can execute instructions in a distributed manner. These instructions are simple computer programs (code) that perform a specific job. This program is what we call `An Ethereum Smart Contract`. \n\nThe distributed manner of the contract execution is achieved by the nodes on a blockchain :). That means that the program once might run on a node (server) in USA and the next moment on a node in China. Traditionally, we were used to programs running on a limited number of nodes (e.g. on the cloud), that we managed by a single entity (e.g. cloud provider, company datacenter). However, in the blockchain case, the nodes are just another user terminal (e.g. a server at home, our laptops, etc.).\n\nOn another side, a blockchain solves the well known `double spending` problem (which we will analyze in another post) and due to that, it can be used as an exchange mechanism for digital currencies! Combining these two aspects together, we achieve tradeable contracts (digital assets), which we generally call tokens (they can hold a balance, have a defined supply amount, and can be transferred).\n\nAs an example, imagine a digital currency where you are able to program it with something like. If, the user solves a puzzle on the internet, wins one coin! Or, if the user deposits purchases the coin with some \\$, then these \\$ will be used to purchase other coins and hold them locked in the initial coin, until the owner decides to sell the coin and as a consequence sell the subsequent coins! \n\nNow, the contract creation usually follows some coding principles and is implemented using a programming language. This language is called [Solidity](https://docs.soliditylang.org/en/v0.8.1/) (on Ethereum) and the principles are called standards. ERC20 is such a standard and allows for a digital currency/token creation!\n\nThe tokens can be used in many different ways, which I will describe in a future post. For now we will focus on the creation of digital currency feature.\n\n## The contract\n\nBelow I'm pasting the simplified version of the contract with inlined comments.\n\n```javascript\n// define the solidity compiler version to be used\npragma solidity 0.6.10;\n// additional functionalities for passing around structs  \npragma experimental \"ABIEncoderV2\";\n\n// We use openzeppelin library which provides proper interfaces and\n// solves lot's of boilerplate work we would do otherwise.\nimport { Address } from \"@openzeppelin/contracts/utils/Address.sol\";\nimport { ERC20 } from \"@openzeppelin/contracts/token/ERC20/ERC20.sol\";\nimport { SafeMath } from \"@openzeppelin/contracts/math/SafeMath.sol\";\nimport { SignedSafeMath } from \"@openzeppelin/contracts/math/SignedSafeMath.sol\";\n\n// We define the name of the contract and specify that is an ERC20 token\ncontract PortfolioToken is ERC20 {\n    using Address for address;\n    using SafeMath for uint256;\n    using SignedSafeMath for int256;\n\n    // the user who creates the contract is also the administrator\n    address PortfolioManager;\n\n    // the asset definition\n    struct Asset {\n        uint8 weight; // weight in the Portfolio\n        uint timeStamp; // last updated\n        string name; // name of the asset. For the purposes of the example, since tokens can have the same name\n        string symbol; // symbol of the asset. Also, the symbol of a token can be the same among tokens :(\n    }\n\n    mapping ( address => Asset ) assets; // this allows to store tokens by their ethereum address\n    address[] public allAssets; // a directory of all the token addresses in the fund\n    mapping(address => bool) public isResource; // for validation purposes\n\n    // the initial price of the token\n    int256 private strikePrice;\n\n    // the contsrucor of the token will be called when deploying the contract\n    constructor(\n        int256 _strikePrice,\n        string memory _name,\n        string memory _symbol\n    )\n        public\n        ERC20(_name, _symbol)\n    {\n        PortfolioManager = msg.sender;\n        strikePrice = _strikePrice;\n    }\n\n    modifier onlyManager() {\n        _validateOnlyManager();\n        _;\n    }\n\n    function mint(address _account, uint256 _quantity) public {\n        _mint(_account, _quantity);\n    }\n\n    // the main method used to add an asset to the portfolio\n    function addAsset(address _assetAddress, uint8 _weight, string memory _name, string memory _symbol) external onlyManager {\n        require(!isResource[_assetAddress], \"Asset already exists\"); // not adding true/false val in struct to save gas\n        assets[_assetAddress].weight = _weight;\n        assets[_assetAddress].name = _name;\n        assets[_assetAddress].symbol = _symbol;\n        assets[_assetAddress].timeStamp = block.timestamp;\n        isResource[_assetAddress] = true;\n        allAssets.push(_assetAddress);\n    }\n\n    // edits an asset's weight\n    function editAsset(address _assetAddress, uint8 _weight) external onlyManager {\n        assets[_assetAddress].weight = _weight;\n        assets[_assetAddress].timeStamp = block.timestamp;\n    }\n\n    // removes an asset from the portfolio\n    function removeAsset(address _assetAddress) external onlyManager {\n        delete assets[_assetAddress];\n        // allAssets = allAssets.remove(_assetAddress);\n        isResource[_assetAddress] = false;\n    }\n\n    // returns a list of assets in the portfolio\n    function getAssets() external view returns(address[] memory) {\n        return allAssets;\n    }\n\n    // return the info of an asset by passing the address of it\n    function getAssetInfo(address _assetAddress) external view returns(string memory name, string memory symbol, uint8 weight, uint timeStamp) {\n        return (assets[_assetAddress].name, assets[_assetAddress].symbol, assets[_assetAddress].weight, assets[_assetAddress].timeStamp);\n    } \n\n    function _validateOnlyManager() internal view {\n        require(msg.sender == PortfolioManager, \"Only manager has access\");\n    }\n}\n```\n\n## How to try it out\n\n> Note: Make sure that MetaMask is either disabled, or configured to use the local blockchain. OR, use browser in incognito mode. Be extra careful! (you have been warned ;))\n\nFor this project I have used:\n\n* ganache, which provides a local Ethereum blockchain.\n* truffle, which provides tooling to compile, test and deploy contracts\n* drizzle, which implements client side interaction with the contract (through web3.js)\n* react, for building a simple web application to be able to graphically interact with the contract\n\nTo run the example, clone the [github repo](https://github.com/investingsnippets/tradeable-investment-portfolio-as-erc20-token) and make sure that docker is installed on your machine.\n\nThen, use the command\n\n```bash\ndocker-compose run -p \"3000:3000\" -p \"8545:8545\" --rm develop\n```\n\nwhich starts everything needed in docker and exposes the web app on port 3000!\n\nJust visit `http://localhost:3000/` on your browser and experiment with the app.\n"},{"slug":"drawdown","frontmatter":{"title":"Drawdown","description":"A great measure of risk indicator.","date":"February 18, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/drawdown.png","colab":"https://colab.research.google.com/drive/1LFUprI0yMOLPDK4yqgpRAi5sPrGrDhTi?usp=sharing"},"excerpt":"","content":"\nIn a previous [post](/post/from-portfolio-wealth-index-to-index-fund) we talked about the wealth index of an asset as well as a portfolio of assets. The idea of the `wealth index` is very powerful because it represents the cumulative profit of an asset (since it depends on the price returns).\n\nNow, if we have invested 100$ on an asset and we were asked to find the maximum loss, when did that happen and for how long did it last? We need to walk through our wealth index and find all the deeps, then see which one was the largest, when it did happen and when it finally recovered to the previous peak value.\n\nWe employ a well known measure of risk in Investing, called **Drawdown**.\n\n## Computation and Plotting of the Drawdown\n\nFirst the ground code that allows us to fetch stock historical data.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n```\n\n</p>\n</details>\n\n```python\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    df.columns = [ticker]\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n```\n\nI'll randomly pick Apple's (AAPL) stock for this analysis.\n\n```python\napple_stock_prices = retrieve_stock_data(\"AAPL\", \"1990-03-14\", \"2021-02-17\")\napple_rets = normal_rets(apple_stock_prices).dropna()\nfig, (ax1, ax2) = plt.subplots(2, sharex=True, figsize=(14,7))\nfig.suptitle(\"Apple's Price & Returns\")\napple_stock_prices.plot(ax=ax1, label='Price')\napple_rets.plot(ax=ax2, label='Returns')\nplt.legend(loc=\"upper left\")\nplt.show()\n```\n\n![png](drawdown/drawdown_3_0.png)\n\nSay now that we invested 100$ late 2016. Let's build the wealth index like we did in this [post](/post/from-portfolio-wealth-index-to-index-fund), and find the peaks of the wealth index. That is, the highest generated wealth prices before a deep.\n\n```python\nwealth_index = 100*(1+apple_rets.AAPL[\"12-2016\":]).cumprod()\npeaks = wealth_index.cummax()\nax = wealth_index.plot(figsize=(14,7), label=\"W-Index\")\npeaks.plot(ax=ax, label=\"Peaks\")\nplt.legend(loc=\"upper left\")\nplt.show()\n```\n \n![png](drawdown/drawdown_5_0.png)\n\nDo you see these nice lagoons? Well, we wouldn't want them to be deep and long in duration, cause that is when our investment looses value and we have to wait until it recovers!\n\nSo, moving forward we want to find which lagoon was the deepest, how deep? and how long did it take to move back to the previous peak.\n\nFirst things first, we have to measure at any given point what is the difference between the peak and the wealth index. For example, the peak at a given point is 220\\$ and the index is 150\\$. That means that the index is 70\\$ below the peak. Since our target point is 220\\$ and we have lost 70\\$, we can say that we we are $-\\frac{70}{220}=31.8$% below the target.\n\n```python\ndrawdown =  (wealth_index - peaks)/peaks\ndrawdown.plot(figsize=(14,7), title=\"Drawdown\")\n```\n\n![png](drawdown/drawdown_7_1.png)\n\nThe diagram above is what we call a **Drawdown** of an asset and it doesn't really have to do with any initial investment. Drawdown is a very nice indicator of risk since it is more realistic when compared to other risk indicators that involve standard deviations (Since returns deviate from normality as we proved in [Are Stock Returns Normally Distributed](/post/are-stock-returns-normally-distributed))\n\n## Useful insights from the Drawdown\n\nWe are now ready to find the largest drawdown and the date that occurred. \n\n```python\ndrawdown.min(), drawdown.idxmin()\n```\n    (-0.38515910000506054, Timestamp('2019-01-03 00:00:00'))\n\nWe see that on the 3rd of January 2019 our investment was loosing 38.5% of its value!\n\nOne step further, we will try to find how long the lagoons lasted and find the longest one and an average of their durations.\n\n```python\ndef compute_drawdown_lagoons_durations(drawdown):\n  # find all the locations where the drawdown == 0\n  zero_locations = np.unique(np.r_[(drawdown == 0).values.nonzero()[0], len(drawdown) - 1])\n  # also assign the dates so we know when things were not sinking\n  zero_locations_series = pd.Series(zero_locations, index=drawdown.index[zero_locations])\n  # do a shift to show what is the last and previous non zero dates\n  df = zero_locations_series.to_frame('zero_loc')\n  df['prev_zloc'] = zero_locations_series.shift()\n  # keep only the dates where the difference is more than 1\n  # that denotes the lagoons\n  df = df[df['zero_loc'] - df['prev_zloc'] > 1].astype(int)\n  df['duration'] = df['zero_loc'].map(drawdown.index.__getitem__) - df['prev_zloc'].map(drawdown.index.__getitem__)\n  df = df.reindex(drawdown.index)\n  df = df.dropna()\n  return df['duration']\n```\n\n```python\ndf = compute_drawdown_lagoons_durations(drawdown)\ndf\n```\n    date\n    2016-12-06     4 days\n    2016-12-13     4 days\n    2016-12-27     6 days\n    2017-01-06    10 days\n    2017-01-17     6 days\n                   ...   \n    2020-08-26     2 days\n    2020-08-31     5 days\n    2020-12-28   118 days\n    2021-01-21    24 days\n    2021-02-16    21 days\n    Name: duration, Length: 69, dtype: timedelta64[ns]\n\nThe DataFrame above prints the last day of a drawdown, and its duration in days.\n\n```python\ndf.max(), df.mean()\n```\n    (Timedelta('372 days 00:00:00'), Timedelta('20 days 02:46:57.391304347'))\n\nThe longest drawdown lasted 372 days! and the average duration of a drawdown was 20 days :)\n\nStay tuned!\n"},{"slug":"from-portfolio-wealth-index-to-index-fund","frontmatter":{"title":"From Portfolio Wealth Index to Index Funds","description":"Ever wandered how an index fund is built? In this post we build a primitive one step by step.","date":"February 5, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/hacking-index-funds.png","colab":"https://colab.research.google.com/drive/1tPuqAsHgQdIYhSbaaoCS9LjEayDrTwjX?usp=sharing"},"excerpt":"","content":"\nImagine a scenario where you have invested 100$ in a stock the last 3 years and you earned 20% in the 1st year, -10% in the 2nd year, and 11% in the 3rd year. You would like to see how the investment progressed over the time until today.\n\nWe have already discussed about [geometric progression and the  compounding of returns](/post/geometric-progression-and-compounding-of-returns) in a previous article, and we will use that knowledge even further here.\n\nSo, at the end of the 3rd year, the investment would be:\n\n$$\n100 × 1.2 × .9 × 1.11 = 119.88\n$$\n\nIn the case of an initial investment of 1$, the result above would be called `Cumulative Wealth Index`!\n\nLet me show you how this index progresses over time.\n\nAt first we set the ground work for fetching historical prices.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n```\n\n</p>\n</details>\n\n```python\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    df.columns = [ticker]\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n```\n\nSay, we have invested 100$ in the MSFT stock on the 11th of October 2019. Earlier we used annualized returns, but for this example we will use the daily returns.\n\nBelow we will download the stock prices for the aforementioned period and calculate the daily returns.\n\n```python\nmsft_stock_prices = retrieve_stock_data(\"MSFT\", \"2019-10-11\", \"2021-02-04\")\nmsft_rets = normal_rets(msft_stock_prices).dropna()\nmsft_rets.head()\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-14</th>\n      <td>-0.000931</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>0.014475</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>-0.008194</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>-0.005128</td>\n    </tr>\n    <tr>\n      <th>2019-10-18</th>\n      <td>-0.016322</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nThen, we will build the cumulative wealth index based on the initial investment, over time.\n\n```python\n# See equation (1) in the post about geometric progression and the \n# compounding of returns\nwealth_index = 100 * (1 + msft_rets).cumprod() \nwealth_index.head()\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-14</th>\n      <td>99.906939</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>101.353104</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>100.522643</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>100.007156</td>\n    </tr>\n    <tr>\n      <th>2019-10-18</th>\n      <td>98.374868</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nAnd let's see how it looks like\n\n```python\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\nmsft_stock_prices.plot(ax=ax1, figsize=(14,7))\nwealth_index.plot(ax=ax2)\nax1.get_legend().remove()\nax2.get_legend().remove()\nax1.title.set_text('MSFT Price Chart')\nax2.title.set_text('Cumulative Wealth Index')\nplt.show()\n```\n  \n![png](from-portfolio-wealth-index-to-index-fund/from-portfolio-wealth-index-to-index-fund_7_0.png)\n\nThere are a few things to notice in the graphs above:\n\n* The price development is the same :) and that makes sense, since the actual investment follows the price move of the stock.\n* The start point is different. Since we invested only 100\\$ and not ~140$ (the price of one stock at the moment).\n* The wealth index cares about the daily returns and not the actual price of the asset.\n\nThis last bullet allows us to extend the previous scenario by including more assets in our investment without taking into account the prices of the assets, but only the returns.\n\n## Portfolio Cumulative Wealth Index\n\nI will simplify how a primitive index fund (or Mutual Fund or an ETF) is built by extending the process from the previous section.\n\nSay now that, instead of investing 100$ to MSFT, we split the amount into 4 equal parts and we buy 4 different stocks. I will randomly pick Google's, Tesla's and Paypal's stocks.\n\n```python\nfrom functools import reduce\ngoogle_stock_prices = retrieve_stock_data(\"GOOGL\", \"2019-10-11\", \"2021-02-04\")\ngoogle_rets = normal_rets(google_stock_prices).dropna()\n\ntsla_stock_prices = retrieve_stock_data(\"TSLA\", \"2019-10-11\", \"2021-02-04\")\ntsla_rets = normal_rets(tsla_stock_prices).dropna()\n\npaypal_stock_prices = retrieve_stock_data(\"PYPL\", \"2019-10-11\", \"2021-02-04\")\npaypal_rets = normal_rets(paypal_stock_prices).dropna()\n\n# bring them all together in a single dataframe\nassets_returns = reduce(lambda left,right: left.merge(right, left_index=True, right_index=True),\n            [msft_rets, google_rets, tsla_rets, paypal_rets])\nassets_returns.head()\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>GOOGL</th>\n      <th>TSLA</th>\n      <th>PYPL</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-14</th>\n      <td>-0.000931</td>\n      <td>0.001695</td>\n      <td>0.036589</td>\n      <td>0.001674</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>0.014475</td>\n      <td>0.020094</td>\n      <td>0.003619</td>\n      <td>0.018084</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>-0.008194</td>\n      <td>0.000612</td>\n      <td>0.007212</td>\n      <td>-0.004827</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>-0.005128</td>\n      <td>0.007884</td>\n      <td>0.008547</td>\n      <td>0.005238</td>\n    </tr>\n    <tr>\n      <th>2019-10-18</th>\n      <td>-0.016322</td>\n      <td>-0.006697</td>\n      <td>-0.019163</td>\n      <td>-0.023256</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nBased on the weight allocation of [.25, .25, .25, .25], let us now find the new cumulative wealth index of the investment.\n\n```python\n# since the weights stay same throughout the index and since 100*0.25 = 25\nportfolio_wealth_index = 25 * (1 + assets_returns).cumprod()\nportfolio_wealth_index.head()\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>GOOGL</th>\n      <th>TSLA</th>\n      <th>PYPL</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-14</th>\n      <td>24.976735</td>\n      <td>25.042363</td>\n      <td>25.914720</td>\n      <td>25.041838</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>25.338276</td>\n      <td>25.545567</td>\n      <td>26.008512</td>\n      <td>25.494683</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>25.130661</td>\n      <td>25.561196</td>\n      <td>26.196096</td>\n      <td>25.371627</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>25.001789</td>\n      <td>25.762725</td>\n      <td>26.419986</td>\n      <td>25.504527</td>\n    </tr>\n    <tr>\n      <th>2019-10-18</th>\n      <td>24.593717</td>\n      <td>25.590192</td>\n      <td>25.913712</td>\n      <td>24.911400</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nNow, we just fave to sum the columns per row and plot the result\n\n```python\nportfolio_wealth_index.sum(axis=1).plot.line(figsize=(14,7))\nplt.title('4 Asset Index')\nplt.show()\n```\n\n![png](from-portfolio-wealth-index-to-index-fund/from-portfolio-wealth-index-to-index-fund_13_0.png)\n\n## Index/Mutual/Exchange-Traded Funds\n\nIn the example above, I chose some random assets and equally weighted them in a portfolio! However, even simplistic, this is how a traded fund looks like.\n\nIn practice, a fund is a bucket of assets weighted in a structured way, and initialized with a price (like i did above with the 100$). Then, they are offered in the stock exchange for purchasing. In our example above, if it was a traded fund, the investors would deposit money to the fund and the fund would buy the underlying assets based on the specified weights. The buying and selling of the fund, doesn't affect the price of the fund directly (but indirectly through the underlying assets).\n\nThe difference among the different types of funds is due to the way weights are calculated. Mutual and ETFs have managers that pick these weights based on market research, and other characteristics. The managers, can change the weights by performing a rebalancing of the portfolio.\n\nIn index funds (also known as passive ETFs) the weights are usually based on market cap and maybe other attributes and require minimum intervention from a manager (and due to that are normally much cheaper than the other types of funds). In the simplest case, an index fund could follow all the assets of a specific industry and allocate the weights according to the market capitalization of each asset (divided by the total market cap of all the assets in the index) or just follow the same weighting strategy of a well known index, such as S&P 500.\n\nIn future posts, I will try to build (and invent) different types of funds.\n\nUntil next time!\n"},{"slug":"portfolio-expected-return-and-risk","frontmatter":{"title":"Return & Volatility of a Multi-Asset Portfolio","description":"Maths are magical :) And why diversification makes sense!","date":"January 31, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/magic.png","colab":"https://colab.research.google.com/drive/1lNirrCFUfWaZ_Cci-mmsio79wX_KwR5b?usp=sharing"},"excerpt":"","content":"\nIn previous posts we talked about the [expected return](/post/measures-of-location) (mean value of a distribution) and the [volatility](/post/measures-of-variability) (standard deviation) of an asset.\n\nBut, in investing, we rarely hold a portfolio of just one stock! Let's start then, by picking two stocks.\n\nThe first question is: OK, what percentage of the total investment amount shall we allocate to stock A and what to stock B?\n\nIf we allocate 100% to A and 0% to B or the other way around, then we get into the \"one asset\" portfolio and that is not desirable! Let's assume that we set 50% on asset A and 50% on asset B. Let's also call this percentage allocations, weights (w_A, w_B respectively).\n\nIn that case, we are asked to come up with the return (mean) and the volatility (standard deviation) of the portfolio.\n\nSomeone would blindly assume that the `Return = (w_A * R_A) + (w_B * R_B)` and `Volatility = (w_A * std_A) + (w_B * std_B)`. Well, maths keep always surprising us, and that is the case here!\n\nWhile indeed the return of the 2 asset portfolio is the average weighted returns,\n\n$$\nR_{A,B} = w_A*R_A + w_B*R_B  \\qquad (1)\n$$\n\nthe volatility is\n\n$$\n\\sigma_{A,B}=\\sqrt{\\sigma_A^2w_A^2 + \\sigma_B^2w_B^2 + 2w_Aw_B\\sigma_A\\sigma_B\\rho_{A,B}}  \\qquad (2)\n$$\n\nThis second (2) equation tells us that the standard deviation of a 2 asset distribution is equal to the square root of the variance of asset A multiplied by the squared weight of A plus the variance of asset B multiplied by the squared weight of B, plus twice the product of variance of A times the variance of B times the weight of A times the weight of B times the correlation coefficient of A and B!\n\nSo far so good! But where exactly does the magic begin? Well, the correlation coefficient is not always a positive number :O\n\nThe correlation coefficient can take values between -1 and 1. -1 when the two assets are totally uncorrelated, which means that when the first asset goes up the other goes down at the same pace and same angle. 1 when both assets move to the same direction with the same pace and same angle (either positive or negative direction). Values between -1 and 1 indicate a more loose correlation, but show the trend.\n\nBack to the equation (2). If we have a negative correlation of the assets, the total volatility is less than the average volatility, and if we have a positive correlation the total volatility is more that the average.\n\nIt becomes pretty obvious that by just combining two non correlated assets we can achieve volatility sometimes even smaller than the assets' individually. Who wouldn't want that!\n\n> Keep in mind that $\\rho_{A,B} = \\frac{cov_{A,B}}{\\sigma_A\\sigma_B}$ where $cov_{A,B}$ is the covariance of the two variables.\n\nLet's see an example...\n\n## Real example of a two asset portfolio\n\nAt first, let's set the ground work to be able to fetch some stock prices.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n```\n\n</p>\n</details>\n\n\n```python\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n```\n\nWe are now ready to fetch prices. I have picked Microsoft Corporation (MSFT) and Alpha Pro Tech, Ltd. (APT). Below we see how the price of the stocks unfolded throughout 2020! \n\n```python\nmsft_stock_prices = retrieve_stock_data(\"MSFT\", \"2020-01-01\", \"2021-01-01\")\nmsft_rets = normal_rets(msft_stock_prices).dropna()\nmsft_rets.columns = ['returns']\n\napt_stock_prices = retrieve_stock_data(\"APT\", \"2020-01-01\", \"2021-01-01\")\napt_rets = normal_rets(apt_stock_prices).dropna()\napt_rets.columns = ['returns']\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\nmsft_stock_prices.plot(figsize=(14,7), ax=ax1)\napt_stock_prices.plot(figsize=(14,7), ax=ax2)\nax1.get_legend().remove()\nax2.get_legend().remove()\nax1.title.set_text('MSFT Price Chart')\nax2.title.set_text('APT Price Chart')\nplt.show()\n```\n\n![png](portfolio-expected-return-and-risk/portfolio-expected-return-and-risk_4_0.png)\n\nThe graphs show some king of un-correlation. When one stock goes up the other goes down and vice versa. Let's explore the average return, standard deviation and correlation of the stocks.\n\n```python\nmsft_rets.mean().values[0], apt_rets.mean().values[0]\n```\n    (0.0017171460669071206, 0.009654517798428635)\n\n\n```python\nmsft_rets.std().values[0], apt_rets.std().values[0]\n```\n    (0.027679154652983044, 0.10987021868530256)\n\n```python\nreturns = msft_rets.merge(apt_rets, left_index=True, right_index=True)\nreturns.columns = ['MSFT', 'APT']\nreturns.corr()\n```\n\n<div>\n<table border=\"1\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>APT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MSFT</th>\n      <td>1.0000</td>\n      <td>-0.2182</td>\n    </tr>\n    <tr>\n      <th>APT</th>\n      <td>-0.2182</td>\n      <td>1.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\nObviously, both stocks yield a positive average daily return (small but positive), and while MSFT has a volatility around ~2.8%, APT is at ~11%, which denotes a very volatile asset. As expected, the correlation of the two assets is negative.\n\nLet us now try to construct a portfolio of these two assets. From equations (1) and (2) we see that the weights are variable. We should also notice that the return is not a series of returns anymore but a single value. This value is the total return of an asset over the year. It is the so called [`Annualized Return`](/post/geometric-progression-and-compounding-of-returns).\n\n\n```python\ndef annualize_rets(r, periods_per_year):\n    compounded_growth = (1+r).prod()\n    n_periods = r.shape[0]\n    return compounded_growth**(periods_per_year/n_periods)-1\n\nannualized_returns = annualize_rets(returns, 252)\nannualized_returns\n```\n    MSFT    0.399429\n    APT     2.222543\n    dtype: float64\n\n\nAs you can see the annual return for 2020 for MSFT was ~40%, while for APT was ~220%! It is pretty obvious from the price graphs :)\n\nNow, we move on and try to generate some portfolios where we assign different weights to the assets and try to calculate the return and the volatility of the portfolio. I will not get into what transposing a matrix means in algebra since it is not the focus of this post. Please check [this wikipedia article](https://en.wikipedia.org/wiki/Transpose) for more info.\n\n\n```python\n# from equation (1)\ndef portfolio_return(weights, returns):\n    return weights.T @ returns\n\n# from equation (2)\ndef portfolio_vol(weights, covariance_matrix):\n    return (weights.T @ covariance_matrix @ weights)**0.5\n\n# first we construct 10 pairs of weights like [(0.1,0.9), (0.2,0.8) ...]\nweights = [np.array([w, 1-w]) for w in np.linspace(0, 1, 10)]\n\n# then we calculate the return of the portfolio for each pair of weights\nportfolio_returns = [portfolio_return(w, annualized_returns) for w in weights]\n\n# and the volatility of the portfolio for each pair of weights\nvols = [portfolio_vol(w, returns.cov()) for w in weights]\n\nef = pd.DataFrame({\n    \"Return\": portfolio_returns, \n    \"Volatility\": vols,\n    \"weights\": weights\n})\n\nax = ef.plot(x=\"Volatility\", y=\"Return\", style=\".-\", figsize=(11,6),\n             title=\"2 Asset Portfolio Risk/Return\", legend=False)\nplt.ylabel(\"Return\")\n\ndef label_point(x, y, val, ax):\n  a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n  for i, point in a.iterrows():\n    prettified_p = f\"({round(point['val'][0], 2)},{round(point['val'][1], 2)})\"\n    ax.text(point['x'], point['y'], prettified_p)\n\nlabel_point(ef.Volatility, ef.Return, ef.weights, ax)\n```\n  \n![png](portfolio-expected-return-and-risk/portfolio-expected-return-and-risk_12_0.png)\n\nWhat the graph above tells us is that by combining the two assets we are able to achieve a total volatility (risk) that is less than each asset's individual volatility!\n\nObserve the left most point on the graph!\n\nIn a next post we will calculate the optimal weights that minimize the risk of a portfolio as well as explore portfolios with more than 2 assets.\n\n## Some More Notes\n\n* The approach I followed above is not something new. Is called [Markowitz Model](https://en.wikipedia.org/wiki/Markowitz_model) and won a [Nobel Price](https://www.nobelprize.org/prizes/economic-sciences/1990/press-release/) in 1990.\n* I tried to oversimplify the example, just to show the basics.\n* I randomly picked the two assets, in the example, from [IMPACTOPIA](http://www.market-topology.com/correlation/MSFT?etf=0).\n* We will prove, later on, that volatility changes over time :) and that would normally lead to rebalances.\n* As always, `Historical returns are no guarantee of future returns.`\n\nStay tuned ...\n"},{"slug":"global-market-insights-after-the-pandemic","frontmatter":{"title":"Global Market Insights after the Pandemic.","description":"What are the projections after the Pandemic? Let's explore ...","date":"January 21, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/after-covid-19.jpeg","colab":"https://colab.research.google.com/drive/1nPyDupJHidnP5DLfAEKX3eYiY-MHCDmM?usp=sharing"},"excerpt":"","content":"\n2020 was a difficult year and most of the market segments diverged from the projections! With the new facts at hand, and the emerging needs after the pandemic, we should expect changes in the market.\n\nBelow is an attempt to get a feeling of the emerging sectors by analyzing some [Global Market Insights](https://www.gminsights.com/) reports.\n\nAnd as always, let's automate ...\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n!pip install requests beautifulsoup4\nimport requests\nimport re\nfrom bs4 import BeautifulSoup\nimport string\nimport pandas as pd\n```\n\n</p>\n</details>\n\n```python\nclass GlobalMarketInsights:\n  __DEFAULT_BASE_URL = 'https://www.gminsights.com/industry-reports'\n  \n  @staticmethod\n  def _escape(input):\n    printable = string.ascii_letters + string.digits + string.punctuation + ' '\n    return ''.join(c if c in printable else ' ' for c in input )\n\n  def _description_matcher(self, descr):\n    descr = GlobalMarketInsights._escape(descr.replace('\\t', ' ').replace('\\n', ' ').replace('\\r', ' '))\n\n    start_date = end_date = percentage = market_name = None\n\n    r1 = re.search('^(.*) (?:Market|Aftermarket) .*$', descr, re.IGNORECASE)\n    if r1:\n      market_name = r1.group(1).strip()\n\n    r2 = re.search('.* between (\\d+) (?:and|to) (\\d+)', descr)\n    if r2:\n      start_date = r2.group(1).strip()\n      end_date = r2.group(2).strip()\n    \n    r3 = re.search('.* (?:from|of) (\\d+) to (\\d+)', descr)\n    if r3:\n      start_date = r3.group(1).strip()\n      end_date = r3.group(2).strip()\n    \n    r4 = re.search('([-+]?\\d*\\.\\d+|\\d+)%', descr)\n    if r4:\n      percentage = r4.group(1)\n    \n    if None in (market_name, percentage, start_date, end_date):\n      raise Exception(f\"Couldn't parse: {descr}\")\n    else:\n      return {\n        \"market\": market_name,\n        \"percentage\" : float(percentage),\n        \"start\": int(start_date),\n        \"end\": int(end_date)\n      }\n\n  def get(self, page=1):\n    page = requests.get(f\"{GlobalMarketInsights.__DEFAULT_BASE_URL}?page={page}\")\n    soup = BeautifulSoup(page.text, 'html.parser')\n    single_rds = soup.find_all('div', class_='single_rd')\n    reports = []\n    for single_rd in single_rds:\n      single_rd_children = single_rd.findChildren()\n      for single_rd_child in single_rd_children:\n        if single_rd_child.has_attr('class') and single_rd_child['class'][0] == 'rd_desc':\n          description = single_rd_child.getText()\n          try:\n            reports.append(self._description_matcher(description))\n          except Exception as e:\n            # print(e)\n            pass\n          break\n    return reports\n  \n  def fetch_all_reports(self):\n    # get the total number of pages and start iterating\n    page = requests.get(f\"{GlobalMarketInsights.__DEFAULT_BASE_URL}?page=1\")\n    lun_q = 'Displaying \\d+ records out of (\\d+) on Page \\d+ of (\\d+)'\n    r = re.search(lun_q, page.text)\n    if r:\n        number_of_records = r.group(1)\n        number_of_pages = r.group(2)\n    else:\n      raise Exception('No pages or data!')\n    \n    all_reports = []\n    for page in range(1, int(number_of_pages) + 1, 1):\n      page_reports = self.get(page=page)\n      all_reports += page_reports\n\n    return int(number_of_records), all_reports\n```\n\nScraping web pages is always challenging. In this case especially, the task was a bit tedious since the different report descriptions where not following a unique pattern.\n\n```python\nglobal_market_insights = GlobalMarketInsights()\nnumber_of_records, all_reports = global_market_insights.fetch_all_reports()\nprint(f\"Parsed {len(all_reports)} out of {number_of_records} report descriptions!\")\n```\n    Parsed 1200 out of 1964 report descriptions!\n\nNext, we add the reports to a dataframe for better presentation and easier data manipulation.\n\n```python\ngmi_reports_df = pd.DataFrame(all_reports) \ngmi_reports_df.head()\n```\n\n<div>\n<table border=\"1\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>market</th>\n      <th>percentage</th>\n      <th>start</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Food phosphate</td>\n      <td>6.0</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Supply Chain Analytics</td>\n      <td>16.0</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Cooking Coconut Milk</td>\n      <td>8.5</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Steel Rebar</td>\n      <td>4.0</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2,5-Dimethyl-2,4-Hexadiene</td>\n      <td>2.5</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nSo far, so good! Let's try to sort by percentage and see which sector is projected to perform more than 30% the following years.\n\n```python\nsector_projection_ascending = gmi_reports_df.sort_values('percentage', ascending=False)\nsector_projection_ascending.loc[(sector_projection_ascending['percentage']>30) & (sector_projection_ascending['start']>=2020)]\n```\n\n<div>\n<table border=\"1\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>market</th>\n      <th>percentage</th>\n      <th>start</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>435</th>\n      <td>SD-WAN</td>\n      <td>60.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>Cannabidiol (CBD)</td>\n      <td>52.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>(Light Fidelity) Li-Fi</td>\n      <td>50.0</td>\n      <td>2020</td>\n      <td>2030</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Healthcare Artificial Intelligence</td>\n      <td>43.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>Automotive Subscription Services</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>AI in Manufacturing</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>363</th>\n      <td>Artificial Intelligence (AI) in BFSI</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Robotic Process Automation</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>Fuel Cell Electric Vehicle</td>\n      <td>38.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>AI in Automotive</td>\n      <td>35.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>Artificial Intelligence Chipsets</td>\n      <td>35.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>Total Knee Replacement</td>\n      <td>34.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>212</th>\n      <td>Vaginal Rejuvenation</td>\n      <td>33.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>Carbon Wheels</td>\n      <td>32.3</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nIt is becoming pretty obvious that everything around Artificial Intelligence yields the best projections, and is an attractive area for investments :)\n"},{"slug":"are-stock-returns-normally-distributed","frontmatter":{"title":"Are Stock Returns Normally Distributed?","description":"What do you think?","date":"December 20, 2020","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/normal-dist-with-hist.png","colab":"https://colab.research.google.com/drive/1iYrNJ9ISktohy1dG2s16_FZKakB8FLU5?usp=sharing"},"excerpt":"","content":"\nIn a previous post we talked about the [Higher Moments of a Distribution](/post/higher-moments-of-a-distribution). We saw that skewness and kurtosis are two attributes that can identify if a distribution is normal or not (skewnes = 0 & kurtosis = 3).\n\nLet's try this approach on the MSFT stock.\n\nFirst step is to to fetch the data and print the returns.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n\nmatplotlib.rcParams['figure.figsize'] = (10.0, 5.0)\nmatplotlib.style.use('ggplot')\n```\n\n</p>\n</details>\n\n```python\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n\ndef log_rets(S):\n    rets = np.log(S) - np.log( S.shift(1))\n    return rets[1:]\n\nstock_prices = retrieve_stock_data(\"MSFT\", \"2000-01-01\", \"2020-01-01\")\n\nrets = normal_rets(stock_prices).dropna()\nrets.columns = ['returns']\nrets.plot(figsize=(14,7))\nplt.title(\"Daily returns\", weight=\"bold\");\n```\n\n![png](are-stock-returns-normally-distributed/are-stock-returns-normally-distributed-1-1.png)\n\nLet's find skewness and kurtosis:\n\n```python\nfrom scipy.stats import kurtosis, skew\nskew(rets, bias=False)[0], kurtosis(rets, bias=False, fisher=False)[0]\n```\n    (0.20887713542026032, 13.229622042763442)\n\nIt is obvious that the MSFT stock returns (for that period) do not comply with the kurtosis and skewness of a normal distribution. Same, of course, happens if we get the log returns instead.\n\n```python\nlog_msft_rets = log_rets(stock_prices).dropna()\nskew(log_msft_rets, bias=False)[0], kurtosis(log_msft_rets, bias=False, fisher=False)[0]\n```\n    (-0.12981025399984283, 12.81366131030108)\n\n## Normality Tests\n\nThere are several interrelated approaches to determining normality:\n\n* Histogram with the normal curve superimposed. Unfortunately, there is no automated way to represent the \"fitness\" as a value. This approach is empirical mostly and requires experience.\n* Skewness & Kurtosis Tests.\n* Normality plots. “Normal Q-Q Plot” provides a graphical way to determine the level of normality.\n* Normality tests. The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above .05 indicates normality. If the test is significant (less than .05), then the data are non-normal.\n\n### Histogram & Normal PDF\n\n```python\nfrom scipy.stats import norm\nx = np.linspace(min(rets.returns.values), max(rets.returns.values))\nax = rets.plot(kind='hist', bins=500, density=True)\npdf_fitted = norm.pdf(x, *norm.fit(rets.returns.values))\npd.Series(pdf_fitted, x).plot(ax=ax)\nplt.show()\n```\n    \n![png](are-stock-returns-normally-distributed/are-stock-returns-normally-distributed-8-0.png)\n\n### Skewness & Kurtosis Tests\n\n```python\nfrom scipy import stats\nstats.kurtosistest(rets.returns)\n```\n    KurtosistestResult(statistic=29.93227785492693, pvalue=7.484189304773088e-197)\n\n```python\nstats.skewtest(rets.returns)\n```\n    SkewtestResult(statistic=5.99114785753993, pvalue=2.083650895527666e-09)\n\n### QQ-Plot\n\n```python\nfrom numpy.random import seed\nfrom statsmodels.graphics.gofplots import qqplot\nfrom matplotlib import pyplot\nseed(1)\nqqplot(rets.returns, line='s')\npyplot.show()\n```\n    \n![png](are-stock-returns-normally-distributed/are-stock-returns-normally-distributed-13-0.png)\n    \n\nThe Quantile-Quantile plot, as the name suggests, will compare the quantiles between the normal distribution and our data. We notice here that, the tails of the distribution of our data are diverging a lot from the normal distribution. This is what we would expect. Fat tails (leptokurtic)!\n\n### Statistical Normality Tests\n\nThe tests assume that the sample was drawn from a Gaussian distribution. Technically this is called the null hypothesis, or H0. A threshold level is chosen called alpha, typically 5% (or 0.05), that is used to interpret the p-value.\n\nIn the SciPy implementation of these tests, you can interpret the p value as follows.\n\n* p <= alpha: reject H0, not normal.\n* p > alpha: fail to reject H0, normal.\n\nThis means that, in general, we are seeking results with a larger p-value to confirm that our sample was likely drawn from a Gaussian distribution.\n\nA result above 5% does not mean that the null hypothesis is true. It means that it is very likely true given available evidence. The p-value is not the probability of the data fitting a Gaussian distribution; it can be thought of as a value that helps us interpret the statistical test.\n\n#### Kolmogorov-Smirnov test (K-S)\n\n\n```python\nkstest = stats.kstest(rets.returns, 'norm')\nkstest.pvalue > 0.05\n```\n    False\n\n#### Shapiro-Wilk Test\n\n```python\nshapiro_stat, shapiro_p = stats.shapiro(rets.returns)\nshapiro_p > 0.05\n```\n    False\n\n\n#### D’Agostino’s K^2 Test\n\nThe D’Agostino’s K^2 test calculates summary statistics from the data, namely kurtosis and skewness, to determine if the data distribution departs from the normal distribution. (named for Ralph D’Agostino)\n\n* Skew is a quantification of how much a distribution is pushed left or right, a measure of asymmetry in the distribution.\n* Kurtosis quantifies how much of the distribution is in the tail.\n\nIt is a simple and commonly used statistical test for normality.\n\n```python\nseed(1)\ndagostino_stat, dagostino_p = stats.normaltest(rets.returns)\ndagostino_p > 0.05\n```\n    False\n\n#### Jarque-Bera Test for Normality\n\n```python\njarque_bera_stat, jarque_bera_p = stats.jarque_bera(rets.returns)\njarque_bera_p > 0.05\n```\n    False\n\n## Conclusion\n\nIn this article we went through some techniques that allow us identify if stock returns are normally distributed. We saw, with examples, that returns (arithmetic, or log) are not normally distributed but instead exhibit fat tails. We cannot generalize, of course, just by looking into one stock, but I will leave that as a small exercise to the curious readers.\n\nThe question is still... Since the returns are not following a normal distribution, then what type of distribution do they follow?\n\nThe answer to that in [Fit Multiple Distributions to Asset Returns!](/post/fit-distributions-to-asset-returns)\n"},{"slug":"higher-moments-of-a-distribution","frontmatter":{"title":"Higher Moments of a Distribution","description":"See how higher moments can reveal more characteristics of a data series.","date":"December 17, 2020","topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/skewness-kurtosis.png","colab":"https://colab.research.google.com/drive/1k2Ek0o9UpV2f7NcS0R6aZ36UDvR24BRP?usp=sharing"},"excerpt":"","content":"\nWe have already discussed about the [mean](/post/measures-of-location) and the [variance](/post/measures-of-variability) of a series of data. \n\nMean is also called the 1st moment and variance the 2nd moment. The type to get a moment (the movement) about a non-random value c, of a density function is:\n\n$$\nE[(X-c)^κ] = \\int_{-\\infty}^{+\\infty} (x-c)^k f(x) dx  \\qquad (1)\n$$\n\nBefore explaining the moments, we should first understand what a density function is. Commonly called probability density function (PDF).\n\n## Density Function\n\n30 people are gathered in a house party. Let us measure their weights:\n\n\n```python\nimport pandas as pd\nm_v = [56.8, 81.3, 47.9, 32.5, 24.1, 25.3, 14.3, 29.4, 71.3, 86.0, 54.2, 15.2,\n       54.7, 25.1, 49.5, 1.9, 70.0, 69.6, 75.4, 38.9, 49.2, 22.5, 68.6, 60.1,\n       52.7, 109.7, 38.9, 45.9, 47.7, 52.9]\nvalues = pd.Series(m_v)\nvalues.describe()\n```\n    count     30.000000\n    mean      49.053333\n    std       24.001634\n    min        1.900000\n    25%       30.175000\n    50%       49.350000\n    75%       66.475000\n    max      109.700000\n    dtype: float64\n\n\n\nWhat if we change the way we present the data, and instead of having them in a simple series, we try to split them up into buckets.\n\nWe will get the, so called, histogram of the values. It shows how the probabilities of measurement are distributed.\n\n\n```python\nimport matplotlib.pyplot as plt\nhistogram = values.plot.hist(bins=10, figsize=(10,5))\nplt.show()\n```\n  \n![png](higher-moments-of-a-distribution/higher-moments-of-a-distribution_3_0.png)\n    \n\nIf we ask the question: What is the probability, the next person that joins the party, weights between 80 and 90 kilos?\n\nTo answer this question, we need to imagine as if the upper boundaries of the blue colored space above, are a continuous line, a curve. This curve is what we call PDF or density function.\n\n\n```python\nfrom scipy.stats import norm\nimport numpy as np\nx = np.linspace(min(values), max(values))\nax = values.plot(kind='hist', bins=10, figsize=(10,5), density=True)\npdf_fitted = norm.pdf(x, *norm.fit(values))\npd.Series(pdf_fitted, x).plot(ax=ax)\nplt.show()\n```\n    \n![png](higher-moments-of-a-distribution/higher-moments-of-a-distribution_5_0.png)\n\nWe observe that the curve is not a perfect fit. It is an approximation and there are hundreds of different curves we can plot and several of them will be very close to fitting the data (I will show that in another post).\n\nIn this case above, I have intentionally picked the data as such to resemble the, so called, `normal` distribution.\n\nBack to our question now! The probability, the next person that joins the party, weights between 80 and 90 kilos, can be estimated by measuring the area below the curve for that bucket. So, if the whole area below the curve is 1 (zero moment, see below), the part that belongs to bucket 80-90 is a percentage :) and that is the probability we are after. The expression is:\n\n$$\nP( \\text{weight between 80 and 90 kilos} | \\text{mean=x and standard-deviation=y} )\n$$\n\nWhich is translated to: The probability a person weights between 80 and 90 kilos given an average of x and standard-deviation of y.\n\nAnd the area below the curve is the integral between the points:\n\n$$\nP = \\int_{x=80}^{x=90} f(x)dx \\text{  where f the density function}\n$$ \n\nThere are many pros in trying to use distributions to represent how the values in a dataset are distributed:\n\n* makes it easy to measure the areas below (with integrals, since the function of the curve is known)\n* the presentation is much better for the human\n* well known distributions have really nice properties\n\n## Zero Moment (Total Mass)\n\nThat means that in (1), k=0 and as such $(...)^0 = 1$. That leaves us with:\n\n$$\n\\int_{x=-\\infty}^{x=\\infty} f(x)dx = 1 \\qquad (2)\n$$\n\n> Probability Distributions are normalized quantities, that always sum to one. Think of that as the probability that at least one of the events in a sample space will occur. Isn't that 100%?\n\n## 1st Moment - Mean\n\n$$\nμ_1 = E[(X-0)^1] = E[X] = \\int_{-\\infty}^{+\\infty} xf(x)dx  \\qquad (3)\n$$\n\nc=0 in this case since we do not have an origin to get the moment (movement) about.\n\nFrom (3) is obvious that we talk about the mean and that alternatively talk about the balance of the total mass (the area below the curve) around a point :)\n\n## 2nd Moment - Variance\n\nFrom (1), we can take c=0 and k=2! But what will that show us? How the mass is balanced around again the same point, which in practice is the average again but squared? Doesn't provide much value in understanding our data.\n\nFor that reason we get $c=μ$ and that will start making sense, since we se how the mass is diverging from the mean. It will show the variance of the data around the mean :)\n\n$$\nVar = \\int_{-\\infty}^{+\\infty} (x-μ_x)^2f(x)dx  \\qquad (4)\n$$\n\n## 3rd Moment - Skewness\n\nFollowing the pattern above and using k=3 around the mean $c=μ$ then we get the skewness which measures the relative size of the two tails of a distribution.\n\n\n```python\nfrom scipy.stats import skew\nskew(values, bias=False) # bias=False calculates the skewness and kurtosis of the sample as opposed to the population.\n```\n    0.274192939649461\n\n\nA left-skewed (negatively-skewed) distribution has a long left tail. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA right-skewed (positive-skew) distribution has a long right tail. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n![png](higher-moments-of-a-distribution/Relationship_between_mean_and_median_under_different_skewness.png)\n\n## 4th Moment - Kurtosis\n\nThe fourth central moment is a measure of the heaviness of the tail of the distribution.\n\n```python\nfrom scipy.stats import kurtosis\nkurtosis(values, bias=False)\n```\n    0.14330737818315065\n\n![jpg](higher-moments-of-a-distribution/kurtosis-types.jpg)\n\n## Higher Moments of the Normal Distribution\n\n```python\ndata = np.random.normal(0, 1, 10000000)\nplt.hist(data, bins='auto')\n\nprint(\"mean : \", np.mean(data))\nprint(\"var  : \", np.var(data))\nprint(\"skew : \", skew(data, bias=False))\nprint(\"kurt : \", kurtosis(data, bias=False, fisher=False))\n```\n\n    mean :  -0.00015674618345404924\n    var  :  1.0002202373014222\n    skew :  0.0007495220785926886\n    kurt :  3.0013415645199695\n \n![png](higher-moments-of-a-distribution/higher-moments-of-a-distribution_12_1.png)\n    \n\nFor a normal distribution the skewness is zero and the kurtosis is 3. These properties are specific to the normal distribution and are used for normality testing of distributions. We will go deeper in that in a later post.\n"},{"slug":"load-google-drive-folder-in-google-colab","frontmatter":{"title":"Load Google Drive folder in Google Colab","description":"How to mount a Google Drive in Google Colab and load some stock data.","date":"December 7, 2020","topic":{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/google-colab.png"},"excerpt":"","content":"\nThere are several cases where:\n\n* fetching stock prices is not possible through python libraries like `yahoofinancials` or other APIs\n* you want to load the same data over and over again (for parallelization)\n* you want to use you python modules without publishing them to a registry\n\nIn these cases I find it very handy to store my data (csv format) and my modules in google drive.\n\nHowever, loading the data to Google Colab turned into pain since I had to manually upload the files each time I wanted to run a notebook.\n\nTo avoid this situation I mount google drive and:\n1. add the folder with my python modules to the path\n2. copy the data to the Colab data folder\n\n```python\nimport warnings\nwarnings.simplefilter('ignore')\n\n%config InlineBackend.figure_formats=[\"png\"]\n\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nimport os\nimport sys\nimport glob\nfrom shutil import copytree, copy\n\n# This will add my python modules in the path\ngdrive_base_path = '/content/drive/My Drive/my-python-modules'\nsys.path.append(gdrive_base_path)\n\ntry:\n  # copy the data we need\n  copytree('/content/drive/My Drive/Colab Notebooks/data', '/content/data')\nexcept Exception as e:\n  pass\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as st\n```\n\nSince the folder that contain my python modules is now in the system path, I'm able to\n\n```python\nimport my_module\n```\n\nand whenever I change my modules (add more functionality, improvements), it practically saves it to Google Drive, since it is a pure mount.\n\nWhen it comes to loading data, then I simply:\n\n```python\nMSFT = pd.read_csv('data/msft_daily.csv', parse_dates=True, index_col=0, header=0)\n```\n\n> **Note**: Since `copytree` is used, uploading new data files to Google Colab, will not automatically save it to Google Drive!\n"},{"slug":"geometric-progression-and-compounding-of-returns","frontmatter":{"title":"Geometric Progression and the Compounding of the Returns","description":"Your savings account offers a 1% annual interest! The account balance in 10 years? +10%? Naaaah","date":"December 6, 2020","topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/fractal-star.png","tags":[]},"excerpt":"","content":"\n## Compounding of Returns\n\nLet us consider the following scenario. An investment yield an average annual return R of 2% for the last 5 years. If we had invested 100\\$ 5 years ago, what would be the outcome of this investment today?\n\nConsider an initial investment amount $P_0 = 100$. After the first year, the outcome would be:\n\n$$\nP_1 = 100 + (100 * 0.02) = (100 * 1) + (100 * 0.02) = 100 * (1 + 0.02) = P_0 * 1.02\n$$\n\nThe outcome after the 2nd year would be:\n\n$$\nP_2 = P_1 + (P_1 * 0.02) = P_1 * 1.02\n$$\n\n3rd year:\n\n$$\nP_3 = P_2 + (P_2 * 0.02) = P_2 * 1.02\n$$\n\n(ν-1)th year:\n\n$$\nP_{ν-1} = P_{ν-2} + (P_{ν-2} * 0.02) = P_{ν-2} * 1.02\n$$\n\nνth year:\n\n$$\nP_ν = P_{ν-1} + (P_{ν-1} * 0.02) = P_{ν-1} * 1.02\n$$\n\nIt becomes obvious from the above that generating each element in the sequence $P_0, P_1, ..., P_{ν-1}, P_ν$ follows a pattern where each occurrence is generated by multiplying the previous number with a constant.\n\nIn our case, the constant is the average annual return plus 1 (1+R). If we combine the previous equations in an attempt to formulate a final equation that provides the νth occurrence based on the $P_0$ element then:\n\n$$\nP_ν = P_{ν-1} * 1.02 = (P_{ν-2} * 1.02) * 1.02 = P_{ν-2} * 1.02^2 = .... = P_0 * (1 + 0.02)^ν\n$$\n\nSo, after 5 years we will have $100 * 1.02^{5} = 110.41$!\n\nThe generic equation is:\n\n$$\nP_ν = P_0 * (1 + R)^ν \\qquad (1)\n$$\n\n## Geometric Progression\n\nIn calculus, a sequence of numbers where each term after the first is found by multiplying the previous one by a fixed, non-one number, called `geometric progression`. The fixed number is called, the `common ratio`.\n\n$$\na, ar, ar^2, ... , ar^{n-1}\n$$\n\nwhere $1+R$ is the `common ratio` and $a$ is the `coefficient`.\n\nSome interesting properties to notice:\n\n* a common ratio greater than 1, will produce exponential growth towards positive infinity!\n* a common ratio greater than 0 and up to 1, will produce exponential decay towards zero!\n\nIn finance, the periodic reinvestment on a rate is called **Compounding of Returns**.\n\n## From cumulative to periodic returns\n\nLet's see the example above from another angle. Let's say that an investment had a total return over the last 5 years of 10%. What was the average annual return of this investment?\n\nThe cumulative return (`pandas.pct_change`) is given by the type:\n\n$$\nR_i = \\frac{P_i-P_{i-1}}{P_{i-1}} = \\frac{P_i}{P_{i-1}} - 1 \\qquad (2)\n$$\n\nwhere $P_i$ is the price of the investment at period $i$.\n\nThe equation (2) can be written as $P_i = P_{i-1} * (1 + R_i)$. Or $P_{end-of-investment} = P_{start-of-investment} * (1 + R_{total})$.\n\nThe equation (1) will take the form,\n\n$$\nP_{end-of-investment} = P_{start-of-investment} * (1 + R_{annual})^{5}\n$$\n\nCombining them,\n\n$$\n1 + R_{total} = (1 + R_{annual})^{5} => R_{annual} = \\sqrt[5]{1+R_{total}} - 1\n$$\n\nWe use ν = 5, due to 5 compounding periods. If instead of annual returns we were asked to find semiannual returns then we would use ν = 10 ($R_{semiannual} = \\sqrt[10]{1+R_{total}} - 1$).\n\nFinally,\n\n$$\nR_{annual} = \\sqrt[5]{1 + 0.1} - 1 = \\sqrt[5]{1.1} - 1 = 1.01927 - 1 = 0.01927\n$$ \n\nSo, an annual return of 1.9% will produce a 10% return at the end of the 5 years.\n\nTo generalize\n\n$$\nR_{periodic} = \\sqrt[number-of-periods]{1+R_{total}} - 1 \\qquad (3)\n$$\n\n## Compounding variable returns\n\nIt is commonly accepted that returns do not stay the same over periods. For example, the average return of this month is not the same as the one from last month! However, the same principle of compounding applies in this case too. Let's see an example.\n\nAn investment yield the following returns for the past couple of months $0.021, 0.032, -0.018, 0.06, -0.043, 0.048$. The total return is the **product** of the individual returns when 1 is added to them:\n\n$$\nR = (1 + 0.021) * (1 + 0.032) * (1 - 0.018) * (1 + 0.06) * (1 - 0.043) * (1 + 0.048) = ...\n$$\n\nThis, resonates with the equation (1) above where for a fixed return we have $R_{total} = (1 + R_{fixed})^{number-of-periods}$\n"},{"slug":"measures-of-variability","frontmatter":{"title":"Measures of Variability","description":"Standard deviation & percentiles! Used for measuring volatility, and allow for a rough \"estimate\" of the near future.","date":"November 1, 2020","topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"tags":[{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/standard-deviation.png"},"excerpt":"","content":"\n[Measures of location](/post/measures-of-location) cannot identify how well average, mode, etc. represent the data. They fail to answer questions like:\n\n* How close are the values to each other?\n* What is the largest difference between the values?\n* On average, how far are the values from each other?\n\nMeasures of variability (also called spread) fill this gap. The most common are:\n\n* Standard Deviation (SD)\n* Range, which is the largest value minus the smallest value\n* Interquartile Range (IQR) which is the upper quartile (75th percentile), minus the lower quartile (25th percentile)\n\nIn investing, Standard Deviation is one of the most common methods for determining the risk of an investment (also called `volatility`).\n\n## Variance ($Var$ or $\\sigma^2$) & Standard Deviation ($\\sigma$)\n\nVariance is the average squared difference from the mean, or alternatively, how spread out the data are around their mean.  \n\n$$\nVar(x) = \\sigma^2 = \\sigma_x^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}\n$$\n\n> **Note**: We can divide by n or n-1. n is the total population of an event. n-1 a series of it. numpy is, by default, using n-1! The difference is negligible for long sequences, but not for short ones!\n\nWe see above that, for some reason, we squared the difference ($(x_i - \\bar{x})^2$), instead of just taking the absolute difference ($x_i - \\bar{x}$). We did that in order to avoid the [same problem](/post/measures-of-location) we have with the mean, where the sum can be zero because of negative values in the list. Imagine, for example, having the sequence `-10,1,2,3,4`. What is the mean value? 0! And, how spread are the data around the mean? 0 again. It doesn't really make much sense, right? By observing the sequence, we see that there is some spreading of the data around \"something\". This, \"something\" doesn't feel to be 0. It feels like something close to the `1,2,3,4` values, which look like a group of values that are really close to each other. I know that is kinda difficult to grasp. But, not until we raise it to another \"dimension\" :). And that is what we do when we square the values.\n\n* we remove the negative sign\n* we make the values that are far from \"the group\", to stand out (the outliers are emphasized).\n* we change the unit of measurement (it is now something much bigger). Like another dimension!\n\nTo return back to the unit we started with, but meanwhile keep the notion of spreading, we use the RMS ([Root Mean Square](/post/measures-of-location)) of all the distances to the mean. We call that Standard Deviation (SD): \n\n$$\n\\sigma_x = \\sqrt{Var(x)}\n$$\n\n\n### Standard Deviation in Investing\n\nStandard Deviation measures the volatility of an asset. Large values mean that the asset's price can diverge a lot from the mean value. Thus, an indication of higher risk. On the flip side, low volatility may indicate low risk. \n\n> A critical point to remember is that standard Deviation is a backward looking tool and not a guarantee of future moves.\n\nWhile daily returns (usually log returns) of an asset are more like leptokurtotic distributions (i.e. exhibit fat tails), we can add some abstraction by saying that are normally distributed (not to measure actual risk, but to get an initial approximation of risk). \n\nNormal distributions have some very unique attributes. The most important are:\n\n* The center of a normal distribution is located at its peak, and 50% of the data lies above the mean, while 50% lies below. The mean, median, and mode are all equal\n* Approximately 68% of the data lies within 1 SD of the mean, Approximately 95% of the data lies within 2 SD of the mean, Approximately 99.7% of the data lies within 3 SD of the mean. This is known as `The empirical rule`.\n\n![png](measures-of-variability-normal-distribution.png)\n\nLet's assume that the average price of an asset is 40\\$ (measured with daily returns for some period) and the standard deviation is 5\\$. We can assume with 95% certainty the next closing price remains between 30\\$ and 50\\$! (More on that in a later post). In other words, we are 95% sure that the max loss will not exceed 10\\$.\n\n> Standard Deviation is considered a pure measure of risk (due to returns distribution deviate from the normal one), but is a good indicator of how volatile as asset is. \n\n## Percentiles\n\nWe find the percentile by ordering all the values in a dataset from smallest to largest. Then we multiply the number of values by the percentile we want. This, will give us the index of this percentile in the dataset. The value at this index is the requested percentile.\n\nFor example, suppose you have 12 daily stock return, ordered from lowest to highest: `2.3, 2.7, 3.1, 3.5, 3.9, 4.6, 4.7, 5, 5.1, 5.2, 5.4, 9`. To find the 90th percentile for these (ordered) returns, start by multiplying 90% times the size of the dataset, which gives 90% * 12 = 0.90 * 12 = 10.8 (the index). Rounding up to the nearest whole number, you get 11.\n\nCounting from left to right (from the smallest to the largest value in the dataset), you go until you find the 11th value in the dataset. That value is 5.4, and it’s the 90th percentile for this dataset.\n\nSomeone can think of the percentile as a lower dimension of the quantile. And the quartile a lower dimension of the percentile. This, because a quantile (which spans to any type of quantity) can take any upper value (like ex. 1500). The percentile (cent=100) is bound between 0-100. And the quartile (quarter) bound between 1-4. \n\nThus:\n\n* The 25th percentile is also called the first quartile.\n* The 50th percentile is generally the median.\n* The 75th percentile is also called the third quartile.\n* The difference between the third and first quartiles is the interquartile range.\n\n### Percentiles in Investing\n\nA simple example of how percentiles can be used in Investing was introduced in the previous section, where we asked, how sure we were that a loss will not exceed a value. Questions like that can be asked in numerous ways, and in the majority of the cases involve some sort of percentile representation."},{"slug":"measures-of-location","frontmatter":{"title":"Measures of Location","description":"Use average, median, mode and more, to understand basic investing.","date":"October 18, 2020","tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"https://www.picpedia.org/highway-signs/images/average.jpg"},"excerpt":"","content":"\nIn investing, we are often presented with the challenge to analyze a data set (with historical prices of stocks, funds, etc.). \n\nWe are asked to answer questions like:\n\n* What is the average return of a stock the last x days (given the daily returns)?\n* What is the return of a stock that occurred the most the last x days?\n* etc. \n\nTo answer, we commonly start by exploring some attributes that best describe the data. We often call them [`Measures of Location`](https://www.encyclopedia.com/computing/dictionaries-thesauruses-pictures-and-press-releases/measures-location).\n\nThe most common measures of location are the Mean, Median and Mode.\n\n### Mean (or **Average**, or **Expected Value**) and Root Mean Square (RMS)\n\nMean is the sum of the values of the data points divided by the number of data points. That is,\n\n$$ \n\\bar{Y} = \\frac {\\sum_{i=1}^N Y_{i}}{N},\n$$\n\nBut, what if we have a sequence like `0\t4\t1\t-4\t-1`? The Mean will be 0! In that case we reside to the RMS which is \n\n$$\n\\bar{Y}_{rms} = \\sqrt \\frac {\\sum_{i=1}^N Y_{i}^2}{N},\n$$\n\n### Median\n\nIs the value of the point which has half the data smaller than that point and half the data larger than that point.\n\n$$\n\\tilde{Y} = Y_{\\frac {N+1}{2}}, \\text{if } N = odd\n$$\n\n$$\n\\tilde{Y} = \\frac {Y_{\\frac {N}{2}} + Y_{\\frac {N}{2} + 1}}{2}, \\text{if } N = even\n$$\n\n### Mode\n\nIs the value of the random sample that occurs with the greatest frequency (might not be unique).\n\n## Example\n\nLet us generate some random data to showcase the above. We use `random.normal` here, which generates normally distributed numbers (we will discuss about normality in a following article) between -10 and 10. In terms of Investing, think of it as the simple returns (return = the percentage change of the today's closing price, over the yesterday's closing price) of a stock over a period.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set() \n```\n\n</p>\n</details>\n\n```python\nrandomInts = np.random.normal(loc=10, scale=3, size=1000).astype(int)-10\n\ndf = pd.DataFrame(randomInts, columns=['Returns'])\ndf.plot();\n```\n   \n![png](measures-of-location/measures-of-location_3_0.png)\n\n### Mean\n\n```python\ndf.Returns.sum()/df.Returns.size\n```\n    -0.612\n\n```python\n# or\nmean = df.Returns.mean()\nmean\n```\n    -0.612\n\nWhich is the answer to: `What is the average return of a stock the last x days?`\n\n### Median\n\n```python\nnp.sort(df.Returns.values)[int(df.Returns.size/2)]\n```\n    -1\n\n```python\n# or\nmedian = np.percentile(df.Returns,50)\nmedian\n```\n    -1.0\n\n### Mode\n\n```python\nsample_data = [1,2,3,4,3,5,3,6,3,7,8,9]\nsample_data_df = pd.DataFrame(sample_data, columns=['Returns'])\nsample_data_df.Returns.mode()[0]\n```\n    3\n\nIt is apparent from the above that the number with the most frequent appearance is the number 3. That is because the numbers are discrete.\n\nBut, in a sequence of data that is continuous, the numbers can take any value in a range, which means decimal part (fractions). In cases like that it is difficult to find a single number that is present more often in the set. And that is normally the case in investing (the returns can take any value). Thus, we have to follow another approach and that is to separate the data into buckets. Here is where the notion of the [histogram](https://en.wikipedia.org/wiki/Histogram) comes into play and generates an outcome like:\n\n\n```python\ndf.plot.hist(bins=50, figsize=(12,6), grid=True);\n```\n    \n![png](measures-of-location/measures-of-location_14_0.png)\n\nWhat really happens is to order the values in ascending order and then separate them in a number of buckets. For example if the min value is 1 and the max 10 and we split them in 9 buckets, the first one will include all numbers from 1 to 2, the second all numbers from 2 to 3 and so on. The next step is to find the bucket with the highest amount of elements and take the middle value.  \n\n```python\ncounts, bins = np.histogram(df.Returns, bins=50)\nmax_index_col = np.argmax(counts, axis=0)\nmode = bins[max_index_col]\nmode\n```\n    -1.0\n\n```python\n# or\ndf.Returns.mode()[0]\n```\n    -1\n\n\n## Alternative Measures of Location\n\nIn addition to the more common measures, there are several more that can also be included to the investing algorithms. \n\n* Mid-Mean - computes a mean using the data between the 25th and 75th percentiles.\n* Trimmed Mean - similar to the mid-mean except different percentile values are used. A common choice is to trim 5% of the points in both the lower and upper tails, i.e., calculate the mean for data between the 5th and 95th percentiles.\n* Winsorized Mean - similar to the trimmed mean. However, instead of trimming the points, they are set to the lowest (or highest) value. For example, all data below the 5th percentile are set equal to the value of the 5th percentile and all data greater than the 95th percentile are set equal to the 95th percentile.\n* Mid-range = (largest + smallest)/2.\n\n### Mid-Mean\n\n```python\n# p_25 = df.Returns.quantile(0.25)  # Much slower than np.percentile\np_25 = np.percentile(df.Returns,25) # attention : the percentile is given in percent (5 = 5%)\np_75 = np.percentile(df.Returns,75)\nmid_mean = df[df.Returns.gt(p_25) & df.Returns.lt(p_75)].Returns.mean()\nmid_mean\n```\n    -1.010498687664042\n\n### Trimmed-Mean\n\n```python\np_5 = np.percentile(df.Returns,5)\np_95 = np.percentile(df.Returns,95)\ntrimmed_mean = df[df.Returns.gt(p_5) & df.Returns.lt(p_95)].Returns.mean()\ntrimmed_mean\n```\n    -0.5480427046263345\n\n### Winsorized Mean\n\n```python\ndata_indexes_to_stay_same = ( df.Returns > p_5 ) & ( df.Returns < p_95 )\ndata_to_stay_same = df[data_indexes_to_stay_same]\nall_data_bellow_p_5 = df[df.Returns <= p_5].copy()\nall_data_bellow_p_5.Returns.values[:] = p_5\nall_data_above_p_95 = df[df.Returns >= p_95].copy()\nall_data_above_p_95.Returns.values[:] = p_95\nwinsored_rets_list = [all_data_bellow_p_5, data_to_stay_same, all_data_above_p_95]\nwinsored_rets = pd.concat(winsored_rets_list)\nwinsored_mean = winsored_rets.Returns.mean()\nwinsored_mean\n```\n    -0.608\n\n### Mid-range\n\n```python\nmid_range = (df.Returns.max() + df.Returns.min())/2\nmid_range\n```\n    -1.0\n\n#### All Together\n\n```python\n# df.Returns.plot.hist(bins=200, figsize=(15,6), grid=True)\nplt.figure(figsize=(15,6))\nsns.histplot(df.Returns, kde=True, bins=100)\nplt.title(\"Measures of Location\")\nplt.xlabel(\"Utv\", labelpad=20, weight='bold', size=12)\nplt.ylabel(\"# of Utv in a bin\", labelpad=20, weight='bold', size=12)\nplt.axvline(x=mid_range, label='mid_range={:.3f}'.format(mid_range), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=mean, label='mean={:.3f}'.format(mean), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=median, label='median={:.3f}'.format(median), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=mode, label='mode={:.3f}'.format(mode), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=mid_mean, label='mid_mean={:.3f}'.format(mid_mean), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=winsored_mean, label='winsored_mean={:.3f}'.format(winsored_mean), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=trimmed_mean, label='trimmed_mean={:.3f}'.format(trimmed_mean), ymax=0.95, c=np.random.rand(3,))\nplt.gca().legend(loc=\"upper left\")\nplt.show()\n```\n\n![png](measures-of-location/measures-of-location_28_0.png)\n"}],"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg","slug":"python","count":19},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg","slug":"statistics","count":4},{"id":"cryptos","name":"cryptos","image":"bitcoin.png","description":"The amazing world of Blockchain opens one more chapter in the Investing.","color":"bg-green-300","icon":"bitcoin.svg","slug":"cryptos","count":1}],"sortedTopics":[{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300","slug":"investing","count":14},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300","slug":"automation","count":5},{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300","slug":"mathematics","count":4},{"id":"booknotes","name":"Book Notes","image":"booknotes.jpg","description":"Reading books, and keeping notes! I seek to what the books have to tell me.","color":"bg-green-300","slug":"booknotes","count":1}],"allTopics":[{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},{"id":"booknotes","name":"Book Notes","image":"booknotes.jpg","description":"Reading books, and keeping notes! I seek to what the books have to tell me.","color":"bg-green-300"}]},"__N_SSG":true}