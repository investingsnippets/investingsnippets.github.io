{"pageProps":{"postData":{"frontmatter":{"title":"Fit Multiple Distributions to Asset Returns!","description":"Why normal distribution is not preferred for stock returns analysis.","date":"March 28, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/normal-dist-with-hist.png","colab":"https://colab.research.google.com/drive/1p3KbU09vOrplisEzMFEQqGV5Jtuxr3FC?usp=sharing"},"post":{"content":"\nEarlier in [Are Stock Returns Normally Distributed?](post/are-stock-returns-normally-distributed) we went through different ways to validate when asset returns are normally distributed. While we used only one stock to prove that stock returns are not normally distributed, the phenomenon applies to any volatile asset, in general.\n\nThe question though is: Really, which distribution do returns follow?\n\nBelow we load the MSFT stock returns as we did before! \n\n\n```\n%%capture\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\n\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n\nstock_prices = retrieve_stock_data(\"MSFT\", \"2019-01-01\", \"2020-01-01\")\n\nrets = normal_rets(stock_prices).dropna()\nrets.columns = ['returns']\n```\n\n## Fit all known distributions\n\nInstead of trying to fit the Gaussian Distribution to our data, we will try to fit all the known (scipy.stats implementations) distributions and see which one (or ones) fits the data best.\n\n\n```\nimport numpy as np\nfrom scipy.stats._continuous_distns import _distn_names\nimport scipy.stats as st\nimport warnings\n\ndef fit_all_distributions(data):\n    \"\"\"\n    Returns: Dict of the density function and the Sum of Squared Errors (SSE)\n    \"\"\"\n    # Get histogram of original data. First, get the x for the pdf\n    # second, get y to calculate the distance between the \n    # distribution and the real data\n    y, x = np.histogram(data, bins='auto', density=True)\n    x = (x + np.roll(x, -1))[:-1] / 2.0\n\n    dist_fit = {}\n\n    # Estimate distribution parameters from data\n    for distname in _distn_names:\n        distribution = getattr(st, distname)\n\n        # Try to fit the distribution\n        try:\n            # Ignore warnings for data that can't be fit\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore')\n\n                # fit dist to data\n                params = distribution.fit(data)\n\n                # Calculate fitted PDF and error\n                pdf = distribution.pdf(x, *params)\n                sse = np.sum(np.power(y - pdf, 2.0))\n                dist_fit[distname] = {}\n                dist_fit[distname]['pdf'] = pdf\n                dist_fit[distname]['sse'] = sse\n        except Exception:\n            pass\n\n    return dist_fit\n```\n\n\n```\ndist_fit = fit_all_distributions(rets.returns)\nlen(dist_fit)\n```\n    99\n\n\n\nWhat? 99? :) Well, It should not be of a surprise that several distributions would fit the data. But the \"fit\" could be really off :) For example an exponential or even a uniform distribution could fit the data, but the squared error (the distance between the actual data points and the respective distribution points) would be really large compared to distributions that approximate the histogram of our data.\n\nLet's get the 15 distributions with the lowest SSE (the ones that best fit the data).\n\n\n```\ndname_list = []\nsse_list = []\npdf = []\nfor dname in dist_fit:\n  dname_list.append(dname)\n  sse_list.append(dist_fit[dname]['sse'])\n  pdf.append(dist_fit[dname]['pdf'])\n\ndf = pd.DataFrame({'dist': dname_list, 'sse': sse_list, 'pdf':pdf})\ndf = df.sort_values('sse')\ndf_15 = df[:15]\ndf_15\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dist</th>\n      <th>sse</th>\n      <th>pdf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>52</th>\n      <td>laplace</td>\n      <td>148.651407</td>\n      <td>[1.1152568034938246, 1.787521047421414, 2.8650...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>gennorm</td>\n      <td>164.078771</td>\n      <td>[0.8367820735065007, 1.5154080142470994, 2.686...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>dweibull</td>\n      <td>176.635717</td>\n      <td>[0.8793256263714793, 1.5296378660593237, 2.635...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>dgamma</td>\n      <td>191.050465</td>\n      <td>[0.8552917947109029, 1.4658525793451274, 2.502...</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>hypsecant</td>\n      <td>213.097016</td>\n      <td>[0.7998518705010982, 1.383459240576739, 2.3919...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>norminvgauss</td>\n      <td>230.918089</td>\n      <td>[0.9381562968673568, 1.5495654573259756, 2.580...</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>tukeylambda</td>\n      <td>232.611350</td>\n      <td>[0.7436837695141802, 1.2913003803519416, 2.270...</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>johnsonsu</td>\n      <td>241.389639</td>\n      <td>[0.9016851129004099, 1.4956683047905153, 2.519...</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>t</td>\n      <td>243.179748</td>\n      <td>[0.715121368715189, 1.2350959953532723, 2.1881...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>nct</td>\n      <td>250.858249</td>\n      <td>[0.8688042602613769, 1.4480199008827952, 2.464...</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>genlogistic</td>\n      <td>260.651315</td>\n      <td>[0.8748265286114669, 1.5253897598139983, 2.650...</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>logistic</td>\n      <td>263.548520</td>\n      <td>[0.6451586874653275, 1.2223472162162576, 2.299...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>burr12</td>\n      <td>271.252587</td>\n      <td>[0.7636589016658705, 1.3949317532979122, 2.529...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>cauchy</td>\n      <td>272.502209</td>\n      <td>[1.4774507773037833, 1.8876747700442502, 2.491...</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>levy_stable</td>\n      <td>285.357222</td>\n      <td>[0.7622030792115081, 1.252074636445709, 2.2016...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nInterestingly `laplace` was the distribution which approximates best the initial data. However, several other distributions are not that far from `laplace` with regards to SSE distances. To showcase that, let's see how the distributions visually correlate with the actual data.\n\n\n```\ny, x = np.histogram(rets.returns, bins=19, density=True)\nx = (x + np.roll(x, -1))[:-1] / 2.0\nax = rets.returns.plot(kind='hist', bins=19, figsize=(14,7), density=True, alpha=0.5, \n                       color=list(matplotlib.rcParams['axes.prop_cycle'])[1]['color'])\n\nfor index, row in df_15.iterrows():\n  pd.Series(row.pdf, x).plot(ax=ax, label=row.dist)\nax.set_title(u'All Fitted Distributions')\nax.set_xlabel(u'Returns')\nax.set_ylabel('Frequency')\nplt.legend(loc=\"upper left\")\nax.plot()\n```\n \n![png](fit-distributions-to-asset-returns/fit_distributions_to_asset_returns_8_1.png)\n    \n\nAn interesting observation from the graph above is that all there is almost a concurrence of the distribution legs and tails. On the contrary they do not really agree on the height of the mean bar, but all of them approximate it pretty well.\n\nSo, are we done yet? Shall we, from now on, use `laplace` as the distribution to represent asset returns? \n\nUnfortunately no :(! And the reason why, is that there are several other parameters that can easily interfere with what we expect:\n\n* The number of data points (daily vs. minute vs. monthly)\n* The selected period. (last 3 months will give different result compared to last 6 months or 1 year, or more)\n* The reason we use a distribution (to measure risk? to predict returns? etc.)\n\n## Why T-Student is often used?\n\nYou might have wondered, why so many people use the T-Student distribution when analyzing stock return data!\n\nThe answer to that has to do with the risk that an investor can accept when placing money to a risky asset. There are different ways to measure risk and one of them is the risk estimation based on distributions (model risk). \n\nSay for example, in the MSFT scenario above, that we would like to avoid any daily price drop of more than 2%. And we ask, how confident are we that there will be no drop of more than 2% tomorrow (since we have daily returns)? \n\nThe answer to that question can be derived from the CDF (Cumulative Distribution Function) of a distribution (which is the area under the PDF (Point Distribution Function))\n\nHere is how the normal and student-t PDF looks like for MSFT returns in the period we test. \n\n```\nax = rets.returns.plot(kind='hist', bins=19, figsize=(14,7), density=True, alpha=0.5, \n                       color=list(matplotlib.rcParams['axes.prop_cycle'])[1]['color'])\n\nstudent_t_pdf = df.loc[df['dist'] == 't'].pdf.values[0]\npd.Series(student_t_pdf, x).plot(ax=ax, label='t')\nnormal_pdf = df.loc[df['dist'] == 'norm'].pdf.values[0]\npd.Series(normal_pdf, x).plot(ax=ax, label='normal')\n\nax.set_title(u'Student-T and Normal Distributions')\nax.set_xlabel(u'Returns')\nax.set_ylabel('Frequency')\nplt.legend(loc=\"upper left\")\nax.plot()\n```\n \n![png](fit-distributions-to-asset-returns/fit_distributions_to_asset_returns_10_1.png)\n    \n\nBack to our question now. How confident are we that tomorrow the price will not drop below 2% (-0.02)? \n\n```\nnorm_cdf = np.cumsum(normal_pdf)\nt_cdf = np.cumsum(student_t_pdf)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,7))\nfig.suptitle('CDF for Normal and T dists')\nax1.plot(x, norm_cdf)\nax2.plot(x, t_cdf)\nplt.show()\n```\n\n![png](fit-distributions-to-asset-returns/fit_distributions_to_asset_returns_12_0.png)\n    \n```\nnorm_cdf[np.where(x <= -0.02)][-1]\n```\n    9.876651890962592\n\n```\nt_cdf[np.where(x <= -0.02)][-1]\n```\n    8.086428763077489\n\nAccording to the normal distribution, there is 9.9% probability that the price will drop below 2% while only 8.1% student-t probability. Someone, based on their risk tolerance, would be more confident to place some more money knowing that the estimate is much closer to the reality (well, historical reality)!  \n\nBut, again, it is really up to us to use any distribution we feel comfortable with, based on the data we have, our risk level, and of course (when it comes to statistics) the tools to do our analysis easier.","excerpt":""},"previousPost":{"slug":"a-tradeable-investment-portfolio-as-erc20-token","frontmatter":{"title":"A Tradeable Investment Portfolio as ERC20 Token","description":"Building a simple portfolio on Ethereum blockchain as an ERC20 token.","date":"February 21, 2021","topic":{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},"tags":[{"id":"cryptos","name":"cryptos","image":"bitcoin.png","description":"The amazing world of Blockchain opens one more chapter in the Investing.","color":"bg-green-300","icon":"bitcoin.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/erc20-exchange.png"},"excerpt":"","content":"\nIn a previous post we discussed how it is possible to create a [simple index fund by combining several tradeable assets](/post/from-portfolio-wealth-index-to-index-fund). We also mentioned that such a portfolio can be traded on the secondary market as a basic ETF. \n\nSuch a scenario is pretty difficult to achieve since special licenses and time consuming actions are needed. However, it is fairly easy to do on the Ethereum blockchain!\n\nThis is what we will try to showcase in this article. We will build a very simple contract that keeps information about a fictional portfolio of other tokens.\n\nIn its extended form, every time someone purchases this token, the amount invested, will be used to purchase the tokens listed in the portfolio based on their weights, and as such, it will follow the development of these tokens (like the index fund we described in the other article).\n\nBelow is a screenshot of the end result. The code is published on [github](https://github.com/investingsnippets/tradeable-investment-portfolio-as-erc20-token).\n\n![png](a-tradeable-investment-portfolio-as-erc20-token/portfolio-token.png)\n\n## Introduction to Ethereum Tokens\n\nI will not go deep into how Ethereum works, since it is a huge topic. We can describe it as a decentralized computer which can execute instructions in a distributed manner. These instructions are simple computer programs (code) that perform a specific job. This program is what we call `An Ethereum Smart Contract`. \n\nThe distributed manner of the contract execution is achieved by the nodes on a blockchain :). That means that the program once might run on a node (server) in USA and the next moment on a node in China. Traditionally, we were used to programs running on a limited number of nodes (e.g. on the cloud), that we managed by a single entity (e.g. cloud provider, company datacenter). However, in the blockchain case, the nodes are just another user terminal (e.g. a server at home, our laptops, etc.).\n\nOn another side, a blockchain solves the well known `double spending` problem (which we will analyze in another post) and due to that, it can be used as an exchange mechanism for digital currencies! Combining these two aspects together, we achieve tradeable contracts (digital assets), which we generally call tokens (they can hold a balance, have a defined supply amount, and can be transferred).\n\nAs an example, imagine a digital currency where you are able to program it with something like. If, the user solves a puzzle on the internet, wins one coin! Or, if the user deposits purchases the coin with some $, then these $ will be used to purchase other coins and hold them locked in the initial coin, until the owner decides to sell the coin and as a consequence sell the subsequent coins! \n\nNow, the contract creation usually follows some coding principles and is implemented using a programming language. This language is called [Solidity](https://docs.soliditylang.org/en/v0.8.1/) (on Ethereum) and the principles are called standards. ERC20 is such a standard and allows for a digital currency/token creation!\n\nThe tokens can be used in many different ways, which I will describe in a future post. For now we will focus on the creation of digital currency feature.\n\n## The contract\n\nBelow I'm pasting the simplified version of the contract with inlined comments.\n\n```\n// define the solidity compiler version to be used\npragma solidity 0.6.10;\n// additional functionalities for passing around structs  \npragma experimental \"ABIEncoderV2\";\n\n// We use openzeppelin library which provides proper interfaces and\n// solves lot's of boilerplate work we would do otherwise.\nimport { Address } from \"@openzeppelin/contracts/utils/Address.sol\";\nimport { ERC20 } from \"@openzeppelin/contracts/token/ERC20/ERC20.sol\";\nimport { SafeMath } from \"@openzeppelin/contracts/math/SafeMath.sol\";\nimport { SignedSafeMath } from \"@openzeppelin/contracts/math/SignedSafeMath.sol\";\n\n// We define the name of the contract and specify that is an ERC20 token\ncontract PortfolioToken is ERC20 {\n    using Address for address;\n    using SafeMath for uint256;\n    using SignedSafeMath for int256;\n\n    // the user who creates the contract is also the administrator\n    address PortfolioManager;\n\n    // the asset definition\n    struct Asset {\n        uint8 weight; // weight in the Portfolio\n        uint timeStamp; // last updated\n        string name; // name of the asset. For the purposes of the example, since tokens can have the same name\n        string symbol; // symbol of the asset. Also, the symbol of a token can be the same among tokens :(\n    }\n\n    mapping ( address => Asset ) assets; // this allows to store tokens by their ethereum address\n    address[] public allAssets; // a directory of all the token addresses in the fund\n    mapping(address => bool) public isResource; // for validation purposes\n\n    // the initial price of the token\n    int256 private strikePrice;\n\n    // the contsrucor of the token will be called when deploying the contract\n    constructor(\n        int256 _strikePrice,\n        string memory _name,\n        string memory _symbol\n    )\n        public\n        ERC20(_name, _symbol)\n    {\n        PortfolioManager = msg.sender;\n        strikePrice = _strikePrice;\n    }\n\n    modifier onlyManager() {\n        _validateOnlyManager();\n        _;\n    }\n\n    function mint(address _account, uint256 _quantity) public {\n        _mint(_account, _quantity);\n    }\n\n    // the main method used to add an asset to the portfolio\n    function addAsset(address _assetAddress, uint8 _weight, string memory _name, string memory _symbol) external onlyManager {\n        require(!isResource[_assetAddress], \"Asset already exists\"); // not adding true/false val in struct to save gas\n        assets[_assetAddress].weight = _weight;\n        assets[_assetAddress].name = _name;\n        assets[_assetAddress].symbol = _symbol;\n        assets[_assetAddress].timeStamp = block.timestamp;\n        isResource[_assetAddress] = true;\n        allAssets.push(_assetAddress);\n    }\n\n    // edits an asset's weight\n    function editAsset(address _assetAddress, uint8 _weight) external onlyManager {\n        assets[_assetAddress].weight = _weight;\n        assets[_assetAddress].timeStamp = block.timestamp;\n    }\n\n    // removes an asset from the portfolio\n    function removeAsset(address _assetAddress) external onlyManager {\n        delete assets[_assetAddress];\n        // allAssets = allAssets.remove(_assetAddress);\n        isResource[_assetAddress] = false;\n    }\n\n    // returns a list of assets in the portfolio\n    function getAssets() external view returns(address[] memory) {\n        return allAssets;\n    }\n\n    // return the info of an asset by passing the address of it\n    function getAssetInfo(address _assetAddress) external view returns(string memory name, string memory symbol, uint8 weight, uint timeStamp) {\n        return (assets[_assetAddress].name, assets[_assetAddress].symbol, assets[_assetAddress].weight, assets[_assetAddress].timeStamp);\n    } \n\n    function _validateOnlyManager() internal view {\n        require(msg.sender == PortfolioManager, \"Only manager has access\");\n    }\n}\n```\n\n## How to try it out\n\n> Note: Make sure that MetaMask is either disabled, or configured to use the local blockchain. OR, use browser in incognito mode. Be extra careful! (you have been warned ;))\n\nFor this project I have used:\n\n* ganache, which provides a local Ethereum blockchain.\n* truffle, which provides tooling to compile, test and deploy contracts\n* drizzle, which implements client side interaction with the contract (through web3.js)\n* react, for building a simple web application to be able to graphically interact with the contract\n\nTo run the example, clone the [github repo](https://github.com/investingsnippets/tradeable-investment-portfolio-as-erc20-token) and make sure that docker is installed on your machine.\n\nThen, use the command\n\n```\ndocker-compose run -p \"3000:3000\" -p \"8545:8545\" --rm develop\n```\n\nwhich starts everything needed in docker and exposes the web app on port 3000!\n\nJust visit `http://localhost:3000/` on your browser and experiment with the app.\n"},"nextPost":{"slug":"modern-portfolio-theory","frontmatter":{"title":"Modern Portfolio Theory - Part 1","description":"A Nobel Price winning theory that has shaped Investing.","date":"April 2, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/nobel-price.jpg","colab":"https://colab.research.google.com/drive/1U6dR3xx-g4NWXl-5NKtGlMZVIkSJcJEz?usp=sharing"},"excerpt":"","content":"\nIn a previous post about [Return & Volatility of a Multi-Asset Portfolio](/post/portfolio-expected-return-and-risk) we saw how the correlation of the prices of two assets was a key part to achieving lower volatility than the volatility of the assets' individually. We built a visual proof with just two random assets.\n\nThis \"visual proof\" is called Efficient Frontier\n\n![jpg](https://upload.wikimedia.org/wikipedia/commons/e/e1/Markowitz_frontier.jpg)\n\nwhich is part of Modern portfolio theory (MPT) and according to [wikipedia](https://en.wikipedia.org/wiki/Modern_portfolio_theory)\n\n> Modern portfolio theory (MPT), or mean-variance analysis, is a mathematical framework for assembling a portfolio of assets such that the expected return is maximized for a given level of risk. It is a formalization and extension of diversification in investing, the idea that owning different kinds of financial assets is less risky than owning only one type. Its key insight is that an asset's risk and return should not be assessed by itself, but by how it contributes to a portfolio's overall risk and return. It uses the variance of asset prices as a proxy for risk.\n\n> Economist Harry Markowitz introduced MPT in a 1952 essay, for which he was later awarded a Nobel Memorial Prize in Economic Sciences; see [Markowitz model](https://en.wikipedia.org/wiki/Markowitz_model).\n\nThe \"two asset\" Efficient Frontier we built in the previous post was done through carefully picking the asset weights and printing the mean-variance graph. Then, we were able to find on the graph which pair of wights was responsible for the minimum volatility amount.\n\nWe will do the same today but instead of using the Efficient Frontier for that, we will use some optimizers provided by `numpy`. \n\n## Data Collection\n\nAs always, we set the ground work needed to fetch some stock data.\n\n\n```python\n%%capture\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport dateutil.parser\nimport numpy as np\n\nSTOCK_SYMBOLS = ['MSFT', 'APT']\n\ndef retrieve_stock_data(symbol, start, end):\n    '''\n    Fetches daily stock prices from Yahoo Finance\n    '''\n    json = YahooFinancials(symbol).get_historical_price_data(start, end, \"daily\")\n    df = pd.DataFrame(columns=[\"adjclose\"])\n    for row in json[symbol][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    '''Returns the returns. (price_today - price_yesterday)/price_yesterday'''\n    return S.pct_change().dropna()\n\ndef annualize_rets(r, periods_per_year):\n    compounded_growth = (1+r).prod()\n    n_periods = r.shape[0]\n    return compounded_growth**(periods_per_year/n_periods)-1\n\ndef portfolio_return(weights, returns):\n    return weights.T @ returns\n\ndef portfolio_volatility(weights, covariance_matrix):\n    return (weights.T @ covariance_matrix @ weights)**0.5\n\ndef generate_returns_dataframe(symbols_list, d_from=\"2020-01-01\", d_to=\"2021-01-01\"):\n    '''\n    Generates a DataFrame with daily returns for a list of Symbols.\n    '''\n    returns = pd.DataFrame()\n    for symbol in symbols_list:\n        stock_prices = retrieve_stock_data(symbol, d_from, d_to)\n        rets = normal_rets(stock_prices).dropna()\n        rets.columns = [symbol]\n\n        if returns.empty:\n            returns = rets\n        else:\n            returns = returns.merge(rets, left_index=True, right_index=True)\n    return returns\n```\n\n\n```python\nreturns = generate_returns_dataframe(STOCK_SYMBOLS)\nreturns.head(5)\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>APT</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-01-03</th>\n      <td>-0.012452</td>\n      <td>0.005780</td>\n    </tr>\n    <tr>\n      <th>2020-01-06</th>\n      <td>0.002585</td>\n      <td>0.008621</td>\n    </tr>\n    <tr>\n      <th>2020-01-07</th>\n      <td>-0.009118</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2020-01-08</th>\n      <td>0.015928</td>\n      <td>-0.002849</td>\n    </tr>\n    <tr>\n      <th>2020-01-09</th>\n      <td>0.012493</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n## Volatility Minimization\n\nThe data we collected above is the same as [Return & Volatility of a Multi-Asset Portfolio](/post/portfolio-expected-return-and-risk), and the purpose is to find the weights that minimize the volatility of the portfolio. As we saw from the graph in the previous article, the weights should be somewhere close to [0.89, 0.11].\n\nFor example, given weights [0.5, 0.5], the portfolio return is:\n\n\n```python\nportfolio_return(np.array([0.5, 0.5]), annualize_rets(returns, 252)) - 1\n```\n\n    0.3109859252509841\n\n\n\nTo avoid a brute-force approach with trying different weights and get the ones that give the minimum volatility, we can use algorithms that do this for us. Algorithms like [Sequential quadratic programming](https://en.wikipedia.org/wiki/Sequential_quadratic_programming) which based on the amount of constraints performs different levels of differentiations with purpose to minimize a cost function. In general, the objective function which in our case is the Efficient Frontier, has a point where the tangent either gets 0 or 1. In our case the tangent should be 1. The algorithm iteratively walks through the objective function, finds the tangent, and applies the constraints. Of course, like any cost function optimization in Machine Learning the accuracy is not always 100%! The algorithm may get stuck in local minima and not be able to find the best result. However, in the case of Efficient Frontier, the objective function has only one minimum and thus makes it easier to find.\n\nLet's apply the algorithm to our data. The only constraints we provide for now is that the sum of the weights must be equal to 1.\n\n\n```python\nfrom scipy.optimize import minimize\nn = len(STOCK_SYMBOLS)\ncov = returns.cov()\nweights_sum_to_1 = {\n    'type': 'eq',\n    'fun': lambda weights: np.sum(weights) - 1\n}\nresult = minimize(portfolio_volatility, np.repeat(0, n), args=(cov, ),\n                  method=\"SLSQP\", options={'disp': False},\n                  constraints=(weights_sum_to_1), bounds=((0.0, 1.0),)*n)\nresult.x\n```\n\n    array([0.89906844, 0.10093156])\n\n\nAs expected the weights are in the range we expected!\n\nIn a next article we will go one step further in the Efficient Frontier analysis and try to find other points in the graph that are of interest.\n\nStay tuned!"}},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg","slug":"python","count":9},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg","slug":"statistics","count":4},{"id":"cryptos","name":"cryptos","image":"bitcoin.png","description":"The amazing world of Blockchain opens one more chapter in the Investing.","color":"bg-green-300","icon":"bitcoin.svg","slug":"cryptos","count":1}],"sortedTopics":[{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300","slug":"investing","count":7},{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300","slug":"mathematics","count":4},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300","slug":"automation","count":2}],"allTopics":[{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"}],"slug":"fit-distributions-to-asset-returns"},"__N_SSG":true}