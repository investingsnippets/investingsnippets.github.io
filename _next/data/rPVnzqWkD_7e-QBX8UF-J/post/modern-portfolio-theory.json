{"pageProps":{"postData":{"frontmatter":{"title":"Modern Portfolio Theory - Part 1","description":"A Nobel Price winning theory that has shaped Investing.","date":"April 2, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/nobel-price.jpg","colab":"https://colab.research.google.com/drive/1U6dR3xx-g4NWXl-5NKtGlMZVIkSJcJEz?usp=sharing"},"post":{"content":"\nIn a previous post about [Return & Volatility of a Multi-Asset Portfolio](/post/portfolio-expected-return-and-risk) we saw how the correlation of the prices of two assets was a key part to achieving lower volatility than the volatility of the assets' individually. We built a visual proof with just two random assets.\n\nThis \"visual proof\" is called Efficient Frontier\n\n![jpg](https://upload.wikimedia.org/wikipedia/commons/e/e1/Markowitz_frontier.jpg)\n\nwhich is part of Modern portfolio theory (MPT) and according to [wikipedia](https://en.wikipedia.org/wiki/Modern_portfolio_theory)\n\n> Modern portfolio theory (MPT), or mean-variance analysis, is a mathematical framework for assembling a portfolio of assets such that the expected return is maximized for a given level of risk. It is a formalization and extension of diversification in investing, the idea that owning different kinds of financial assets is less risky than owning only one type. Its key insight is that an asset's risk and return should not be assessed by itself, but by how it contributes to a portfolio's overall risk and return. It uses the variance of asset prices as a proxy for risk.\n\n> Economist Harry Markowitz introduced MPT in a 1952 essay, for which he was later awarded a Nobel Memorial Prize in Economic Sciences; see [Markowitz model](https://en.wikipedia.org/wiki/Markowitz_model).\n\nThe \"two asset\" Efficient Frontier we built in the previous post was done through carefully picking the asset weights and printing the mean-variance graph. Then, we were able to find on the graph which pair of wights was responsible for the minimum volatility amount.\n\nWe will do the same today but instead of using the Efficient Frontier for that, we will use some optimizers provided by `numpy`. \n\n## Data Collection\n\nAs always, we set the ground work needed to fetch some stock data.\n\n\n```python\n%%capture\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport dateutil.parser\nimport numpy as np\n\nSTOCK_SYMBOLS = ['MSFT', 'APT']\n\ndef retrieve_stock_data(symbol, start, end):\n    '''\n    Fetches daily stock prices from Yahoo Finance\n    '''\n    json = YahooFinancials(symbol).get_historical_price_data(start, end, \"daily\")\n    df = pd.DataFrame(columns=[\"adjclose\"])\n    for row in json[symbol][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    '''Returns the returns. (price_today - price_yesterday)/price_yesterday'''\n    return S.pct_change().dropna()\n\ndef annualize_rets(r, periods_per_year):\n    compounded_growth = (1+r).prod()\n    n_periods = r.shape[0]\n    return compounded_growth**(periods_per_year/n_periods)-1\n\ndef portfolio_return(weights, returns):\n    return weights.T @ returns\n\ndef portfolio_volatility(weights, covariance_matrix):\n    return (weights.T @ covariance_matrix @ weights)**0.5\n\ndef generate_returns_dataframe(symbols_list, d_from=\"2020-01-01\", d_to=\"2021-01-01\"):\n    '''\n    Generates a DataFrame with daily returns for a list of Symbols.\n    '''\n    returns = pd.DataFrame()\n    for symbol in symbols_list:\n        stock_prices = retrieve_stock_data(symbol, d_from, d_to)\n        rets = normal_rets(stock_prices).dropna()\n        rets.columns = [symbol]\n\n        if returns.empty:\n            returns = rets\n        else:\n            returns = returns.merge(rets, left_index=True, right_index=True)\n    return returns\n```\n\n\n```python\nreturns = generate_returns_dataframe(STOCK_SYMBOLS)\nreturns.head(5)\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>APT</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-01-03</th>\n      <td>-0.012452</td>\n      <td>0.005780</td>\n    </tr>\n    <tr>\n      <th>2020-01-06</th>\n      <td>0.002585</td>\n      <td>0.008621</td>\n    </tr>\n    <tr>\n      <th>2020-01-07</th>\n      <td>-0.009118</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2020-01-08</th>\n      <td>0.015928</td>\n      <td>-0.002849</td>\n    </tr>\n    <tr>\n      <th>2020-01-09</th>\n      <td>0.012493</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n## Volatility Minimization\n\nThe data we collected above is the same as [Return & Volatility of a Multi-Asset Portfolio](/post/portfolio-expected-return-and-risk), and the purpose is to find the weights that minimize the volatility of the portfolio. As we saw from the graph in the previous article, the weights should be somewhere close to [0.89, 0.11].\n\nFor example, given weights [0.5, 0.5], the portfolio return is:\n\n\n```python\nportfolio_return(np.array([0.5, 0.5]), annualize_rets(returns, 252)) - 1\n```\n\n    0.3109859252509841\n\n\n\nTo avoid a brute-force approach with trying different weights and get the ones that give the minimum volatility, we can use algorithms that do this for us. Algorithms like [Sequential quadratic programming](https://en.wikipedia.org/wiki/Sequential_quadratic_programming) which based on the amount of constraints performs different levels of differentiations with purpose to minimize a cost function. In general, the objective function which in our case is the Efficient Frontier, has a point where the tangent either gets 0 or 1. In our case the tangent should be 1. The algorithm iteratively walks through the objective function, finds the tangent, and applies the constraints. Of course, like any cost function optimization in Machine Learning the accuracy is not always 100%! The algorithm may get stuck in local minima and not be able to find the best result. However, in the case of Efficient Frontier, the objective function has only one minimum and thus makes it easier to find.\n\nLet's apply the algorithm to our data. The only constraints we provide for now is that the sum of the weights must be equal to 1.\n\n\n```python\nfrom scipy.optimize import minimize\nn = len(STOCK_SYMBOLS)\ncov = returns.cov()\nweights_sum_to_1 = {\n    'type': 'eq',\n    'fun': lambda weights: np.sum(weights) - 1\n}\nresult = minimize(portfolio_volatility, np.repeat(0, n), args=(cov, ),\n                  method=\"SLSQP\", options={'disp': False},\n                  constraints=(weights_sum_to_1), bounds=((0.0, 1.0),)*n)\nresult.x\n```\n\n    array([0.89906844, 0.10093156])\n\n\nAs expected the weights are in the range we expected!\n\nIn a next article we will go one step further in the Efficient Frontier analysis and try to find other points in the graph that are of interest.\n\nStay tuned!","excerpt":""},"previousPost":{"slug":"fit-distributions-to-asset-returns","frontmatter":{"title":"Fit Multiple Distributions to Asset Returns!","description":"Why normal distribution is not preferred for stock returns analysis.","date":"March 28, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/normal-dist-with-hist.png","colab":"https://colab.research.google.com/drive/1p3KbU09vOrplisEzMFEQqGV5Jtuxr3FC?usp=sharing"},"excerpt":"","content":"\nEarlier in [Are Stock Returns Normally Distributed?](post/are-stock-returns-normally-distributed) we went through different ways to validate when asset returns are normally distributed. While we used only one stock to prove that stock returns are not normally distributed, the phenomenon applies to any volatile asset, in general.\n\nThe question though is: Really, which distribution do returns follow?\n\nBelow we load the MSFT stock returns as we did before! \n\n\n```\n%%capture\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\n\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n\nstock_prices = retrieve_stock_data(\"MSFT\", \"2019-01-01\", \"2020-01-01\")\n\nrets = normal_rets(stock_prices).dropna()\nrets.columns = ['returns']\n```\n\n## Fit all known distributions\n\nInstead of trying to fit the Gaussian Distribution to our data, we will try to fit all the known (scipy.stats implementations) distributions and see which one (or ones) fits the data best.\n\n\n```\nimport numpy as np\nfrom scipy.stats._continuous_distns import _distn_names\nimport scipy.stats as st\nimport warnings\n\ndef fit_all_distributions(data):\n    \"\"\"\n    Returns: Dict of the density function and the Sum of Squared Errors (SSE)\n    \"\"\"\n    # Get histogram of original data. First, get the x for the pdf\n    # second, get y to calculate the distance between the \n    # distribution and the real data\n    y, x = np.histogram(data, bins='auto', density=True)\n    x = (x + np.roll(x, -1))[:-1] / 2.0\n\n    dist_fit = {}\n\n    # Estimate distribution parameters from data\n    for distname in _distn_names:\n        distribution = getattr(st, distname)\n\n        # Try to fit the distribution\n        try:\n            # Ignore warnings for data that can't be fit\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore')\n\n                # fit dist to data\n                params = distribution.fit(data)\n\n                # Calculate fitted PDF and error\n                pdf = distribution.pdf(x, *params)\n                sse = np.sum(np.power(y - pdf, 2.0))\n                dist_fit[distname] = {}\n                dist_fit[distname]['pdf'] = pdf\n                dist_fit[distname]['sse'] = sse\n        except Exception:\n            pass\n\n    return dist_fit\n```\n\n\n```\ndist_fit = fit_all_distributions(rets.returns)\nlen(dist_fit)\n```\n    99\n\n\n\nWhat? 99? :) Well, It should not be of a surprise that several distributions would fit the data. But the \"fit\" could be really off :) For example an exponential or even a uniform distribution could fit the data, but the squared error (the distance between the actual data points and the respective distribution points) would be really large compared to distributions that approximate the histogram of our data.\n\nLet's get the 15 distributions with the lowest SSE (the ones that best fit the data).\n\n\n```\ndname_list = []\nsse_list = []\npdf = []\nfor dname in dist_fit:\n  dname_list.append(dname)\n  sse_list.append(dist_fit[dname]['sse'])\n  pdf.append(dist_fit[dname]['pdf'])\n\ndf = pd.DataFrame({'dist': dname_list, 'sse': sse_list, 'pdf':pdf})\ndf = df.sort_values('sse')\ndf_15 = df[:15]\ndf_15\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dist</th>\n      <th>sse</th>\n      <th>pdf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>52</th>\n      <td>laplace</td>\n      <td>148.651407</td>\n      <td>[1.1152568034938246, 1.787521047421414, 2.8650...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>gennorm</td>\n      <td>164.078771</td>\n      <td>[0.8367820735065007, 1.5154080142470994, 2.686...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>dweibull</td>\n      <td>176.635717</td>\n      <td>[0.8793256263714793, 1.5296378660593237, 2.635...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>dgamma</td>\n      <td>191.050465</td>\n      <td>[0.8552917947109029, 1.4658525793451274, 2.502...</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>hypsecant</td>\n      <td>213.097016</td>\n      <td>[0.7998518705010982, 1.383459240576739, 2.3919...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>norminvgauss</td>\n      <td>230.918089</td>\n      <td>[0.9381562968673568, 1.5495654573259756, 2.580...</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>tukeylambda</td>\n      <td>232.611350</td>\n      <td>[0.7436837695141802, 1.2913003803519416, 2.270...</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>johnsonsu</td>\n      <td>241.389639</td>\n      <td>[0.9016851129004099, 1.4956683047905153, 2.519...</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>t</td>\n      <td>243.179748</td>\n      <td>[0.715121368715189, 1.2350959953532723, 2.1881...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>nct</td>\n      <td>250.858249</td>\n      <td>[0.8688042602613769, 1.4480199008827952, 2.464...</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>genlogistic</td>\n      <td>260.651315</td>\n      <td>[0.8748265286114669, 1.5253897598139983, 2.650...</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>logistic</td>\n      <td>263.548520</td>\n      <td>[0.6451586874653275, 1.2223472162162576, 2.299...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>burr12</td>\n      <td>271.252587</td>\n      <td>[0.7636589016658705, 1.3949317532979122, 2.529...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>cauchy</td>\n      <td>272.502209</td>\n      <td>[1.4774507773037833, 1.8876747700442502, 2.491...</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>levy_stable</td>\n      <td>285.357222</td>\n      <td>[0.7622030792115081, 1.252074636445709, 2.2016...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nInterestingly `laplace` was the distribution which approximates best the initial data. However, several other distributions are not that far from `laplace` with regards to SSE distances. To showcase that, let's see how the distributions visually correlate with the actual data.\n\n\n```\ny, x = np.histogram(rets.returns, bins=19, density=True)\nx = (x + np.roll(x, -1))[:-1] / 2.0\nax = rets.returns.plot(kind='hist', bins=19, figsize=(14,7), density=True, alpha=0.5, \n                       color=list(matplotlib.rcParams['axes.prop_cycle'])[1]['color'])\n\nfor index, row in df_15.iterrows():\n  pd.Series(row.pdf, x).plot(ax=ax, label=row.dist)\nax.set_title(u'All Fitted Distributions')\nax.set_xlabel(u'Returns')\nax.set_ylabel('Frequency')\nplt.legend(loc=\"upper left\")\nax.plot()\n```\n \n![png](fit-distributions-to-asset-returns/fit_distributions_to_asset_returns_8_1.png)\n    \n\nAn interesting observation from the graph above is that all there is almost a concurrence of the distribution legs and tails. On the contrary they do not really agree on the height of the mean bar, but all of them approximate it pretty well.\n\nSo, are we done yet? Shall we, from now on, use `laplace` as the distribution to represent asset returns? \n\nUnfortunately no :(! And the reason why, is that there are several other parameters that can easily interfere with what we expect:\n\n* The number of data points (daily vs. minute vs. monthly)\n* The selected period. (last 3 months will give different result compared to last 6 months or 1 year, or more)\n* The reason we use a distribution (to measure risk? to predict returns? etc.)\n\n## Why T-Student is often used?\n\nYou might have wondered, why so many people use the T-Student distribution when analyzing stock return data!\n\nThe answer to that has to do with the risk that an investor can accept when placing money to a risky asset. There are different ways to measure risk and one of them is the risk estimation based on distributions (model risk). \n\nSay for example, in the MSFT scenario above, that we would like to avoid any daily price drop of more than 2%. And we ask, how confident are we that there will be no drop of more than 2% tomorrow (since we have daily returns)? \n\nThe answer to that question can be derived from the CDF (Cumulative Distribution Function) of a distribution (which is the area under the PDF (Point Distribution Function))\n\nHere is how the normal and student-t PDF looks like for MSFT returns in the period we test. \n\n```\nax = rets.returns.plot(kind='hist', bins=19, figsize=(14,7), density=True, alpha=0.5, \n                       color=list(matplotlib.rcParams['axes.prop_cycle'])[1]['color'])\n\nstudent_t_pdf = df.loc[df['dist'] == 't'].pdf.values[0]\npd.Series(student_t_pdf, x).plot(ax=ax, label='t')\nnormal_pdf = df.loc[df['dist'] == 'norm'].pdf.values[0]\npd.Series(normal_pdf, x).plot(ax=ax, label='normal')\n\nax.set_title(u'Student-T and Normal Distributions')\nax.set_xlabel(u'Returns')\nax.set_ylabel('Frequency')\nplt.legend(loc=\"upper left\")\nax.plot()\n```\n \n![png](fit-distributions-to-asset-returns/fit_distributions_to_asset_returns_10_1.png)\n    \n\nBack to our question now. How confident are we that tomorrow the price will not drop below 2% (-0.02)? \n\n```\nnorm_cdf = np.cumsum(normal_pdf)\nt_cdf = np.cumsum(student_t_pdf)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,7))\nfig.suptitle('CDF for Normal and T dists')\nax1.plot(x, norm_cdf)\nax2.plot(x, t_cdf)\nplt.show()\n```\n\n![png](fit-distributions-to-asset-returns/fit_distributions_to_asset_returns_12_0.png)\n    \n```\nnorm_cdf[np.where(x <= -0.02)][-1]\n```\n    9.876651890962592\n\n```\nt_cdf[np.where(x <= -0.02)][-1]\n```\n    8.086428763077489\n\nAccording to the normal distribution, there is 9.9% probability that the price will drop below 2% while only 8.1% student-t probability. Someone, based on their risk tolerance, would be more confident to place some more money knowing that the estimate is much closer to the reality (well, historical reality)!  \n\nBut, again, it is really up to us to use any distribution we feel comfortable with, based on the data we have, our risk level, and of course (when it comes to statistics) the tools to do our analysis easier."},"nextPost":{"slug":"inflation-planning-future-cash-flows","frontmatter":{"title":"Inflation Looking Forward!","description":"How to plan for future cash flows taking into account inflation, taxes and fees.","date":"May 23, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/inflation.jpeg","colab":"https://drive.google.com/file/d/13aiLUQUIXhfjfnO_20U4Dap0tsfRiF-B/view?usp=sharing"},"excerpt":"","content":"\nInflation is the reason behind the erosion of the value of money over time. 1\\\\$ today is more valuable that 1\\\\$ after 5 years given a positive Inflation rate. I will not get into the details of what inflation is since there is lots of literature around that.\n\nIn this article I will showcase how to plan the financing of future expenses by including inflation, taxes on earnings and annual fees.\n\nLet's see that with an example. Let's assume that a person decides a job break for 3 years. Estimates his/her monthly expenses to be 300$. What is the initial amount of money the person needs to have to be able to fulfill their goal?\n\nThe quick response to that is $3 * 12 * 300 = 10800\\$$! but that is not actually true due to the inflation. Let's (for the sake of the example) say that the inflation is 2% per year. That means that 1\\$ today will be valued at $1*\\frac{1}{1 + .02}=0.9804\\$$ in one year from now.\n\nSo, for the first year, the person will pay 3600\\\\$ but the actual cost of 3600\\$ today is $3600*(1+.02)^1 =3672\\$$. That means that there are $3672-3600=72\\$$ that are missing and should be added to the initial amount. The second year the person will need to pay 3600 again, but the value today is $3600*(1+.02)^2=3745.44\\$$. The third year, the situation is the same and the calculated value for the 3600\\$ would today be $3600*(1+.02)^3=3820.348\\$$.\n\n> Note: the notation $(1+i)^n$ where i is the annual inflation rate and n is the periods in years, is something we discussed in the [Geometric Progression and the Compounding of the Returns](post/geometric-progression-and-compounding-of-returns) article. Think of inflation as compounding with negative rate.\n\nSo far, the deficit for the first year is 72, for the second year is 145 and for the third year is 220. A total of 437\\$ needs to be added to our initial thought of 10800!\n\nThe example above can easily be extended to loan payments, house rent or even pension forecasting.\n\nLet's get into code and replicate the scenario above.\n\n\n```python\n%%capture\nimport pandas as pd\nimport numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display\n%matplotlib inline\n```\n\n\n```python\nmonthly_liabilities = 300\nyears = 3\nannual_liabilities = 12 * monthly_liabilities\ninflation_rate = 0.02\nliabilities = pd.Series(data=[annual_liabilities for i in range(years)], index=[i+1 for i in range(years)])\n\ndef liabilities_inflated(liabilities, inflation_rate):\n  years = liabilities.index\n  inflation_over_years = pd.Series(data=(1+inflation_rate)**years, index=years)\n  return liabilities * inflation_over_years\n\ninflated_liabilities = liabilities_inflated(liabilities, inflation_rate)\nprint(f\"Liabilities per year: {list(liabilities.values)}, \\\n        \\nInflated liabilities: {list(inflated_liabilities.values)}, \\\n        \\nTotal amount needed today: {inflated_liabilities.sum():.2f}, \\\n        \\nNo inflation case: {liabilities.sum():.2f}\")\n```\n\n    Liabilities per year: [3600, 3600, 3600],         \n    Inflated liabilities: [3672.0, 3745.44, 3820.3488000000007],         \n    Total amount needed today: 11237.79,         \n    No inflation case: 10800.00\n\n\n## Balancing inflation through investing\n\nInflation is eroding the value of money when they stand still! However, a balanced situation can be achieved if the money is invested to something that yields a return equal (or even better, more) than the inflation rate. There are several options to that, from 0 risk saving accounts that pay an interest, to a type of bond that increases the risk a bit and gives a better return (or even inflation-adjusted bonds), to more risky assets like equity funds that might yield an even better return.\n\nIn any case, we should take into account the taxes on profits and the administration fees each investment scenario requires.\n\nLet's go back to our example and say that we found a savings account that pays 2% annually in interest without fees and taxes. That would mean that we could invest 10800 to this account today and each year pay out the bill.\n\nBut what if there was a 10% tax on the profits from this interest? How much money would we need today to start with?\n\n\n```python\ninvestment_annual_return = 0.02\ninvestment_profit_tax = 0.1\n```\n\n\n```python\ndef discount(t, r, earnings_tax):\n  \"\"\"\n    Computes the amount needed today which when invested for t periods with\n    an r return per period and earning_tax on the profits, will equal to 1$  \n  \"\"\"\n  return (1 + r - r*earnings_tax)**-t \n\ndef present_value(liabilities, return_annual, earnings_tax):\n  \"\"\"\n    Given a list of liabilities, an annual return and tax rate on earnings,\n    returns the amount of money needed today.\n  \"\"\"\n  dates = liabilities.index\n  discounts = discount(dates, return_annual, earnings_tax)\n  return (discounts * liabilities).sum()\n```\n\n\n```python\npresent_value(inflated_liabilities, investment_annual_return, investment_profit_tax)\n```\n    10842.491757684631\n\n\n\nMoving forward, lets say that instead of a savings account we invest in funds that have an annual return of 2%, a profit tax of 10% and an annual administration fee of 0.5% on the total invested amount.\n\nWhat should the initial investment be?\n\n\n```python\nannual_fee = .005 # percentage of total amount per year\n```\n\n\n```python\ndef present_value_with_fees(liabilities, return_annual, earnings_tax, annual_fee):\n  dates = liabilities.index\n  discounts = discount(dates, return_annual, earnings_tax)\n  liabilities_with_fees = [liabilities[len(dates) +1 - i] * (1-annual_fee)**-i for i in list(dates)[::-1]]\n  return ((discounts * liabilities_with_fees).values).sum()\n  \n```\n\n\n```python\npresent_value_with_fees(inflated_liabilities, investment_annual_return, investment_profit_tax, annual_fee)\n```\n    10951.755210394398\n\n\n\nIt becomes apparent that taxes and fees play an important role in calculating future cash flows.\n\n## The Funding Ratio\n\nThe funding ratio is a percentage that shows how much our current assets can contribute to future cash flows. A 100% funding ratio means that the initial amount of assets will be just enough to fulfill our goals. A ratio below 100% means that we will need more assets to cover for the future flows and a ratio more than 100% means that we are overfunded and as such we have a surplus of money we can use to fund other goals.\n\n\n```python\ndef funding_ratio(current_assets_value, liabilities, return_annual, earnings_tax, annual_fee):\n  return 100*current_assets_value/present_value_with_fees(liabilities, return_annual, earnings_tax, annual_fee)\n\ndef show_funding_ratio(monthly_liabilities, years, inflation_rate, current_assets_value, return_annual, earnings_tax, annual_fee):\n  annual_liabilities = 12 * monthly_liabilities\n  liabilities = pd.Series(data=[annual_liabilities for i in range(years)], index=[i+1 for i in range(years)])\n  inflated_liabilities = liabilities_inflated(liabilities, inflation_rate)\n  fr = funding_ratio(current_assets_value, inflated_liabilities, return_annual, earnings_tax, annual_fee)\n  print(f'{fr:.2f}')\n```\n\n\n```python\nfunding_ratio(10951.76, inflated_liabilities, investment_annual_return, investment_profit_tax, annual_fee)\n```\n    100.00004373368022\n\n\nBelow I have included a widget which helps me plan for my future goals. I will try to have it as an HTML widget at some point but for the ones who are interested try the [colab](https://drive.google.com/file/d/13aiLUQUIXhfjfnO_20U4Dap0tsfRiF-B/view?usp=sharing) representation of this article. \n\n```python\ncontrols = widgets.interactive(show_funding_ratio,\n                               monthly_liabilities = widgets.IntSlider(min=100, max=100000, step=100, value=300),\n                               years = widgets.IntSlider(min=3, max=100, step=1, value=3),\n                               inflation_rate = widgets.FloatSlider(min=0.01, max=0.3, step=.01, value=.02),\n                               current_assets_value = widgets.IntSlider(min=10000, max=30000000, step=1000, value=10800),\n                               return_annual = widgets.FloatSlider(min=0.01, max=0.2, step=.01, value=.08),\n                               earnings_tax = widgets.FloatSlider(min=0.1, max=.5, step=.02, value=.3),\n                               annual_fee = widgets.FloatSlider(min=0.01, max=.05, step=.01, value=.03))\ndisplay(controls)\n```\n\nEnjoy!"}},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg","slug":"python","count":10},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg","slug":"statistics","count":4},{"id":"cryptos","name":"cryptos","image":"bitcoin.png","description":"The amazing world of Blockchain opens one more chapter in the Investing.","color":"bg-green-300","icon":"bitcoin.svg","slug":"cryptos","count":1}],"sortedTopics":[{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300","slug":"investing","count":8},{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300","slug":"mathematics","count":4},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300","slug":"automation","count":2}],"allTopics":[{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"}],"slug":"modern-portfolio-theory"},"__N_SSG":true}