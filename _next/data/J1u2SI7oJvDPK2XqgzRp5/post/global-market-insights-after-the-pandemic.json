{"pageProps":{"postData":{"frontmatter":{"title":"Global Market Insights after the Pandemic.","description":"What are the projections after the Pandemic? Let's explore ...","date":"January 21, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/after-covid-19.jpeg","colab":"https://colab.research.google.com/drive/1nPyDupJHidnP5DLfAEKX3eYiY-MHCDmM?usp=sharing"},"post":{"content":"\n2020 was a difficult year and most of the market segments diverged from the projections! With the new facts at hand, and the emerging needs after the pandemic, we should expect changes in the market.\n\nBelow is an attempt to get a feeling of the emerging sectors by analyzing some [Global Market Insights](https://www.gminsights.com/) reports.\n\nAnd as always, let's automate ...\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n!pip install requests beautifulsoup4\nimport requests\nimport re\nfrom bs4 import BeautifulSoup\nimport string\nimport pandas as pd\n```\n\n</p>\n</details>\n\n```python\nclass GlobalMarketInsights:\n  __DEFAULT_BASE_URL = 'https://www.gminsights.com/industry-reports'\n  \n  @staticmethod\n  def _escape(input):\n    printable = string.ascii_letters + string.digits + string.punctuation + ' '\n    return ''.join(c if c in printable else ' ' for c in input )\n\n  def _description_matcher(self, descr):\n    descr = GlobalMarketInsights._escape(descr.replace('\\t', ' ').replace('\\n', ' ').replace('\\r', ' '))\n\n    start_date = end_date = percentage = market_name = None\n\n    r1 = re.search('^(.*) (?:Market|Aftermarket) .*$', descr, re.IGNORECASE)\n    if r1:\n      market_name = r1.group(1).strip()\n\n    r2 = re.search('.* between (\\d+) (?:and|to) (\\d+)', descr)\n    if r2:\n      start_date = r2.group(1).strip()\n      end_date = r2.group(2).strip()\n    \n    r3 = re.search('.* (?:from|of) (\\d+) to (\\d+)', descr)\n    if r3:\n      start_date = r3.group(1).strip()\n      end_date = r3.group(2).strip()\n    \n    r4 = re.search('([-+]?\\d*\\.\\d+|\\d+)%', descr)\n    if r4:\n      percentage = r4.group(1)\n    \n    if None in (market_name, percentage, start_date, end_date):\n      raise Exception(f\"Couldn't parse: {descr}\")\n    else:\n      return {\n        \"market\": market_name,\n        \"percentage\" : float(percentage),\n        \"start\": int(start_date),\n        \"end\": int(end_date)\n      }\n\n  def get(self, page=1):\n    page = requests.get(f\"{GlobalMarketInsights.__DEFAULT_BASE_URL}?page={page}\")\n    soup = BeautifulSoup(page.text, 'html.parser')\n    single_rds = soup.find_all('div', class_='single_rd')\n    reports = []\n    for single_rd in single_rds:\n      single_rd_children = single_rd.findChildren()\n      for single_rd_child in single_rd_children:\n        if single_rd_child.has_attr('class') and single_rd_child['class'][0] == 'rd_desc':\n          description = single_rd_child.getText()\n          try:\n            reports.append(self._description_matcher(description))\n          except Exception as e:\n            # print(e)\n            pass\n          break\n    return reports\n  \n  def fetch_all_reports(self):\n    # get the total number of pages and start iterating\n    page = requests.get(f\"{GlobalMarketInsights.__DEFAULT_BASE_URL}?page=1\")\n    lun_q = 'Displaying \\d+ records out of (\\d+) on Page \\d+ of (\\d+)'\n    r = re.search(lun_q, page.text)\n    if r:\n        number_of_records = r.group(1)\n        number_of_pages = r.group(2)\n    else:\n      raise Exception('No pages or data!')\n    \n    all_reports = []\n    for page in range(1, int(number_of_pages) + 1, 1):\n      page_reports = self.get(page=page)\n      all_reports += page_reports\n\n    return int(number_of_records), all_reports\n```\n\nScraping web pages is always challenging. In this case especially, the task was a bit tedious since the different report descriptions where not following a unique pattern.\n\n```python\nglobal_market_insights = GlobalMarketInsights()\nnumber_of_records, all_reports = global_market_insights.fetch_all_reports()\nprint(f\"Parsed {len(all_reports)} out of {number_of_records} report descriptions!\")\n```\n    Parsed 1200 out of 1964 report descriptions!\n\nNext, we add the reports to a dataframe for better presentation and easier data manipulation.\n\n```python\ngmi_reports_df = pd.DataFrame(all_reports) \ngmi_reports_df.head()\n```\n\n<div>\n<table border=\"1\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>market</th>\n      <th>percentage</th>\n      <th>start</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Food phosphate</td>\n      <td>6.0</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Supply Chain Analytics</td>\n      <td>16.0</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Cooking Coconut Milk</td>\n      <td>8.5</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Steel Rebar</td>\n      <td>4.0</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2,5-Dimethyl-2,4-Hexadiene</td>\n      <td>2.5</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nSo far, so good! Let's try to sort by percentage and see which sector is projected to perform more than 30% the following years.\n\n```python\nsector_projection_ascending = gmi_reports_df.sort_values('percentage', ascending=False)\nsector_projection_ascending.loc[(sector_projection_ascending['percentage']>30) & (sector_projection_ascending['start']>=2020)]\n```\n\n<div>\n<table border=\"1\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>market</th>\n      <th>percentage</th>\n      <th>start</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>435</th>\n      <td>SD-WAN</td>\n      <td>60.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>Cannabidiol (CBD)</td>\n      <td>52.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>(Light Fidelity) Li-Fi</td>\n      <td>50.0</td>\n      <td>2020</td>\n      <td>2030</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Healthcare Artificial Intelligence</td>\n      <td>43.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>Automotive Subscription Services</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>AI in Manufacturing</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>363</th>\n      <td>Artificial Intelligence (AI) in BFSI</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Robotic Process Automation</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>Fuel Cell Electric Vehicle</td>\n      <td>38.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>AI in Automotive</td>\n      <td>35.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>Artificial Intelligence Chipsets</td>\n      <td>35.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>Total Knee Replacement</td>\n      <td>34.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>212</th>\n      <td>Vaginal Rejuvenation</td>\n      <td>33.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>Carbon Wheels</td>\n      <td>32.3</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nIt is becoming pretty obvious that everything around Artificial Intelligence yields the best projections, and is an attractive area for investments :)\n","excerpt":""},"previousPost":{"slug":"are-stock-returns-normally-distributed","frontmatter":{"title":"Are Stock Returns Normally Distributed?","description":"What do you think?","date":"December 20, 2020","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/normal-dist-with-hist.png","colab":"https://colab.research.google.com/drive/1iYrNJ9ISktohy1dG2s16_FZKakB8FLU5?usp=sharing"},"excerpt":"","content":"\nIn a previous post we talked about the [Higher Moments of a Distribution](/post/higher-moments-of-a-distribution). We saw that skewness and kurtosis are two attributes that can identify if a distribution is normal or not (skewnes = 0 & kurtosis = 3).\n\nLet's try this approach on the MSFT stock.\n\nFirst step is to to fetch the data and print the returns.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n\nmatplotlib.rcParams['figure.figsize'] = (10.0, 5.0)\nmatplotlib.style.use('ggplot')\n```\n\n</p>\n</details>\n\n```python\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n\ndef log_rets(S):\n    rets = np.log(S) - np.log( S.shift(1))\n    return rets[1:]\n\nstock_prices = retrieve_stock_data(\"MSFT\", \"2000-01-01\", \"2020-01-01\")\n\nrets = normal_rets(stock_prices).dropna()\nrets.columns = ['returns']\nrets.plot(figsize=(14,7))\nplt.title(\"Daily returns\", weight=\"bold\");\n```\n\n![png](are-stock-returns-normally-distributed/are-stock-returns-normally-distributed-1-1.png)\n\nLet's find skewness and kurtosis:\n\n```python\nfrom scipy.stats import kurtosis, skew\nskew(rets, bias=False)[0], kurtosis(rets, bias=False, fisher=False)[0]\n```\n    (0.20887713542026032, 13.229622042763442)\n\nIt is obvious that the MSFT stock returns (for that period) do not comply with the kurtosis and skewness of a normal distribution. Same, of course, happens if we get the log returns instead.\n\n```python\nlog_msft_rets = log_rets(stock_prices).dropna()\nskew(log_msft_rets, bias=False)[0], kurtosis(log_msft_rets, bias=False, fisher=False)[0]\n```\n    (-0.12981025399984283, 12.81366131030108)\n\n## Normality Tests\n\nThere are several interrelated approaches to determining normality:\n\n* Histogram with the normal curve superimposed. Unfortunately, there is no automated way to represent the \"fitness\" as a value. This approach is empirical mostly and requires experience.\n* Skewness & Kurtosis Tests.\n* Normality plots. “Normal Q-Q Plot” provides a graphical way to determine the level of normality.\n* Normality tests. The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above .05 indicates normality. If the test is significant (less than .05), then the data are non-normal.\n\n### Histogram & Normal PDF\n\n```python\nfrom scipy.stats import norm\nx = np.linspace(min(rets.returns.values), max(rets.returns.values))\nax = rets.plot(kind='hist', bins=500, density=True)\npdf_fitted = norm.pdf(x, *norm.fit(rets.returns.values))\npd.Series(pdf_fitted, x).plot(ax=ax)\nplt.show()\n```\n    \n![png](are-stock-returns-normally-distributed/are-stock-returns-normally-distributed-8-0.png)\n\n### Skewness & Kurtosis Tests\n\n```python\nfrom scipy import stats\nstats.kurtosistest(rets.returns)\n```\n    KurtosistestResult(statistic=29.93227785492693, pvalue=7.484189304773088e-197)\n\n```python\nstats.skewtest(rets.returns)\n```\n    SkewtestResult(statistic=5.99114785753993, pvalue=2.083650895527666e-09)\n\n### QQ-Plot\n\n```python\nfrom numpy.random import seed\nfrom statsmodels.graphics.gofplots import qqplot\nfrom matplotlib import pyplot\nseed(1)\nqqplot(rets.returns, line='s')\npyplot.show()\n```\n    \n![png](are-stock-returns-normally-distributed/are-stock-returns-normally-distributed-13-0.png)\n    \n\nThe Quantile-Quantile plot, as the name suggests, will compare the quantiles between the normal distribution and our data. We notice here that, the tails of the distribution of our data are diverging a lot from the normal distribution. This is what we would expect. Fat tails (leptokurtic)!\n\n### Statistical Normality Tests\n\nThe tests assume that the sample was drawn from a Gaussian distribution. Technically this is called the null hypothesis, or H0. A threshold level is chosen called alpha, typically 5% (or 0.05), that is used to interpret the p-value.\n\nIn the SciPy implementation of these tests, you can interpret the p value as follows.\n\n* p <= alpha: reject H0, not normal.\n* p > alpha: fail to reject H0, normal.\n\nThis means that, in general, we are seeking results with a larger p-value to confirm that our sample was likely drawn from a Gaussian distribution.\n\nA result above 5% does not mean that the null hypothesis is true. It means that it is very likely true given available evidence. The p-value is not the probability of the data fitting a Gaussian distribution; it can be thought of as a value that helps us interpret the statistical test.\n\n#### Kolmogorov-Smirnov test (K-S)\n\n\n```python\nkstest = stats.kstest(rets.returns, 'norm')\nkstest.pvalue > 0.05\n```\n    False\n\n#### Shapiro-Wilk Test\n\n```python\nshapiro_stat, shapiro_p = stats.shapiro(rets.returns)\nshapiro_p > 0.05\n```\n    False\n\n\n#### D’Agostino’s K^2 Test\n\nThe D’Agostino’s K^2 test calculates summary statistics from the data, namely kurtosis and skewness, to determine if the data distribution departs from the normal distribution. (named for Ralph D’Agostino)\n\n* Skew is a quantification of how much a distribution is pushed left or right, a measure of asymmetry in the distribution.\n* Kurtosis quantifies how much of the distribution is in the tail.\n\nIt is a simple and commonly used statistical test for normality.\n\n```python\nseed(1)\ndagostino_stat, dagostino_p = stats.normaltest(rets.returns)\ndagostino_p > 0.05\n```\n    False\n\n#### Jarque-Bera Test for Normality\n\n```python\njarque_bera_stat, jarque_bera_p = stats.jarque_bera(rets.returns)\njarque_bera_p > 0.05\n```\n    False\n\n## Conclusion\n\nIn this article we went through some techniques that allow us identify if stock returns are normally distributed. We saw, with examples, that returns (arithmetic, or log) are not normally distributed but instead exhibit fat tails. We cannot generalize, of course, just by looking into one stock, but I will leave that as a small exercise to the curious readers.\n\nThe question is still... Since the returns are not following a normal distribution, then what type of distribution do they follow?\n\nThe answer to that in [Fit Multiple Distributions to Asset Returns!](/post/fit-distributions-to-asset-returns)\n"},"nextPost":{"slug":"portfolio-expected-return-and-risk","frontmatter":{"title":"Return & Volatility of a Multi-Asset Portfolio","description":"Maths are magical :) And why diversification makes sense!","date":"January 31, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/magic.png","colab":"https://colab.research.google.com/drive/1lNirrCFUfWaZ_Cci-mmsio79wX_KwR5b?usp=sharing"},"excerpt":"","content":"\nIn previous posts we talked about the [expected return](/post/measures-of-location) (mean value of a distribution) and the [volatility](/post/measures-of-variability) (standard deviation) of an asset.\n\nBut, in investing, we rarely hold a portfolio of just one stock! Let's start then, by picking two stocks.\n\nThe first question is: OK, what percentage of the total investment amount shall we allocate to stock A and what to stock B?\n\nIf we allocate 100% to A and 0% to B or the other way around, then we get into the \"one asset\" portfolio and that is not desirable! Let's assume that we set 50% on asset A and 50% on asset B. Let's also call this percentage allocations, weights (w_A, w_B respectively).\n\nIn that case, we are asked to come up with the return (mean) and the volatility (standard deviation) of the portfolio.\n\nSomeone would blindly assume that the `Return = (w_A * R_A) + (w_B * R_B)` and `Volatility = (w_A * std_A) + (w_B * std_B)`. Well, maths keep always surprising us, and that is the case here!\n\nWhile indeed the return of the 2 asset portfolio is the average weighted returns,\n\n$$\nR_{A,B} = w_A*R_A + w_B*R_B  \\qquad (1)\n$$\n\nthe volatility is\n\n$$\n\\sigma_{A,B}=\\sqrt{\\sigma_A^2w_A^2 + \\sigma_B^2w_B^2 + 2w_Aw_B\\sigma_A\\sigma_B\\rho_{A,B}}  \\qquad (2)\n$$\n\nThis second (2) equation tells us that the standard deviation of a 2 asset distribution is equal to the square root of the variance of asset A multiplied by the squared weight of A plus the variance of asset B multiplied by the squared weight of B, plus twice the product of variance of A times the variance of B times the weight of A times the weight of B times the correlation coefficient of A and B!\n\nSo far so good! But where exactly does the magic begin? Well, the correlation coefficient is not always a positive number :O\n\nThe correlation coefficient can take values between -1 and 1. -1 when the two assets are totally uncorrelated, which means that when the first asset goes up the other goes down at the same pace and same angle. 1 when both assets move to the same direction with the same pace and same angle (either positive or negative direction). Values between -1 and 1 indicate a more loose correlation, but show the trend.\n\nBack to the equation (2). If we have a negative correlation of the assets, the total volatility is less than the average volatility, and if we have a positive correlation the total volatility is more that the average.\n\nIt becomes pretty obvious that by just combining two non correlated assets we can achieve volatility sometimes even smaller than the assets' individually. Who wouldn't want that!\n\n> Keep in mind that $\\rho_{A,B} = \\frac{cov_{A,B}}{\\sigma_A\\sigma_B}$ where $cov_{A,B}$ is the covariance of the two variables.\n\nLet's see an example...\n\n## Real example of a two asset portfolio\n\nAt first, let's set the ground work to be able to fetch some stock prices.\n\n<details><summary>Package Installation</summary>\n<p>\n\n```python\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n```\n\n</p>\n</details>\n\n\n```python\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n```\n\nWe are now ready to fetch prices. I have picked Microsoft Corporation (MSFT) and Alpha Pro Tech, Ltd. (APT). Below we see how the price of the stocks unfolded throughout 2020! \n\n```python\nmsft_stock_prices = retrieve_stock_data(\"MSFT\", \"2020-01-01\", \"2021-01-01\")\nmsft_rets = normal_rets(msft_stock_prices).dropna()\nmsft_rets.columns = ['returns']\n\napt_stock_prices = retrieve_stock_data(\"APT\", \"2020-01-01\", \"2021-01-01\")\napt_rets = normal_rets(apt_stock_prices).dropna()\napt_rets.columns = ['returns']\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\nmsft_stock_prices.plot(figsize=(14,7), ax=ax1)\napt_stock_prices.plot(figsize=(14,7), ax=ax2)\nax1.get_legend().remove()\nax2.get_legend().remove()\nax1.title.set_text('MSFT Price Chart')\nax2.title.set_text('APT Price Chart')\nplt.show()\n```\n\n![png](portfolio-expected-return-and-risk/portfolio-expected-return-and-risk_4_0.png)\n\nThe graphs show some king of un-correlation. When one stock goes up the other goes down and vice versa. Let's explore the average return, standard deviation and correlation of the stocks.\n\n```python\nmsft_rets.mean().values[0], apt_rets.mean().values[0]\n```\n    (0.0017171460669071206, 0.009654517798428635)\n\n\n```python\nmsft_rets.std().values[0], apt_rets.std().values[0]\n```\n    (0.027679154652983044, 0.10987021868530256)\n\n```python\nreturns = msft_rets.merge(apt_rets, left_index=True, right_index=True)\nreturns.columns = ['MSFT', 'APT']\nreturns.corr()\n```\n\n<div>\n<table border=\"1\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>APT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MSFT</th>\n      <td>1.0000</td>\n      <td>-0.2182</td>\n    </tr>\n    <tr>\n      <th>APT</th>\n      <td>-0.2182</td>\n      <td>1.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\nObviously, both stocks yield a positive average daily return (small but positive), and while MSFT has a volatility around ~2.8%, APT is at ~11%, which denotes a very volatile asset. As expected, the correlation of the two assets is negative.\n\nLet us now try to construct a portfolio of these two assets. From equations (1) and (2) we see that the weights are variable. We should also notice that the return is not a series of returns anymore but a single value. This value is the total return of an asset over the year. It is the so called [`Annualized Return`](/post/geometric-progression-and-compounding-of-returns).\n\n\n```python\ndef annualize_rets(r, periods_per_year):\n    compounded_growth = (1+r).prod()\n    n_periods = r.shape[0]\n    return compounded_growth**(periods_per_year/n_periods)-1\n\nannualized_returns = annualize_rets(returns, 252)\nannualized_returns\n```\n    MSFT    0.399429\n    APT     2.222543\n    dtype: float64\n\n\nAs you can see the annual return for 2020 for MSFT was ~40%, while for APT was ~220%! It is pretty obvious from the price graphs :)\n\nNow, we move on and try to generate some portfolios where we assign different weights to the assets and try to calculate the return and the volatility of the portfolio. I will not get into what transposing a matrix means in algebra since it is not the focus of this post. Please check [this wikipedia article](https://en.wikipedia.org/wiki/Transpose) for more info.\n\n\n```python\n# from equation (1)\ndef portfolio_return(weights, returns):\n    return weights.T @ returns\n\n# from equation (2)\ndef portfolio_vol(weights, covariance_matrix):\n    return (weights.T @ covariance_matrix @ weights)**0.5\n\n# first we construct 10 pairs of weights like [(0.1,0.9), (0.2,0.8) ...]\nweights = [np.array([w, 1-w]) for w in np.linspace(0, 1, 10)]\n\n# then we calculate the return of the portfolio for each pair of weights\nportfolio_returns = [portfolio_return(w, annualized_returns) for w in weights]\n\n# and the volatility of the portfolio for each pair of weights\nvols = [portfolio_vol(w, returns.cov()) for w in weights]\n\nef = pd.DataFrame({\n    \"Return\": portfolio_returns, \n    \"Volatility\": vols,\n    \"weights\": weights\n})\n\nax = ef.plot(x=\"Volatility\", y=\"Return\", style=\".-\", figsize=(11,6),\n             title=\"2 Asset Portfolio Risk/Return\", legend=False)\nplt.ylabel(\"Return\")\n\ndef label_point(x, y, val, ax):\n  a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n  for i, point in a.iterrows():\n    prettified_p = f\"({round(point['val'][0], 2)},{round(point['val'][1], 2)})\"\n    ax.text(point['x'], point['y'], prettified_p)\n\nlabel_point(ef.Volatility, ef.Return, ef.weights, ax)\n```\n  \n![png](portfolio-expected-return-and-risk/portfolio-expected-return-and-risk_12_0.png)\n\nWhat the graph above tells us is that by combining the two assets we are able to achieve a total volatility (risk) that is less than each asset's individual volatility!\n\nObserve the left most point on the graph!\n\nIn a next post we will calculate the optimal weights that minimize the risk of a portfolio as well as explore portfolios with more than 2 assets.\n\n## Some More Notes\n\n* The approach I followed above is not something new. Is called [Markowitz Model](https://en.wikipedia.org/wiki/Markowitz_model) and won a [Nobel Price](https://www.nobelprize.org/prizes/economic-sciences/1990/press-release/) in 1990.\n* I tried to oversimplify the example, just to show the basics.\n* I randomly picked the two assets, in the example, from [IMPACTOPIA](http://www.market-topology.com/correlation/MSFT?etf=0).\n* We will prove, later on, that volatility changes over time :) and that would normally lead to rebalances.\n* As always, `Historical returns are no guarantee of future returns.`\n\nStay tuned ...\n"}},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg","slug":"python","count":19},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg","slug":"statistics","count":4},{"id":"cryptos","name":"cryptos","image":"bitcoin.png","description":"The amazing world of Blockchain opens one more chapter in the Investing.","color":"bg-green-300","icon":"bitcoin.svg","slug":"cryptos","count":1}],"sortedTopics":[{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300","slug":"investing","count":14},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300","slug":"automation","count":5},{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300","slug":"mathematics","count":4},{"id":"booknotes","name":"Book Notes","image":"booknotes.jpg","description":"Reading books, and keeping notes! I seek to what the books have to tell me.","color":"bg-green-300","slug":"booknotes","count":1}],"allTopics":[{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... Investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},{"id":"booknotes","name":"Book Notes","image":"booknotes.jpg","description":"Reading books, and keeping notes! I seek to what the books have to tell me.","color":"bg-green-300"}],"slug":"global-market-insights-after-the-pandemic"},"__N_SSG":true}