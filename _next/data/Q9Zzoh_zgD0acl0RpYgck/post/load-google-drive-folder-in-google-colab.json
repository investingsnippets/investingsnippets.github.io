{"pageProps":{"postData":{"frontmatter":{"title":"Load Google Drive folder in Google Colab","description":"How to mount a Google Drive in Google Colab and load some stock data.","date":"December 7, 2020","topic":{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/google-colab.png"},"post":{"content":"\nThere are several cases where:\n\n* fetching stock prices is not possible through python libraries like `yahoofinancials` or other APIs\n* you want to load the same data over and over again (for parallelization)\n* you want to use you python modules without publishing them to a registry\n\nIn these cases I find it very handy to store my data (csv format) and my modules in google drive.\n\nHowever, loading the data to Google Colab turned into pain since I had to manually upload the files each time I wanted to run a notebook.\n\nTo avoid this situation I mount google drive and:\n\n1. add the folder with my python modules to the path\n2. copy the data to the Colab data folder\n\n```python\nimport warnings\nwarnings.simplefilter('ignore')\n\n%config InlineBackend.figure_formats=[\"png\"]\n\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nimport os\nimport sys\nimport glob\nfrom shutil import copytree, copy\n\n# This will add my python modules in the path\ngdrive_base_path = '/content/drive/My Drive/my-python-modules'\nsys.path.append(gdrive_base_path)\n\ntry:\n  # copy the data we need\n  copytree('/content/drive/My Drive/Colab Notebooks/data', '/content/data')\nexcept Exception as e:\n  pass\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as st\n```\n\nSince the folder that contain my python modules is now in the system path, I'm able to\n\n```python\nimport my_module\n```\n\nand whenever I change my modules (add more functionality, improvements), it practically saves it to Google Drive, since it is a pure mount.\n\nWhen it comes to loading data, then I simply:\n\n```python\nMSFT = pd.read_csv('data/msft_daily.csv', parse_dates=True, index_col=0, header=0)\n```\n\n> **Note**: Since `copytree` is used, uploading new data files to Google Colab, will not automatically save it to Google Drive!\n","excerpt":""},"previousPost":{"slug":"geometric-progression-and-compounding-of-returns","frontmatter":{"title":"Geometric Progression and the Compounding of the Returns","description":"Your savings account offers a 1% annual interest! The account balance in 10 years? +10%? Naaaah","date":"December 6, 2020","topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/fractal-star.png","tags":[]},"excerpt":"","content":"\n## Compounding of Returns\n\nLet us consider the following scenario. A stock asset yields an average monthly return R of 2% for the last 12 months. If we had invested 100$ on this asset a year ago, what would be the outcome of this investment today?\n\nConsider the initial asset price $P_0 = 100$. After the first month of the investment the outcome would be:\n\n$$\nP_1 = 100 + (100 * 0.02) = (100 * 1) + (100 * 0.02) = 100 * (1 + 0.02) = P_0 * 1.02\n$$\n\nThe outcome after the 2nd month would be:\n\n$$\nP_2 = P_1 + (P_1 * 0.02) = P_1 * 1.02\n$$\n\n3rd month:\n\n$$\nP_3 = P_2 + (P_2 * 0.02) = P_2 * 1.02\n$$\n\n(ν-1)th month:\n\n$$\nP_{ν-1} = P_{ν-2} + (P_{ν-2} * 0.02) = P_{ν-2} * 1.02\n$$\n\nνth month:\n\n$$\nP_ν = P_{ν-1} + (P_{ν-1} * 0.02) = P_{ν-1} * 1.02\n$$\n\nIt becomes obvious from the above that generating each element in the sequence $P_0, P_1, ..., P_{ν-1}, P_ν$ follows a pattern where the present number is generated by multiplying the previous number with a constant.\n\nIn our case, the constant is the average monthly return + 1. If we combine the previous equations in an attempt to formulate a final equation that provides the νth based on the $P_0$ element then:\n\n$$\nP_ν = P_{ν-1} * 1.02 = (P_{ν-2} * 1.02) * 1.02 = P_{ν-2} * 1.02^2 = .... = P_0 * (1 + 0.02)^ν\n$$\n\nSo, after 12 months we will have $100 * 1.02^{12} = 126.82$!\n\nThe generic equation is:\n\n$$\nP_ν = P_0 * (1 + R)^ν \\qquad (1)\n$$\n\n## Geometric Progression\n\nIn calculus, a sequence of numbers where each term after the first is found by multiplying the previous one by a fixed, non-one number, called `geometric progression`. The fixed number is called, the `common ratio`.\n\n$$\na, ar, ar^2, ... , ar^{n-1}\n$$\n\n$1+R$ is the `common ratio` in our case. \n\nSome interesting properties to notice:\n\n* a common ratio greater than 1, will produce exponential growth towards positive infinity!\n* a common ratio greater than 0 and up to 1, will produce exponential decay towards zero!\n\nIn the example above the way we used the return to calculate the end result is called **Compounding of Returns**.\n\n## From annual to periodic returns\n\nLet's see the example above from another angle. Let's say that an assets had a total return over the last year of 10%. What was the average monthly return of the asset?\n\nWe calculate a simple/arithmetic return (pandas.pct_change) like:\n\n$$\nR_i = \\frac{P_i-P_{i-1}}{P_{i-1}} = \\frac{P_i}{P_{i-1}} - 1 \\qquad (2)\n$$\n\nwhere $P_i$ is the price of the asset on the period $i$.\n\nBy assuming (for simplicity) that $P_0 = 1$ and from (2), $P_1 = P_0 * (1 + R_{annual})$\n\nThen from (1),\n\n$$\n1 + R_{annual} = 1 * (1 + R_{month})^12 => R_{month} = \\sqrt[12]{1+R_{annual}} - 1 \\qquad (3)\n$$\n\nWe use ν = 12, because 1 year = 12 months (=> 12 compounding periods). If instead of monthly returns we were asked to find quarter returns then we would use ν = 4 ($R_{quarter} = \\sqrt[4]{1+R_{annual}} - 1$).\n\nFinally, from (3),\n\n$$\nR_{monthly} = \\sqrt[12]{1 + 0.1} - 1 = \\sqrt[12]{1.1} - 1 = 1.0079 - 1 = 0.0079\n$$\n\n## Compounding variable returns\n\nIt is commonly accepted that returns do not stay the same over periods. For example, the average return of this month is not the same as the one from last month! However, the same principle of compounding applies in this case too. Let's see an example.\n\nAn asset yields the following return for the past couple of months $0.021, 0.032, -0.018, 0.06, -0.043, 0.048$. The total return is the **product** of the individual returns when 1 is added to them:\n\n$$\nR = (1 + 0.021) * (1 + 0.032) * (1 - 0.018) * (1 + 0.06) * (1 - 0.043) * (1 + 0.048) = ...\n$$\n\nThis, resonates with the equation (1) above where for a fixed return we have $R_{total} = (1 + R_{fixed})^{number-of-periods}$"},"nextPost":{"slug":"higher-moments-of-a-distribution","frontmatter":{"title":"Higher Moments of a Distribution","description":"See how higher moments can reveal more characteristics of a data series.","date":"December 17, 2020","topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/skewness-kurtosis.png","colab":"https://colab.research.google.com/drive/1k2Ek0o9UpV2f7NcS0R6aZ36UDvR24BRP?usp=sharing"},"excerpt":"","content":"\nWe have already discussed about the [mean](/post/measures-of-location) and the [variance](/post/measures-of-variability) of a series of data. \n\nMean is also called the 1st moment and variance the 2nd moment. The type to get a moment (the movement) about a non-random value c, of a density function is:\n\n$$\nE[(X-c)^κ] = \\int_{-\\infty}^{+\\infty} (x-c)^k f(x) dx  \\qquad (1)\n$$\n\nBefore explaining the moments, we should first understand what a density function is. Commonly called probability density function (PDF).\n\n## Density Function\n\n30 people are gathered in a house party. Let us measure their weights:\n\n\n```\nimport pandas as pd\nm_v = [56.8, 81.3, 47.9, 32.5, 24.1, 25.3, 14.3, 29.4, 71.3, 86.0, 54.2, 15.2,\n       54.7, 25.1, 49.5, 1.9, 70.0, 69.6, 75.4, 38.9, 49.2, 22.5, 68.6, 60.1,\n       52.7, 109.7, 38.9, 45.9, 47.7, 52.9]\nvalues = pd.Series(m_v)\nvalues.describe()\n```\n\n    count     30.000000\n    mean      49.053333\n    std       24.001634\n    min        1.900000\n    25%       30.175000\n    50%       49.350000\n    75%       66.475000\n    max      109.700000\n    dtype: float64\n\n\n\nWhat if we change the way we present the data, and instead of having them in a simple series, we try to split them up into buckets.\n\nWe will get the, so called, histogram of the values. It shows how the probabilities of measurement are distributed.\n\n\n```\nimport matplotlib.pyplot as plt\nhistogram = values.plot.hist(bins=10, figsize=(10,5))\nplt.show()\n```\n  \n![png](higher-moments-of-a-distribution/higher-moments-of-a-distribution_3_0.png)\n    \n\n\nIf we ask the question: What is the probability, the next person that joins the party, weights between 80 and 90 kilos?\n\nTo answer this question, we need to imagine as if the upper boundaries of the blue colored space above, are a continuous line, a curve. This curve is what we call PDF or density function.\n\n\n```\nfrom scipy.stats import norm\nimport numpy as np\nx = np.linspace(min(values), max(values))\nax = values.plot(kind='hist', bins=10, figsize=(10,5), density=True)\npdf_fitted = norm.pdf(x, *norm.fit(values))\npd.Series(pdf_fitted, x).plot(ax=ax)\nplt.show()\n```\n    \n![png](higher-moments-of-a-distribution/higher-moments-of-a-distribution_5_0.png)\n\nWe observe that the curve is not a perfect fit. It is an approximation and there are hundreds of different curves we can plot and several of them will be very close to fitting the data (I will show that in another post).\n\nIn this case above, I have intentionally picked the data as such to resemble the, so called, `normal` distribution.\n\nBack to our question now! The probability, the next person that joins the party, weights between 80 and 90 kilos, can be estimated by measuring the area below the curve for that bucket. So, if the whole area below the curve is 1 (zero moment, see below), the part that belongs to bucket 80-90 is a percentage :) and that is the probability we are after. The expression is:\n\n$$\nP( \\text{weight between 80 and 90 kilos} | \\text{mean=x and standard-deviation=y} )\n$$\n\nWhich is translated to: The probability a person weights between 80 and 90 kilos given an average of x and standard-deviation of y.\n\nAnd the area below the curve is the integral between the points:\n\n$$\nP = \\int_{x=80}^{x=90} f(x)dx \\text{  where f the density function}\n$$ \n\nThere are many pros in trying to use distributions to represent how the values in a dataset are distributed:\n\n* makes it easy to measure the areas below (with integrals, since the function of the curve is known)\n* the presentation is much better for the human\n* well known distributions have really nice properties\n\n## Zero Moment (Total Mass)\n\nThat means that in (1), k=0 and as such $(...)^0 = 1$. That leaves us with:\n\n$$\n\\int_{x=-\\infty}^{x=\\infty} f(x)dx = 1 \\qquad (2)\n$$\n\n> Probability Distributions are normalized quantities, that always sum to one. Think of that as the probability that at least one of the events in a sample space will occur. Isn't that 100%?\n\n## 1st Moment - Mean\n\n$$\nμ_1 = E[(X-0)^1] = E[X] = \\int_{-\\infty}^{+\\infty} xf(x)dx  \\qquad (3)\n$$\n\nc=0 in this case since we do not have an origin to get the moment (movement) about.\n\nFrom (3) is obvious that we talk about the mean and that alternatively talk about the balance of the total mass (the area below the curve) around a point :)\n\n## 2nd Moment - Variance\n\nFrom (1), we can take c=0 and k=2! But what will that show us? How the mass is balanced around again the same point, which in practice is the average again but squared? Doesn't provide much value in understanding our data.\n\nFor that reason we get $c=μ$ and that will start making sense, since we se how the mass is diverging from the mean. It will show the variance of the data around the mean :)\n\n$$\nVar = \\int_{-\\infty}^{+\\infty} (x-μ_x)^2f(x)dx  \\qquad (4)\n$$\n\n## 3rd Moment - Skewness\n\nFollowing the pattern above and using k=3 around the mean $c=μ$ then we get the skewness which measures the relative size of the two tails of a distribution.\n\n\n```\nfrom scipy.stats import skew\nskew(values, bias=False) # bias=False calculates the skewness and kurtosis of the sample as opposed to the population.\n```\n    0.274192939649461\n\n\nA left-skewed (negatively-skewed) distribution has a long left tail. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA right-skewed (positive-skew) distribution has a long right tail. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n![png](higher-moments-of-a-distribution/Relationship_between_mean_and_median_under_different_skewness.png)\n\n## 4th Moment - Kurtosis\n\nThe fourth central moment is a measure of the heaviness of the tail of the distribution.\n\n```\nfrom scipy.stats import kurtosis\nkurtosis(values, bias=False)\n```\n    0.14330737818315065\n\n![jpg](higher-moments-of-a-distribution/kurtosis-types.jpg)\n\n## Higher Moments of the Normal Distribution\n\n```\ndata = np.random.normal(0, 1, 10000000)\nplt.hist(data, bins='auto')\n\nprint(\"mean : \", np.mean(data))\nprint(\"var  : \", np.var(data))\nprint(\"skew : \", skew(data, bias=False))\nprint(\"kurt : \", kurtosis(data, bias=False, fisher=False))\n```\n\n    mean :  -0.00015674618345404924\n    var  :  1.0002202373014222\n    skew :  0.0007495220785926886\n    kurt :  3.0013415645199695\n \n![png](higher-moments-of-a-distribution/higher-moments-of-a-distribution_12_1.png)\n    \n\nFor a normal distribution the skeweness is zero and the kurtosis is 3. These properties are specific to the normal distribution and are used for normality testing of distributions. We will go deeper in that in a later post.\n"}},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg","slug":"python","count":6},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg","slug":"statistics","count":4}],"sortedTopics":[{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300","slug":"investing","count":4},{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300","slug":"mathematics","count":4},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300","slug":"automation","count":1}],"allTopics":[{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"}],"slug":"load-google-drive-folder-in-google-colab"},"__N_SSG":true}