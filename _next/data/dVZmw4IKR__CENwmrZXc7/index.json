{"pageProps":{"posts":[{"slug":"drawdown","frontmatter":{"title":"Drawdown","description":"One of the most famous Risk Management Indicators.","date":"February 18, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/drawdown.png","colab":"https://colab.research.google.com/drive/1LFUprI0yMOLPDK4yqgpRAi5sPrGrDhTi?usp=sharing"},"excerpt":"","content":"\nIn a previous [post](/post/from-portfolio-wealth-index-to-index-fund) we talked about the wealth index of an asset as well as a portfolio of assets. The idea of the `wealth index` is very powerful because it represents the cumulative profit of an asset (since it depends on the price returns).\n\nNow, if we have invested 100$ on an asset and we were asked to find the maximum loss, when did that happen and for how long did it last? We need to walk through our wealth index and find all the deeps, then see which one was the largest, when it happen and when it recovered to the previous value.\n\nWe employ a well known notion in Investing Risk Management called **Drawdown**.\n\n## Computing and Plotting the Drawdown of an asset\n\nFirst the ground code that allows us to fetch stock historical data.\n\n```\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    df.columns = [ticker]\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n```\n\nI'll randomly pick Apple's (AAPL) stock for this analysis.\n\n```\napple_stock_prices = retrieve_stock_data(\"AAPL\", \"1990-03-14\", \"2021-02-17\")\napple_rets = normal_rets(apple_stock_prices).dropna()\nfig, (ax1, ax2) = plt.subplots(2, sharex=True, figsize=(14,7))\nfig.suptitle(\"Apple's Price & Returns\")\napple_stock_prices.plot(ax=ax1, label='Price')\napple_rets.plot(ax=ax2, label='Returns')\nplt.legend(loc=\"upper left\")\nplt.show()\n```\n\n![png](drawdown/drawdown_3_0.png)\n\nSay now that we invested 100$ late 2016. Let's build the wealth index like we did in this [post](/post/from-portfolio-wealth-index-to-index-fund), and find the peaks of the wealth index. That is, the highest generated wealth prices before a deep.\n\n```\nwealth_index = 100*(1+apple_rets.AAPL[\"12-2016\":]).cumprod()\npeaks = wealth_index.cummax()\nax = wealth_index.plot(figsize=(14,7), label=\"W-Index\")\npeaks.plot(ax=ax, label=\"Peaks\")\nplt.legend(loc=\"upper left\")\nplt.show()\n```\n \n![png](drawdown/drawdown_5_0.png)\n\nDo you see these nice lagoons? Well, we wouldn't want then to be deep and long, cause that is when our investment looses value and we need to wait!\n\nSo, moving forward we want to find which lagoon was the deepest, how deep? and how long did it take to move back to the previous peak.\n\nFirst things first, we have to measure at any given point what is the difference between the peak and the wealth index. For example, the peak at a given point is 220\\\\$ and the index is 150\\\\$. That means that the index is 70\\\\$ below the peak. Since our target point is 220\\\\$ and we have lost 70\\\\$, we can say that we we are $-\\frac{70}{220}=31.8$% below the target.\n\n```\ndrawdown =  (wealth_index - peaks)/peaks\ndrawdown.plot(figsize=(14,7), title=\"Drawdown\")\n```\n\n![png](drawdown/drawdown_7_1.png)\n\nThe diagram above is what we call a **Drawdown** of an asset and it doesn't really have to do with any initial investment. Drawdown is a very nice indicator of risk since it is more realistic when compared to other risk indicators that involve standard deviations (Since returns deviate from normality as we proved in [Are Stock Returns Normally Distributed](/post/are-stock-returns-normally-distributed))\n\nWe are now ready to find the largest drawdown and the date that happened. \n\n```\ndrawdown.min(), drawdown.idxmin()\n```\n    (-0.38515910000506054, Timestamp('2019-01-03 00:00:00'))\n\nWe see that on the 3rd of January 2019 our investment was loosing 38.5% of its value!\n\nOne step further, we will try to find how long the lagoons lasted and find the longest one and an average of their durations.\n\n```\ndef compute_drawdown_lagoons_durations(drawdown):\n  # find all the locations where the drawdown == 0\n  zero_locations = np.unique(np.r_[(drawdown == 0).values.nonzero()[0], len(drawdown) - 1])\n  # also assign the dates so we know when things were not sinking\n  zero_locations_series = pd.Series(zero_locations, index=drawdown.index[zero_locations])\n  # do a shift to show what is the last and previous non zero dates\n  df = zero_locations_series.to_frame('zero_loc')\n  df['prev_zloc'] = zero_locations_series.shift()\n  # keep only the dates where the difference is more than 1\n  # that denotes the lagoons\n  df = df[df['zero_loc'] - df['prev_zloc'] > 1].astype(int)\n  df['duration'] = df['zero_loc'].map(drawdown.index.__getitem__) - df['prev_zloc'].map(drawdown.index.__getitem__)\n  df = df.reindex(drawdown.index)\n  df = df.dropna()\n  return df['duration']\n```\n\n```\ndf = compute_drawdown_lagoons_durations(drawdown)\ndf\n```\n    date\n    2016-12-06     4 days\n    2016-12-13     4 days\n    2016-12-27     6 days\n    2017-01-06    10 days\n    2017-01-17     6 days\n                   ...   \n    2020-08-26     2 days\n    2020-08-31     5 days\n    2020-12-28   118 days\n    2021-01-21    24 days\n    2021-02-16    21 days\n    Name: duration, Length: 69, dtype: timedelta64[ns]\n\nThe DataFrame above prints the last day of a drawdown, and for how long it lasted in days\n\n```\ndf.max(), df.mean()\n```\n    (Timedelta('372 days 00:00:00'), Timedelta('20 days 02:46:57.391304347'))\n\nThe longest drawdown lasted 372 days! and the average duration of a drawdown was 20 days :)\n\nStay tuned!\n"},{"slug":"from-portfolio-wealth-index-to-index-fund","frontmatter":{"title":"From Portfolio Wealth Index to Index Funds","description":"Ever wandered how an index fund is built? In this post we build a primitive one step by step.","date":"February 5, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/hacking-index-funds.png","colab":"https://colab.research.google.com/drive/1tPuqAsHgQdIYhSbaaoCS9LjEayDrTwjX?usp=sharing"},"excerpt":"","content":"\nImagine a scenario where you have invested 100$ in a stock the last 3 years and you earned 20% in the 1st year, -10% in the 2nd year, and 11% in the 3rd year. You would like to see how the investment progressed over the time until today.\n\nWe have already discussed about [geometric progression and the  compounding of returns](/post/geometric-progression-and-compounding-of-returns) in a previous article, and we will use that knowledge even further here.\n\nSo, at the end of the 3rd year, the investment would be:\n\n$$\n100 × 1.2 × .9 × 1.11 = 119.88\n$$\n\nIn the case of an initial investment of 1$, the result above would be called `Cumulative Wealth Index`!\n\nLet me show you how this index progresses over time.\n\nAt first we set the ground work for fetching historical prices.\n\n```\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    df.columns = [ticker]\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n```\n\nSay, we have invested 100$ in the MSFT stock on the 11th of October 2019. Earlier we used annualized returns, but for this example we will use the daily returns.\n\nBelow we will download the stock prices for the aforementioned period and calculate the daily returns.\n\n```\nmsft_stock_prices = retrieve_stock_data(\"MSFT\", \"2019-10-11\", \"2021-02-04\")\nmsft_rets = normal_rets(msft_stock_prices).dropna()\nmsft_rets.head()\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-14</th>\n      <td>-0.000931</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>0.014475</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>-0.008194</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>-0.005128</td>\n    </tr>\n    <tr>\n      <th>2019-10-18</th>\n      <td>-0.016322</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nThen, we will build the cummulative wealth index based on the initial investment, over time.\n\n```\n# See equation (1) in the post about geometric progression and the \n# compounding of returns\nwealth_index = 100 * (1 + msft_rets).cumprod() \nwealth_index.head()\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-14</th>\n      <td>99.906939</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>101.353104</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>100.522643</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>100.007156</td>\n    </tr>\n    <tr>\n      <th>2019-10-18</th>\n      <td>98.374868</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nAnd let's see how it looks like\n\n```\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\nmsft_stock_prices.plot(ax=ax1, figsize=(14,7))\nwealth_index.plot(ax=ax2)\nax1.get_legend().remove()\nax2.get_legend().remove()\nax1.title.set_text('MSFT Price Chart')\nax2.title.set_text('Cumulative Wealth Index')\nplt.show()\n```\n  \n![png](from-portfolio-wealth-index-to-index-fund/from-portfolio-wealth-index-to-index-fund_7_0.png)\n\nThere are a few things to notice in the graphs above:\n\n* The price development is the same :) and that makes sense, since the actual investment follows the price move of the stock.\n* The start point is different. Since we invested only 100\\$ and not ~140$ (the price of one stock at the moment).\n* The wealth index cares about the daily returns and not the actual price of the asset.\n\nThis last bullet allows us to extend the previous scenario by including more assets in our investment without taking into account the prices of the assets, but only the returns.\n\n## Portfolio Cumulative Wealth Index\n\nI will simplify how a primitive index fund (or Mutual Fund or an ETF) is built by extending the process from the previous section.\n\nSay now that, instead of investing 100$ to MSFT, we split the amount into 4 equal parts and we buy 4 different stocks. I will randomly pick Google's, Tesla's and Paypal's stocks.\n\n```\nfrom functools import reduce\ngoogle_stock_prices = retrieve_stock_data(\"GOOGL\", \"2019-10-11\", \"2021-02-04\")\ngoogle_rets = normal_rets(google_stock_prices).dropna()\n\ntsla_stock_prices = retrieve_stock_data(\"TSLA\", \"2019-10-11\", \"2021-02-04\")\ntsla_rets = normal_rets(tsla_stock_prices).dropna()\n\npaypal_stock_prices = retrieve_stock_data(\"PYPL\", \"2019-10-11\", \"2021-02-04\")\npaypal_rets = normal_rets(paypal_stock_prices).dropna()\n\n# bring them all together in a single dataframe\nassets_returns = reduce(lambda left,right: left.merge(right, left_index=True, right_index=True),\n            [msft_rets, google_rets, tsla_rets, paypal_rets])\nassets_returns.head()\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>GOOGL</th>\n      <th>TSLA</th>\n      <th>PYPL</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-14</th>\n      <td>-0.000931</td>\n      <td>0.001695</td>\n      <td>0.036589</td>\n      <td>0.001674</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>0.014475</td>\n      <td>0.020094</td>\n      <td>0.003619</td>\n      <td>0.018084</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>-0.008194</td>\n      <td>0.000612</td>\n      <td>0.007212</td>\n      <td>-0.004827</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>-0.005128</td>\n      <td>0.007884</td>\n      <td>0.008547</td>\n      <td>0.005238</td>\n    </tr>\n    <tr>\n      <th>2019-10-18</th>\n      <td>-0.016322</td>\n      <td>-0.006697</td>\n      <td>-0.019163</td>\n      <td>-0.023256</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nBased on the weight allocation of [.25, .25, .25, .25], let us now find the new cumulative wealth index of the investment.\n\n```\n# since the weights stay same throughout the index and since 100*0.25 = 25\nportfolio_wealth_index = 25 * (1 + assets_returns).cumprod()\nportfolio_wealth_index.head()\n```\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>GOOGL</th>\n      <th>TSLA</th>\n      <th>PYPL</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2019-10-14</th>\n      <td>24.976735</td>\n      <td>25.042363</td>\n      <td>25.914720</td>\n      <td>25.041838</td>\n    </tr>\n    <tr>\n      <th>2019-10-15</th>\n      <td>25.338276</td>\n      <td>25.545567</td>\n      <td>26.008512</td>\n      <td>25.494683</td>\n    </tr>\n    <tr>\n      <th>2019-10-16</th>\n      <td>25.130661</td>\n      <td>25.561196</td>\n      <td>26.196096</td>\n      <td>25.371627</td>\n    </tr>\n    <tr>\n      <th>2019-10-17</th>\n      <td>25.001789</td>\n      <td>25.762725</td>\n      <td>26.419986</td>\n      <td>25.504527</td>\n    </tr>\n    <tr>\n      <th>2019-10-18</th>\n      <td>24.593717</td>\n      <td>25.590192</td>\n      <td>25.913712</td>\n      <td>24.911400</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nNow, we just fave to sum the columns per row and plot the result\n\n```\nportfolio_wealth_index.sum(axis=1).plot.line(figsize=(14,7))\nplt.title('4 Asset Index')\nplt.show()\n```\n\n![png](from-portfolio-wealth-index-to-index-fund/from-portfolio-wealth-index-to-index-fund_13_0.png)\n\n## Index/Mutual/Exchange-Traded Funds\n\nIn the example above, I chose some random assets and equally weighted them in a portfolio! However, even simplistic, this is how a traded fund looks like.\n\nIn practice, a fund is a bucket of assets weighted in a structured way, and initialized with a price (like i did above with the 100$). Then, they are offered in the stock exchange for purchasing. In our example above, if it was a traded fund, the investors would deposit money to the fund and the fund would buy the underlying assets based on the specified weights. The buying and selling of the fund, doesn't affect the price of the fund directly (but indirectly through the underlying assets).\n\nThe difference among the different types of funds is due to the way weights are calculated. Mutual and ETFs have managers that pick these weights based on market research, and other characteristics. The managers, can change the weights by performing a rebalancing of the portfolio.\n\nIn index funds (also known as passive ETFs) the weights are usually based on market cap and maybe other attributes and require minimum intervention from a manager (and due to that are normally much cheaper than the other types of funds). In the simplest case, an index fund could follow all the assets of a specific industry and allocate the weights according to the market capitalization of each asset (divided by the total market cap of all the assets in the index) or just follow the same weighting strategy of a well known index, such as S&P 500.\n\nIn future posts, I will try to build (and invent) different types of funds.\n\nUntil next time!\n"},{"slug":"portfolio-expected-return-and-risk","frontmatter":{"title":"Return & Volatility of a Multi-Asset Portfolio","description":"Maths are magical :) And why diversification makes sense!","date":"January 31, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/magic.png","colab":"https://colab.research.google.com/drive/1lNirrCFUfWaZ_Cci-mmsio79wX_KwR5b?usp=sharing"},"excerpt":"","content":"\nIn previous posts we talked about the [expected return](/post/measures-of-location) (mean value of a distribution) and the [volatility](/post/measures-of-variability) (standard deviation) of an asset.\n\nBut, in investing, we rarely hold a portfolio of just one stock! Let's start then, by picking two stocks.\n\nThe first question is: OK, what percentage of the total investment amount shall we allocate to stock A and what to stock B?\n\nIf we allocate 100% to A and 0% to B or the other way around, then we get into the \"one asset\" portfolio and that is not desirable! Let's assume that we set 50% on asset A and 50% on asset B. Let's also call this percentage allocations, weights (w_A, w_B respectively).\n\nIn that case, we are asked to come up with the return (mean) and the volatility (standard deviation) of the portfolio.\n\nSomeone would blindly assume that the `Return = (w_A * R_A) + (w_B * R_B)` and `Volatility = (w_A * std_A) + (w_B * std_B)`. Well, maths keep always surprising us, and that is the case here!\n\nWhile indeed the return of the 2 asset portfolio is the average weighted returns,\n\n$$\nR_{A,B} = w_A*R_A + w_B*R_B  \\qquad (1)\n$$\n\nthe volatility is\n\n$$\n\\sigma_{A,B}=\\sqrt{\\sigma_A^2w_A^2 + \\sigma_B^2w_B^2 + 2w_Aw_B\\sigma_A\\sigma_B\\rho_{A,B}}  \\qquad (2)\n$$\n\nThis second (2) equation tells us that the standard deviation of a 2 asset distribution is equal to the square root of the variance of asset A multiplied by the squared weight of A plus the variance of asset B multiplied by the squared weight of B, plus twice the product of variance of A times the variance of B times the weight of A times the weight of B times the correlation coefficient of A and B!\n\nSo far so good! But where exactly does the magic begin? Well, the correlation coefficient is not always a positive number :O\n\nThe correlation coefficient can take values between -1 and 1. -1 when the two assets are totally uncorrelated, which means that when the first asset goes up the other goes down at the same pace and same angle. 1 when both assets move to the same direction with the same pace and same angle (either positive or negative direction). Values between -1 and 1 indicate a more loose correlation, but show the trend.\n\nBack to the equation (2). If we have a negative correlation of the assets, the total volatility is less than the average volatility, and if we have a positive correlation the total volatility is more that the average.\n\nIt becomes pretty obvious that by just combining two non correlated assets we can achieve volatility sometimes even smaller than the assets' individually. Who wouldn't want that!\n\n> Keep in mind that $\\rho_{A,B} = \\frac{cov_{A,B}}{\\sigma_A\\sigma_B}$ where $cov_{A,B}$ is the covariance of the two variables.\n\nLet's see an example...\n\n## Real example of a two asset portfolio\n\nAt first, let's set the ground work to be able to fetch some stock prices.\n\n```\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n```\n\nWe are now ready to fetch prices. I have picked Microsoft Corporation (MSFT) and Alpha Pro Tech, Ltd. (APT). Below we see how the price of the stocks unfolded throughout 2020! \n\n```\nmsft_stock_prices = retrieve_stock_data(\"MSFT\", \"2020-01-01\", \"2021-01-01\")\nmsft_rets = normal_rets(msft_stock_prices).dropna()\nmsft_rets.columns = ['returns']\n\napt_stock_prices = retrieve_stock_data(\"APT\", \"2020-01-01\", \"2021-01-01\")\napt_rets = normal_rets(apt_stock_prices).dropna()\napt_rets.columns = ['returns']\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\nmsft_stock_prices.plot(figsize=(14,7), ax=ax1)\napt_stock_prices.plot(figsize=(14,7), ax=ax2)\nax1.get_legend().remove()\nax2.get_legend().remove()\nax1.title.set_text('MSFT Price Chart')\nax2.title.set_text('APT Price Chart')\nplt.show()\n```\n\n![png](portfolio-expected-return-and-risk/portfolio-expected-return-and-risk_4_0.png)\n\nThe graphs show some king of un-correlation. When one stock goes up the other goes down and vice versa. Let's explore the average return, standard deviation and correlation of the stocks.\n\n```\nmsft_rets.mean().values[0], apt_rets.mean().values[0]\n```\n    (0.0017171460669071206, 0.009654517798428635)\n\n\n```\nmsft_rets.std().values[0], apt_rets.std().values[0]\n```\n    (0.027679154652983044, 0.10987021868530256)\n\n```\nreturns = msft_rets.merge(apt_rets, left_index=True, right_index=True)\nreturns.columns = ['MSFT', 'APT']\nreturns.corr()\n```\n\n<div>\n<table border=\"1\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSFT</th>\n      <th>APT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MSFT</th>\n      <td>1.0000</td>\n      <td>-0.2182</td>\n    </tr>\n    <tr>\n      <th>APT</th>\n      <td>-0.2182</td>\n      <td>1.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\nObviously, both stocks yield a positive average daily return (small but positive), and while MSFT has a volatility around ~2.8%, APT is at ~11%, which denotes a very volatile asset. As expected, the correlation of the two assets is negative.\n\nLet us now try to construct a portfolio of these two assets. From equations (1) and (2) we see that the weights are variable. We should also notice that the return is not a series of returns anymore but a single value. This value is the total return of an asset over the year. It is the so called [`Annualized Return`](/post/geometric-progression-and-compounding-of-returns).\n\n\n```\ndef annualize_rets(r, periods_per_year):\n    compounded_growth = (1+r).prod()\n    n_periods = r.shape[0]\n    return compounded_growth**(periods_per_year/n_periods)-1\n\nannualized_returns = annualize_rets(returns, 252)\nannualized_returns\n```\n\n    MSFT    0.399429\n    APT     2.222543\n    dtype: float64\n\n\nAs you can see the annual return for 2020 for MSFT was ~40%, while for APT was ~220%! It is pretty obvious from the price graphs :)\n\nNow, we move on and try to generate some portfolios where we assign different weights to the assets and try to calculate the return and the volatility of the portfolio. I will not get into what transposing a matrix means in algebra since it is not the focus of this post. Please check [this wikipedia article](https://en.wikipedia.org/wiki/Transpose) for more info.\n\n\n```\n# from equation (1)\ndef portfolio_return(weights, returns):\n    return weights.T @ returns\n\n# from equation (2)\ndef portfolio_vol(weights, covariance_matrix):\n    return (weights.T @ covariance_matrix @ weights)**0.5\n\n# first we construct 10 pairs of weights like [(0.1,0.9), (0.2,0.8) ...]\nweights = [np.array([w, 1-w]) for w in np.linspace(0, 1, 10)]\n\n# then we calculate the return of the portfolio for each pair of weights\nportfolio_returns = [portfolio_return(w, annualized_returns) for w in weights]\n\n# and the volatility of the portfolio for each pair of weights\nvols = [portfolio_vol(w, returns.cov()) for w in weights]\n\nef = pd.DataFrame({\n    \"Return\": portfolio_returns, \n    \"Volatility\": vols,\n    \"weights\": weights\n})\n\nax = ef.plot(x=\"Volatility\", y=\"Return\", style=\".-\", figsize=(11,6),\n             title=\"2 Asset Portfolio Risk/Return\", legend=False)\nplt.ylabel(\"Return\")\n\ndef label_point(x, y, val, ax):\n  a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n  for i, point in a.iterrows():\n    prettified_p = f\"({round(point['val'][0], 2)},{round(point['val'][1], 2)})\"\n    ax.text(point['x'], point['y'], prettified_p)\n\nlabel_point(ef.Volatility, ef.Return, ef.weights, ax)\n```\n  \n![png](portfolio-expected-return-and-risk/portfolio-expected-return-and-risk_12_0.png)\n\nWhat the graph above tells us is that by combining the two assets we are able to achieve a total volatility (risk) that is less than each asset's individual volatility!\n\nObserve the left most point on the graph!\n\nIn a next post we will calculate the optimal weights that minimize the risk of a portfolio as well as explore portfolios with more than 2 assets.\n\n## Some More Notes\n\n- The approach I followed above is not something new. Is called [Markowitz Model](https://en.wikipedia.org/wiki/Markowitz_model) and won a [Nobel Price](https://www.nobelprize.org/prizes/economic-sciences/1990/press-release/) in 1990.\n- I tried to oversimplify the example, just to show the basics.\n* I randomly picked the two assets, in the example, from [IMPACTOPIA](http://www.market-topology.com/correlation/MSFT?etf=0).\n* We will prove, later on, that volatility changes over time :) and that would normally lead to rebalances.\n* As always, `Historical returns are no guarantee of future returns.`\n\nStay tuned ...\n"},{"slug":"global-market-insights-after-the-pandemic","frontmatter":{"title":"Global Market Insights after the Pandemic.","description":"What are the projections after the Pandemic? Let's explore ...","date":"January 21, 2021","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/after-covid-19.jpeg","colab":"https://colab.research.google.com/drive/1nPyDupJHidnP5DLfAEKX3eYiY-MHCDmM?usp=sharing"},"excerpt":"","content":"\n2020 was a difficult year and most of the market segments diverged from the projections! With the new facts at hand, and the emerging needs after the pandemic, we should expect changes in the market.\n\nBelow is an attempt to get a feeling of the emerging sectors by analyzing some [Global Market Insights](https://www.gminsights.com/) reports.\n\nAnd as always, let's automate ...\n\n```\n!pip install requests beautifulsoup4\nimport requests\nimport re\nfrom bs4 import BeautifulSoup\nimport string\n\nclass GlobalMarketInsights:\n  __DEFAULT_BASE_URL = 'https://www.gminsights.com/industry-reports'\n  \n  @staticmethod\n  def _escape(input):\n    printable = string.ascii_letters + string.digits + string.punctuation + ' '\n    return ''.join(c if c in printable else ' ' for c in input )\n\n  def _description_matcher(self, descr):\n    descr = GlobalMarketInsights._escape(descr.replace('\\t', ' ').replace('\\n', ' ').replace('\\r', ' '))\n\n    start_date = end_date = percentage = market_name = None\n\n    r1 = re.search('^(.*) (?:Market|Aftermarket) .*$', descr, re.IGNORECASE)\n    if r1:\n      market_name = r1.group(1).strip()\n\n    r2 = re.search('.* between (\\d+) (?:and|to) (\\d+)', descr)\n    if r2:\n      start_date = r2.group(1).strip()\n      end_date = r2.group(2).strip()\n    \n    r3 = re.search('.* (?:from|of) (\\d+) to (\\d+)', descr)\n    if r3:\n      start_date = r3.group(1).strip()\n      end_date = r3.group(2).strip()\n    \n    r4 = re.search('([-+]?\\d*\\.\\d+|\\d+)%', descr)\n    if r4:\n      percentage = r4.group(1)\n    \n    if None in (market_name, percentage, start_date, end_date):\n      raise Exception(f\"Couldn't parse: {descr}\")\n    else:\n      return {\n        \"market\": market_name,\n        \"percentage\" : float(percentage),\n        \"start\": int(start_date),\n        \"end\": int(end_date)\n      }\n\n  def get(self, page=1):\n    page = requests.get(f\"{GlobalMarketInsights.__DEFAULT_BASE_URL}?page={page}\")\n    soup = BeautifulSoup(page.text, 'html.parser')\n    single_rds = soup.find_all('div', class_='single_rd')\n    reports = []\n    for single_rd in single_rds:\n      single_rd_children = single_rd.findChildren()\n      for single_rd_child in single_rd_children:\n        if single_rd_child.has_attr('class') and single_rd_child['class'][0] == 'rd_desc':\n          description = single_rd_child.getText()\n          try:\n            reports.append(self._description_matcher(description))\n          except Exception as e:\n            # print(e)\n            pass\n          break\n    return reports\n  \n  def fetch_all_reports(self):\n    # get the total number of pages and start iterating\n    page = requests.get(f\"{GlobalMarketInsights.__DEFAULT_BASE_URL}?page=1\")\n    lun_q = 'Displaying \\d+ records out of (\\d+) on Page \\d+ of (\\d+)'\n    r = re.search(lun_q, page.text)\n    if r:\n        number_of_records = r.group(1)\n        number_of_pages = r.group(2)\n    else:\n      raise Exception('No pages or data!')\n    \n    all_reports = []\n    for page in range(1, int(number_of_pages) + 1, 1):\n      page_reports = self.get(page=page)\n      all_reports += page_reports\n\n    return int(number_of_records), all_reports\n```\n\nScraping web pages is always challenging. In this case especially, the task was a bit tedious since the different report descriptions where not following a unique pattern.\n\n```\nglobal_market_insights = GlobalMarketInsights()\nnumber_of_records, all_reports = global_market_insights.fetch_all_reports()\nprint(f\"Parsed {len(all_reports)} out of {number_of_records} report descriptions!\")\n```\n\n    Parsed 1200 out of 1964 report descriptions!\n\nNext, we add the reports to a dataframe for better presentation and easier data manipulation.\n\n```\nimport pandas as pd\n\ngmi_reports_df = pd.DataFrame(all_reports) \ngmi_reports_df.head()\n```\n\n<div>\n<table border=\"1\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>market</th>\n      <th>percentage</th>\n      <th>start</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Food phosphate</td>\n      <td>6.0</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Supply Chain Analytics</td>\n      <td>16.0</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Cooking Coconut Milk</td>\n      <td>8.5</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Steel Rebar</td>\n      <td>4.0</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2,5-Dimethyl-2,4-Hexadiene</td>\n      <td>2.5</td>\n      <td>2021</td>\n      <td>2027</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nSo far, so good! Let's try to sort by percentage and see which sector is projected to perform more than 30% the following years.\n\n```\nsector_projection_ascending = gmi_reports_df.sort_values('percentage', ascending=False)\nsector_projection_ascending.loc[(sector_projection_ascending['percentage']>30) & (sector_projection_ascending['start']>=2020)]\n```\n\n<div>\n<table border=\"1\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>market</th>\n      <th>percentage</th>\n      <th>start</th>\n      <th>end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>435</th>\n      <td>SD-WAN</td>\n      <td>60.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>Cannabidiol (CBD)</td>\n      <td>52.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>(Light Fidelity) Li-Fi</td>\n      <td>50.0</td>\n      <td>2020</td>\n      <td>2030</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Healthcare Artificial Intelligence</td>\n      <td>43.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>Automotive Subscription Services</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>964</th>\n      <td>AI in Manufacturing</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2025</td>\n    </tr>\n    <tr>\n      <th>363</th>\n      <td>Artificial Intelligence (AI) in BFSI</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Robotic Process Automation</td>\n      <td>40.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>Fuel Cell Electric Vehicle</td>\n      <td>38.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>637</th>\n      <td>AI in Automotive</td>\n      <td>35.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>Artificial Intelligence Chipsets</td>\n      <td>35.0</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>Total Knee Replacement</td>\n      <td>34.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>212</th>\n      <td>Vaginal Rejuvenation</td>\n      <td>33.7</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>Carbon Wheels</td>\n      <td>32.3</td>\n      <td>2020</td>\n      <td>2026</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nIt is becoming pretty obvious that everything around Artificial Intelligence yields the best projections, and is an attractive area for investments :)\n"},{"slug":"are-stock-returns-normally-distributed","frontmatter":{"title":"Are Stock Returns Normally Distributed?","description":"What do you think?","date":"December 20, 2020","topic":{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/normal-dist-with-hist.png","colab":"https://colab.research.google.com/drive/1iYrNJ9ISktohy1dG2s16_FZKakB8FLU5?usp=sharing"},"excerpt":"","content":"\nIn a previous post we talked about the [Higher Moments of a Distribution](/post/higher-moments-of-a-distribution). We saw that skewness and kurtosis are two attributes that can identify if a distribution is normal or not (skewnes = 0 & kurtosis = 3).\n\nLet's try this approach on the MSFT's stock.\n\n\n```\n%pip install yahoofinancials\nfrom yahoofinancials import YahooFinancials\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dateutil.parser\nimport numpy as np\n\nmatplotlib.rcParams['figure.figsize'] = (10.0, 5.0)\nmatplotlib.style.use('ggplot')\n\ndef retrieve_stock_data(ticker, start, end):\n    json = YahooFinancials(ticker).get_historical_price_data(start, end, \"daily\")\n    columns=[\"adjclose\"]  # [\"open\",\"close\",\"adjclose\"]\n    df = pd.DataFrame(columns=columns)\n    for row in json[ticker][\"prices\"]:\n        d = dateutil.parser.isoparse(row[\"formatted_date\"])\n        df.loc[d] = [row[\"adjclose\"]] # [row[\"open\"], row[\"close\"], row[\"adjclose\"]]\n    df.index.name = \"date\"\n    return df\n\ndef normal_rets(S):\n    return S.pct_change().dropna()\n\ndef log_rets(S):\n    rets = np.log(S) - np.log( S.shift(1))\n    return rets[1:]\n\nstock_prices = retrieve_stock_data(\"MSFT\", \"2000-01-01\", \"2020-01-01\")\n\nrets = normal_rets(stock_prices).dropna()\nrets.columns = ['returns']\nrets.plot(figsize=(14,7))\nplt.title(\"Daily returns\", weight=\"bold\");\n```\n  \n![png](are-stock-returns-normally-distributed/are-stock-returns-normally-distributed-1-1.png)\n\nLet's print skewness and kurtosis:\n\n```\nfrom scipy.stats import kurtosis, skew\nskew(rets, bias=False)[0], kurtosis(rets, bias=False, fisher=False)[0]\n```\n\n    (0.20887713542026032, 13.229622042763442)\n\nIt is obvious that the MSFT stock returns for that period follow a leptokurtic distribution and are far from normal. Same, of course, happens if we get the log returns instead.\n\n```\nlog_msft_rets = log_rets(stock_prices).dropna()\nskew(log_msft_rets, bias=False)[0], kurtosis(log_msft_rets, bias=False, fisher=False)[0]\n```\n\n    (-0.12981025399984283, 12.81366131030108)\n\n## Normality Tests\n\nThere are several interrelated approaches to determining normality.\n\n* Histogram with the normal curve superimposed. Unfortunately, there is no automated way to represent the \"fitness\" as a value. This approach is empirical mostly and requires experience.\n* Skewness & Kurtosis Tests.\n* Normality plots. “Normal Q-Q Plot” provides a graphical way to determine the level of normality.\n* Normality tests. The Kolmogorov-Smirnov test (K-S) and Shapiro-Wilk (S-W) test are designed to test normality by comparing your data to a normal distribution with the same mean and standard deviation of your sample. If the test is NOT significant, then the data are normal, so any value above .05 indicates normality. If the test is significant (less than .05), then the data are non-normal.\n\n### Histogram & Normal PDF\n\n```\nfrom scipy.stats import norm\nx = np.linspace(min(rets.returns.values), max(rets.returns.values))\nax = rets.plot(kind='hist', bins=500, density=True)\npdf_fitted = norm.pdf(x, *norm.fit(rets.returns.values))\npd.Series(pdf_fitted, x).plot(ax=ax)\nplt.show()\n```\n    \n![png](are-stock-returns-normally-distributed/are-stock-returns-normally-distributed-8-0.png)\n\n### Skewness & Kurtosis Tests\n\n```\nfrom scipy import stats\nstats.kurtosistest(rets.returns)\n```\n\n    KurtosistestResult(statistic=29.93227785492693, pvalue=7.484189304773088e-197)\n\n```\nstats.skewtest(rets.returns)\n```\n\n    SkewtestResult(statistic=5.99114785753993, pvalue=2.083650895527666e-09)\n\n### QQ-Plot\n\n```\nfrom numpy.random import seed\nfrom statsmodels.graphics.gofplots import qqplot\nfrom matplotlib import pyplot\nseed(1)\nqqplot(rets.returns, line='s')\npyplot.show()\n```\n    \n![png](are-stock-returns-normally-distributed/are-stock-returns-normally-distributed-13-0.png)\n    \n\nThe Quantile-Quantile plot, as the name suggests, will compare the quantiles between the normal distribution and our data. We notice here that, the tails of the distribution of our data are diverging a lot from the normal distribution. This is what we would expect. Fat tails!\n\n### Statistical Normality Tests\n\nThe tests assume that that the sample was drawn from a Gaussian distribution. Technically this is called the null hypothesis, or H0. A threshold level is chosen called alpha, typically 5% (or 0.05), that is used to interpret the p-value.\n\nIn the SciPy implementation of these tests, you can interpret the p value as follows.\n\n* p <= alpha: reject H0, not normal.\n* p > alpha: fail to reject H0, normal.\n\nThis means that, in general, we are seeking results with a larger p-value to confirm that our sample was likely drawn from a Gaussian distribution.\n\nA result above 5% does not mean that the null hypothesis is true. It means that it is very likely true given available evidence. The p-value is not the probability of the data fitting a Gaussian distribution; it can be thought of as a value that helps us interpret the statistical test.\n\n#### Kolmogorov-Smirnov test (K-S)\n\n\n```\nkstest = stats.kstest(rets.returns, 'norm')\nkstest.pvalue > 0.05\n```\n\n    False\n\n#### Shapiro-Wilk Test\n\n```\nshapiro_stat, shapiro_p = stats.shapiro(rets.returns)\nshapiro_p > 0.05\n```\n\n    False\n\n\n#### D’Agostino’s K^2 Test\n\nThe D’Agostino’s K^2 test calculates summary statistics from the data, namely kurtosis and skewness, to determine if the data distribution departs from the normal distribution, named for Ralph D’Agostino.\n\n* Skew is a quantification of how much a distribution is pushed left or right, a measure of asymmetry in the distribution.\n* Kurtosis quantifies how much of the distribution is in the tail.\n\nIt is a simple and commonly used statistical test for normality.\n\n```\nseed(1)\ndagostino_stat, dagostino_p = stats.normaltest(rets.returns)\ndagostino_p > 0.05\n```\n\n    False\n\n#### Jarque-Bera Test for Normality\n\n```\njarque_bera_stat, jarque_bera_p = stats.jarque_bera(rets.returns)\njarque_bera_p > 0.05\n```\n\n    False\n\n## Conclusion\n\nIn this post we went through some techniques that allow us identify if stock returns are normally distributed. We saw, with an example, that returns (arithmetic, or log) are not normally distributed but instead exhibit fat tails. I cannot generalize of course by only one example that I provide here, but I will leave that as a small exercise to the curious readers.\n\nThe question that is now left is? Since the (I could event say here, financial asset returns (everything that is publicly traded)) returns are not following a normal distribution, that what type of distribution they follow?\n\nThe answer to that ... in a later article! :)\n"},{"slug":"higher-moments-of-a-distribution","frontmatter":{"title":"Higher Moments of a Distribution","description":"See how higher moments can reveal more characteristics of a data series.","date":"December 17, 2020","topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/skewness-kurtosis.png","colab":"https://colab.research.google.com/drive/1k2Ek0o9UpV2f7NcS0R6aZ36UDvR24BRP?usp=sharing"},"excerpt":"","content":"\nWe have already discussed about the [mean](/post/measures-of-location) and the [variance](/post/measures-of-variability) of a series of data. \n\nMean is also called the 1st moment and variance the 2nd moment. The type to get a moment (the movement) about a non-random value c, of a density function is:\n\n$$\nE[(X-c)^κ] = \\int_{-\\infty}^{+\\infty} (x-c)^k f(x) dx  \\qquad (1)\n$$\n\nBefore explaining the moments, we should first understand what a density function is. Commonly called probability density function (PDF).\n\n## Density Function\n\n30 people are gathered in a house party. Let us measure their weights:\n\n\n```\nimport pandas as pd\nm_v = [56.8, 81.3, 47.9, 32.5, 24.1, 25.3, 14.3, 29.4, 71.3, 86.0, 54.2, 15.2,\n       54.7, 25.1, 49.5, 1.9, 70.0, 69.6, 75.4, 38.9, 49.2, 22.5, 68.6, 60.1,\n       52.7, 109.7, 38.9, 45.9, 47.7, 52.9]\nvalues = pd.Series(m_v)\nvalues.describe()\n```\n\n    count     30.000000\n    mean      49.053333\n    std       24.001634\n    min        1.900000\n    25%       30.175000\n    50%       49.350000\n    75%       66.475000\n    max      109.700000\n    dtype: float64\n\n\n\nWhat if we change the way we present the data, and instead of having them in a simple series, we try to split them up into buckets.\n\nWe will get the, so called, histogram of the values. It shows how the probabilities of measurement are distributed.\n\n\n```\nimport matplotlib.pyplot as plt\nhistogram = values.plot.hist(bins=10, figsize=(10,5))\nplt.show()\n```\n  \n![png](higher-moments-of-a-distribution/higher-moments-of-a-distribution_3_0.png)\n    \n\n\nIf we ask the question: What is the probability, the next person that joins the party, weights between 80 and 90 kilos?\n\nTo answer this question, we need to imagine as if the upper boundaries of the blue colored space above, are a continuous line, a curve. This curve is what we call PDF or density function.\n\n\n```\nfrom scipy.stats import norm\nimport numpy as np\nx = np.linspace(min(values), max(values))\nax = values.plot(kind='hist', bins=10, figsize=(10,5), density=True)\npdf_fitted = norm.pdf(x, *norm.fit(values))\npd.Series(pdf_fitted, x).plot(ax=ax)\nplt.show()\n```\n    \n![png](higher-moments-of-a-distribution/higher-moments-of-a-distribution_5_0.png)\n\nWe observe that the curve is not a perfect fit. It is an approximation and there are hundreds of different curves we can plot and several of them will be very close to fitting the data (I will show that in another post).\n\nIn this case above, I have intentionally picked the data as such to resemble the, so called, `normal` distribution.\n\nBack to our question now! The probability, the next person that joins the party, weights between 80 and 90 kilos, can be estimated by measuring the area below the curve for that bucket. So, if the whole area below the curve is 1 (zero moment, see below), the part that belongs to bucket 80-90 is a percentage :) and that is the probability we are after. The expression is:\n\n$$\nP( \\text{weight between 80 and 90 kilos} | \\text{mean=x and standard-deviation=y} )\n$$\n\nWhich is translated to: The probability a person weights between 80 and 90 kilos given an average of x and standard-deviation of y.\n\nAnd the area below the curve is the integral between the points:\n\n$$\nP = \\int_{x=80}^{x=90} f(x)dx \\text{  where f the density function}\n$$ \n\nThere are many pros in trying to use distributions to represent how the values in a dataset are distributed:\n\n* makes it easy to measure the areas below (with integrals, since the function of the curve is known)\n* the presentation is much better for the human\n* well known distributions have really nice properties\n\n## Zero Moment (Total Mass)\n\nThat means that in (1), k=0 and as such $(...)^0 = 1$. That leaves us with:\n\n$$\n\\int_{x=-\\infty}^{x=\\infty} f(x)dx = 1 \\qquad (2)\n$$\n\n> Probability Distributions are normalized quantities, that always sum to one. Think of that as the probability that at least one of the events in a sample space will occur. Isn't that 100%?\n\n## 1st Moment - Mean\n\n$$\nμ_1 = E[(X-0)^1] = E[X] = \\int_{-\\infty}^{+\\infty} xf(x)dx  \\qquad (3)\n$$\n\nc=0 in this case since we do not have an origin to get the moment (movement) about.\n\nFrom (3) is obvious that we talk about the mean and that alternatively talk about the balance of the total mass (the area below the curve) around a point :)\n\n## 2nd Moment - Variance\n\nFrom (1), we can take c=0 and k=2! But what will that show us? How the mass is balanced around again the same point, which in practice is the average again but squared? Doesn't provide much value in understanding our data.\n\nFor that reason we get $c=μ$ and that will start making sense, since we se how the mass is diverging from the mean. It will show the variance of the data around the mean :)\n\n$$\nVar = \\int_{-\\infty}^{+\\infty} (x-μ_x)^2f(x)dx  \\qquad (4)\n$$\n\n## 3rd Moment - Skewness\n\nFollowing the pattern above and using k=3 around the mean $c=μ$ then we get the skewness which measures the relative size of the two tails of a distribution.\n\n\n```\nfrom scipy.stats import skew\nskew(values, bias=False) # bias=False calculates the skewness and kurtosis of the sample as opposed to the population.\n```\n    0.274192939649461\n\n\nA left-skewed (negatively-skewed) distribution has a long left tail. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n\nA right-skewed (positive-skew) distribution has a long right tail. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n\n![png](higher-moments-of-a-distribution/Relationship_between_mean_and_median_under_different_skewness.png)\n\n## 4th Moment - Kurtosis\n\nThe fourth central moment is a measure of the heaviness of the tail of the distribution.\n\n```\nfrom scipy.stats import kurtosis\nkurtosis(values, bias=False)\n```\n    0.14330737818315065\n\n![jpg](higher-moments-of-a-distribution/kurtosis-types.jpg)\n\n## Higher Moments of the Normal Distribution\n\n```\ndata = np.random.normal(0, 1, 10000000)\nplt.hist(data, bins='auto')\n\nprint(\"mean : \", np.mean(data))\nprint(\"var  : \", np.var(data))\nprint(\"skew : \", skew(data, bias=False))\nprint(\"kurt : \", kurtosis(data, bias=False, fisher=False))\n```\n\n    mean :  -0.00015674618345404924\n    var  :  1.0002202373014222\n    skew :  0.0007495220785926886\n    kurt :  3.0013415645199695\n \n![png](higher-moments-of-a-distribution/higher-moments-of-a-distribution_12_1.png)\n    \n\nFor a normal distribution the skeweness is zero and the kurtosis is 3. These properties are specific to the normal distribution and are used for normality testing of distributions. We will go deeper in that in a later post.\n"},{"slug":"load-google-drive-folder-in-google-colab","frontmatter":{"title":"Load Google Drive folder in Google Colab","description":"How to mount a Google Drive in Google Colab and load some stock data.","date":"December 7, 2020","topic":{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/google-colab.png"},"excerpt":"","content":"\nThere are several cases where:\n\n* fetching stock prices is not possible through python libraries like `yahoofinancials` or other APIs\n* you want to load the same data over and over again (for parallelization)\n* you want to use you python modules without publishing them to a registry\n\nIn these cases I find it very handy to store my data (csv format) and my modules in google drive.\n\nHowever, loading the data to Google Colab turned into pain since I had to manually upload the files each time I wanted to run a notebook.\n\nTo avoid this situation I mount google drive and:\n\n1. add the folder with my python modules to the path\n2. copy the data to the Colab data folder\n\n```python\nimport warnings\nwarnings.simplefilter('ignore')\n\n%config InlineBackend.figure_formats=[\"png\"]\n\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nimport os\nimport sys\nimport glob\nfrom shutil import copytree, copy\n\n# This will add my python modules in the path\ngdrive_base_path = '/content/drive/My Drive/my-python-modules'\nsys.path.append(gdrive_base_path)\n\ntry:\n  # copy the data we need\n  copytree('/content/drive/My Drive/Colab Notebooks/data', '/content/data')\nexcept Exception as e:\n  pass\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as st\n```\n\nSince the folder that contain my python modules is now in the system path, I'm able to\n\n```python\nimport my_module\n```\n\nand whenever I change my modules (add more functionality, improvements), it practically saves it to Google Drive, since it is a pure mount.\n\nWhen it comes to loading data, then I simply:\n\n```python\nMSFT = pd.read_csv('data/msft_daily.csv', parse_dates=True, index_col=0, header=0)\n```\n\n> **Note**: Since `copytree` is used, uploading new data files to Google Colab, will not automatically save it to Google Drive!\n"},{"slug":"geometric-progression-and-compounding-of-returns","frontmatter":{"title":"Geometric Progression and the Compounding of the Returns","description":"Your savings account offers a 1% annual interest! The account balance in 10 years? +10%? Naaaah","date":"December 6, 2020","topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/fractal-star.png","tags":[]},"excerpt":"","content":"\n## Compounding of Returns\n\nLet us consider the following scenario. A stock asset yields an average monthly return R of 2% for the last 12 months. If we had invested 100$ on this asset a year ago, what would be the outcome of this investment today?\n\nConsider the initial asset price $P_0 = 100$. After the first month of the investment the outcome would be:\n\n$$\nP_1 = 100 + (100 * 0.02) = (100 * 1) + (100 * 0.02) = 100 * (1 + 0.02) = P_0 * 1.02\n$$\n\nThe outcome after the 2nd month would be:\n\n$$\nP_2 = P_1 + (P_1 * 0.02) = P_1 * 1.02\n$$\n\n3rd month:\n\n$$\nP_3 = P_2 + (P_2 * 0.02) = P_2 * 1.02\n$$\n\n(ν-1)th month:\n\n$$\nP_{ν-1} = P_{ν-2} + (P_{ν-2} * 0.02) = P_{ν-2} * 1.02\n$$\n\nνth month:\n\n$$\nP_ν = P_{ν-1} + (P_{ν-1} * 0.02) = P_{ν-1} * 1.02\n$$\n\nIt becomes obvious from the above that generating each element in the sequence $P_0, P_1, ..., P_{ν-1}, P_ν$ follows a pattern where the present number is generated by multiplying the previous number with a constant.\n\nIn our case, the constant is the average monthly return + 1. If we combine the previous equations in an attempt to formulate a final equation that provides the νth based on the $P_0$ element then:\n\n$$\nP_ν = P_{ν-1} * 1.02 = (P_{ν-2} * 1.02) * 1.02 = P_{ν-2} * 1.02^2 = .... = P_0 * (1 + 0.02)^ν\n$$\n\nSo, after 12 months we will have $100 * 1.02^{12} = 126.82$!\n\nThe generic equation is:\n\n$$\nP_ν = P_0 * (1 + R)^ν \\qquad (1)\n$$\n\n## Geometric Progression\n\nIn calculus, a sequence of numbers where each term after the first is found by multiplying the previous one by a fixed, non-one number, called `geometric progression`. The fixed number is called, the `common ratio`.\n\n$$\na, ar, ar^2, ... , ar^{n-1}\n$$\n\n$1+R$ is the `common ratio` in our case. \n\nSome interesting properties to notice:\n\n* a common ratio greater than 1, will produce exponential growth towards positive infinity!\n* a common ratio greater than 0 and up to 1, will produce exponential decay towards zero!\n\nIn the example above the way we used the return to calculate the end result is called **Compounding of Returns**.\n\n## From annual to periodic returns\n\nLet's see the example above from another angle. Let's say that an assets had a total return over the last year of 10%. What was the average monthly return of the asset?\n\nWe calculate a simple/arithmetic return (pandas.pct_change) like:\n\n$$\nR_i = \\frac{P_i-P_{i-1}}{P_{i-1}} = \\frac{P_i}{P_{i-1}} - 1 \\qquad (2)\n$$\n\nwhere $P_i$ is the price of the asset on the period $i$.\n\nBy assuming (for simplicity) that $P_0 = 1$ and from (2), $P_1 = P_0 * (1 + R_{annual})$\n\nThen from (1),\n\n$$\n1 + R_{annual} = 1 * (1 + R_{month})^12 => R_{month} = \\sqrt[12]{1+R_{annual}} - 1 \\qquad (3)\n$$\n\nWe use ν = 12, because 1 year = 12 months (=> 12 compounding periods). If instead of monthly returns we were asked to find quarter returns then we would use ν = 4 ($R_{quarter} = \\sqrt[4]{1+R_{annual}} - 1$).\n\nFinally, from (3),\n\n$$\nR_{monthly} = \\sqrt[12]{1 + 0.1} - 1 = \\sqrt[12]{1.1} - 1 = 1.0079 - 1 = 0.0079\n$$\n\n## Compounding variable returns\n\nIt is commonly accepted that returns do not stay the same over periods. For example, the average return of this month is not the same as the one from last month! However, the same principle of compounding applies in this case too. Let's see an example.\n\nAn asset yields the following return for the past couple of months $0.021, 0.032, -0.018, 0.06, -0.043, 0.048$. The total return is the **product** of the individual returns when 1 is added to them:\n\n$$\nR = (1 + 0.021) * (1 + 0.032) * (1 - 0.018) * (1 + 0.06) * (1 - 0.043) * (1 + 0.048) = ...\n$$\n\nThis, resonates with the equation (1) above where for a fixed return we have $R_{total} = (1 + R_{fixed})^{number-of-periods}$"},{"slug":"measures-of-variability","frontmatter":{"title":"Measures of Variability","description":"Standard deviation & percentiles! Used for measuring volatility, and allow for a rough \"estimate\" of the near future.","date":"November 1, 2020","topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"tags":[{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"/static/standard-deviation.png"},"excerpt":"","content":"\n[Measures of location](/post/measures-of-location) cannot identify how well average, mode, etc. represent the data. They fail to answer questions like:\n\n* How close are the values to each other?\n* What is the largest difference between the values?\n* On average, how far are the values from each other?\n\nMeasures of variability (also called spread) fill this gap. The most common are:\n\n* Standard Deviation (SD)\n* Range, which is the largest value minus the smallest value\n* Interquartile Range (IQR) which is the upper quartile (75th percentile), minus the lower quartile (25th percentile)\n\nIn investing, Standard Deviation is one of the most common methods for determining the risk of an investment (also called `volatility`).\n\n## Variance ($Var$ or $\\sigma^2$) & Standard Deviation ($\\sigma$)\n\nVariance is the average squared difference from the mean, or alternatively, how spread out the data are around their mean.  \n\n$$\nVar(x) = \\sigma^2 = \\sigma_x^2 = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}{n-1}\n$$\n\n> **Note**: We can divide by n or n-1. n is the total population of an event. n-1 a series of it. numpy is, by default, using n-1! The difference is negligible for long sequences, but not for short ones!\n\nWe see above that, for some reason, we squared the difference ($(x_i - \\bar{x})^2$), instead of just taking the absolute difference ($x_i - \\bar{x}$). We did that in order to avoid the [same problem](/post/measures-of-location) we have with the mean, where the sum can be zero because of negative values in the list. Imagine, for example, having the sequence `-10,1,2,3,4`. What is the mean value? 0! And, how spread are the data around the mean? 0 again. It doesn't really make much sense, right? By observing the sequence, we see that there is some spreading of the data around \"something\". This, \"something\" doesn't feel to be 0. It feels like something close to the `1,2,3,4` values, which look like a group of values that are really close to each other. I know that is kinda difficult to grasp. But, not until we raise it to another \"dimension\" :). And that is what we do when we square the values.\n\n* we remove the negative sign\n* we make the values that are far from \"the group\", to stand out (the outliers are emphasized).\n* we change the unit of measurement (it is now something much bigger). Like another dimension!\n\nTo return back to the unit we started with, but meanwhile keep the notion of spreading, we use the RMS ([Root Mean Square](/post/measures-of-location)) of all the distances to the mean. We call that Standard Deviation (SD): \n\n$$\n\\sigma_x = \\sqrt{Var(x)}\n$$\n\n\n### Standard Deviation in Investing\n\nStandard Deviation measures the volatility of an asset. Large values mean that the asset's price can diverge a lot from the mean value. Thus, an indication of higher risk. On the flip side, low volatility may indicate low risk. \n\n> A critical point to remember is that standard Deviation is a backward looking tool and not a guarantee of future moves.\n\nWhile daily returns (usually log returns) of an asset are more like leptokurtotic distributions (i.e. exhibit fat tails), we can add some abstraction by saying that are normally distributed (not to measure actual risk, but to get an initial approximation of risk). \n\nNormal distributions have some very unique attributes. The most important are:\n\n* The center of a normal distribution is located at its peak, and 50% of the data lies above the mean, while 50% lies below. The mean, median, and mode are all equal\n* Approximately 68% of the data lies within 1 SD of the mean, Approximately 95% of the data lies within 2 SD of the mean, Approximately 99.7% of the data lies within 3 SD of the mean. This is known as `The empirical rule`.\n\n![png](measures-of-variability-normal-distribution.png)\n\nLet's assume that the average price of an asset $40 (measured with daily returns for some period) and the standard deviation is $5. We can assume with 95% certainty the next closing price remains between $30 and $50! (More on that in a later post). In other words, we are 95% sure that the max loss will not exceed $10.\n\n> Standard Deviation is considered a pure measure of risk (due to returns distribution deviate from the normal one), but is a good indicator of how volatile as asset is. \n\n## Percentiles\n\nWe find the percentile by ordering all the values in a dataset from smallest to largest. Then we multiply the number of values by the percentile we want. This, will give us the index of this percentile in the dataset. The value at this index is the requested percentile.\n\nFor example, suppose you have 12 daily stock return, ordered from lowest to highest: `2.3, 2.7, 3.1, 3.5, 3.9, 4.6, 4.7, 5, 5.1, 5.2, 5.4, 9`. To find the 90th percentile for these (ordered) returns, start by multiplying 90% times the size of the dataset, which gives 90% * 12 = 0.90 * 12 = 10.8 (the index). Rounding up to the nearest whole number, you get 11.\n\nCounting from left to right (from the smallest to the largest value in the dataset), you go until you find the 11th value in the dataset. That value is 5.4, and it’s the 90th percentile for this dataset.\n\nSomeone can think of the percentile as a lower dimension of the quantile. And the quartile a lower dimension of the percentile. This, because a quantile (which spans to any type of quantity) can take any upper value (like ex. 1500). The percentile (cent=100) is bound between 0-100. And the quartile (quarter) bound between 1-4. \n\nThus:\n\n* The 25th percentile is also called the first quartile.\n* The 50th percentile is generally the median.\n* The 75th percentile is also called the third quartile.\n* The difference between the third and first quartiles is the interquartile range.\n\n### Percentiles in Investing\n\nA simple example of how percentiles can be used in Investing was introduced in the previous section, where we asked, how sure we were that a loss will not exceed a value. Questions like that can be asked in numerous ways, and in the majority of the cases involve some sort of percentile representation."},{"slug":"measures-of-location","frontmatter":{"title":"Measures of Location","description":"Use average, median, mode and more, to understand basic investing.","date":"October 18, 2020","tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg"},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg"}],"topic":{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},"author":{"id":"chris","name":"Chris","image":"https://s.gravatar.com/avatar/db809ecfa64d56da4bd9704c8393005a?s=80","description":"Software Engineer, passionate about ..."},"img":"https://www.picpedia.org/highway-signs/images/average.jpg"},"excerpt":"","content":"\nIn investing, we are often presented with the challenge to analyze a data set (with historical prices of stocks, funds, etc.). \n\nWe are asked to answer questions like:\n\n* What is the average return of a stock the last x days (given the daily returns)?\n* What is the return of a stock that occurred the most the last x days?\n* etc. \n\nTo answer, we commonly start by exploring some attributes that best describe the data. We often call them [`Measures of Location`](https://www.encyclopedia.com/computing/dictionaries-thesauruses-pictures-and-press-releases/measures-location).\n\nThe most common measures of location are the Mean, Median and Mode.\n\n### Mean (or **average**, or **Expected Value**) and Root Mean Square (RMS)\n\nMean is the sum of the values of the data points divided by the number of data points. That is,\n\n$$ \n\\bar{Y} = \\frac {\\sum_{i=1}^N Y_{i}}{N},\n$$\n\nBut, what if we have a sequence like `0\t4\t1\t-4\t-1`? The Mean will be 0! In that case we reside to the RMS which is \n\n$$\n\\bar{Y}_{rms} = \\sqrt \\frac {\\sum_{i=1}^N Y_{i}^2}{N},\n$$\n\n### Median\n\nIs the value of the point which has half the data smaller than that point and half the data larger than that point.\n\n$$\n\\tilde{Y} = Y_{\\frac {N+1}{2}}, \\text{if } N = odd\n$$\n\n$$\n\\tilde{Y} = \\frac {Y_{\\frac {N}{2}} + Y_{\\frac {N}{2} + 1}}{2}, \\text{if } N = even\n$$\n\n### Mode\n\nIs the value of the random sample that occurs with the greatest frequency (might not be unique).\n\n## Example\n\nLet us generate some random data to showcase the above. We use `random.normal` here, which generates normally distributed numbers (we will discuss about normality in a following article) between -10 and 10. In terms of Investing, think of it as the simple returns (return = the percentage change of the today's closing price, over the yesterday's closing price) of a stock over a period.\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set() \n\nrandomInts = np.random.normal(loc=10, scale=3, size=1000).astype(int)-10\n\ndf = pd.DataFrame(randomInts, columns=['Returns'])\ndf.plot();\n```\n   \n![png](measures-of-location/measures-of-location_3_0.png)\n\n### Mean\n\n```python\ndf.Returns.sum()/df.Returns.size\n```\n\n    -0.612\n\n```python\n# or\nmean = df.Returns.mean()\nmean\n```\n\n    -0.612\n\nWhich is the answer to: `What is the average return of a stock the last x days?`\n\n### Median\n\n```python\nnp.sort(df.Returns.values)[int(df.Returns.size/2)]\n```\n\n    -1\n\n```python\n# or\nmedian = np.percentile(df.Returns,50)\nmedian\n```\n    -1.0\n\n### Mode\n\n```python\nsample_data = [1,2,3,4,3,5,3,6,3,7,8,9]\nsample_data_df = pd.DataFrame(sample_data, columns=['Returns'])\nsample_data_df.Returns.mode()[0]\n```\n\n    3\n\nIt is apparent from the above that the number with the most frequent appearance is the number 3. That is because the numbers are discrete.\n\nBut, in a sequence of data that is continuous, the numbers can take any value in a range, which means decimal part (fractions). In cases like that it is difficult to find a single number that is present more often in the set. And that is normally the case in investing (the returns can take any value). Thus, we have to follow another approach and that is to separate the data into buckets. Here is where the notion of the [histogram](https://en.wikipedia.org/wiki/Histogram) comes into play and generates an outcome like:\n\n\n```python\ndf.plot.hist(bins=50, figsize=(12,6), grid=True);\n```\n    \n![png](measures-of-location/measures-of-location_14_0.png)\n\nWhat really happens is to order the values in ascending order and then separate them in a number of buckets. For example if the min value is 1 and the max 10 and we split them in 9 buckets, the first one will include all numbers from 1 to 2, the second all numbers from 2 to 3 and so on. The next step is to find the bucket with the highest amount of elements and take the middle value.  \n\n```python\ncounts, bins = np.histogram(df.Returns, bins=50)\nmax_index_col = np.argmax(counts, axis=0)\nmode = bins[max_index_col]\nmode\n```\n\n    -1.0\n\n```python\n# or\ndf.Returns.mode()[0]\n```\n\n    -1\n\n\n## Alternative Measures of Location\n\nIn addition to the more common measures, there are several more that can also be included to the investing algorithms. \n\n* Mid-Mean - computes a mean using the data between the 25th and 75th percentiles.\n\n* Trimmed Mean - similar to the mid-mean except different percentile values are used. A common choice is to trim 5% of the points in both the lower and upper tails, i.e., calculate the mean for data between the 5th and 95th percentiles.\n\n* Winsorized Mean - similar to the trimmed mean. However, instead of trimming the points, they are set to the lowest (or highest) value. For example, all data below the 5th percentile are set equal to the value of the 5th percentile and all data greater than the 95th percentile are set equal to the 95th percentile.\n\n* Mid-range = (largest + smallest)/2.\n\n### Mid-Mean\n\n```python\n# p_25 = df.Returns.quantile(0.25)  # Much slower than np.percentile\np_25 = np.percentile(df.Returns,25) # attention : the percentile is given in percent (5 = 5%)\np_75 = np.percentile(df.Returns,75)\nmid_mean = df[df.Returns.gt(p_25) & df.Returns.lt(p_75)].Returns.mean()\nmid_mean\n```\n\n    -1.010498687664042\n\n### Trimmed-Mean\n\n```python\np_5 = np.percentile(df.Returns,5)\np_95 = np.percentile(df.Returns,95)\ntrimmed_mean = df[df.Returns.gt(p_5) & df.Returns.lt(p_95)].Returns.mean()\ntrimmed_mean\n```\n\n    -0.5480427046263345\n\n### Winsorized Mean\n\n```python\ndata_indexes_to_stay_same = ( df.Returns > p_5 ) & ( df.Returns < p_95 )\ndata_to_stay_same = df[data_indexes_to_stay_same]\nall_data_bellow_p_5 = df[df.Returns <= p_5].copy()\nall_data_bellow_p_5.Returns.values[:] = p_5\nall_data_above_p_95 = df[df.Returns >= p_95].copy()\nall_data_above_p_95.Returns.values[:] = p_95\nwinsored_rets_list = [all_data_bellow_p_5, data_to_stay_same, all_data_above_p_95]\nwinsored_rets = pd.concat(winsored_rets_list)\nwinsored_mean = winsored_rets.Returns.mean()\nwinsored_mean\n```\n\n    -0.608\n\n### Mid-range\n\n```python\nmid_range = (df.Returns.max() + df.Returns.min())/2\nmid_range\n```\n\n    -1.0\n\n#### All Together\n\n```python\n# df.Returns.plot.hist(bins=200, figsize=(15,6), grid=True)\nplt.figure(figsize=(15,6))\nsns.histplot(df.Returns, kde=True, bins=100)\nplt.title(\"Measures of Location\")\nplt.xlabel(\"Utv\", labelpad=20, weight='bold', size=12)\nplt.ylabel(\"# of Utv in a bin\", labelpad=20, weight='bold', size=12)\nplt.axvline(x=mid_range, label='mid_range={:.3f}'.format(mid_range), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=mean, label='mean={:.3f}'.format(mean), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=median, label='median={:.3f}'.format(median), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=mode, label='mode={:.3f}'.format(mode), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=mid_mean, label='mid_mean={:.3f}'.format(mid_mean), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=winsored_mean, label='winsored_mean={:.3f}'.format(winsored_mean), ymax=0.95, c=np.random.rand(3,))\nplt.axvline(x=trimmed_mean, label='trimmed_mean={:.3f}'.format(trimmed_mean), ymax=0.95, c=np.random.rand(3,))\nplt.gca().legend(loc=\"upper left\")\nplt.show()\n```\n\n![png](measures-of-location/measures-of-location_28_0.png)\n"}],"tags":[{"id":"python","name":"python","image":"python-header.png","description":"Python is very handy in investing","color":"bg-green-300","icon":"python.svg","slug":"python","count":7},{"id":"statistics","name":"statistics","image":"statistics.jpg","description":"Statistics is the discipline that concerns the collection, organization, analysis, interpretation and presentation of data. - Wikipedia","color":"bg-green-300","icon":"statistics.svg","slug":"statistics","count":4}],"sortedTopics":[{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300","slug":"investing","count":5},{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300","slug":"mathematics","count":4},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300","slug":"automation","count":1}],"allTopics":[{"id":"mathematics","name":"Mathematics","image":"mathematics.png","description":"Investing, as part of the Science of Finance, is subject to the lows of Mathematics!","color":"bg-green-300"},{"id":"automation","name":"Automation","image":"automation.jpg","description":"Spending time to do the same thing over and over again is tedious! Thus, I like to automate as much as possible.","color":"bg-green-300"},{"id":"investing","name":"Investing","image":"investing.png","description":"The piggy bank digests a good amount of the savings! There is only one way to get away ... investing. BUT, there is a hidden enemy, Speculation!","color":"bg-green-300"}]},"__N_SSG":true}